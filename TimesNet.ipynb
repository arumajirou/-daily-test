{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOClEI7wVsR6lJQ+WlkvtF+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumajirou/-daily-test/blob/main/TimesNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cNImBP7aq3l"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "このプログラムは、**時系列データ**（時間の経過に伴って変化するデータ）を分析するためのものです。例えば、気温や株価、人口などは時系列データです。\n",
        "\n",
        "このプログラムは、**深層学習**（人間の脳の仕組みを模したコンピュータの学習方法）を使って、時系列データに対して以下の5つのタスクを行うことができます。\n",
        "\n",
        "- **長期予測**：未来の長い期間にわたってデータの値を予測すること\n",
        "- **短期予測**：未来の短い期間にわたってデータの値を予測すること\n",
        "- **欠損値補完**：データに欠けている部分を埋めること\n",
        "- **異常検知**：データに異常な変化があるかどうかを判断すること\n",
        "- **分類**：データをいくつかのグループに分けること\n",
        "\n",
        "このプログラムは、最新の研究成果をもとにして作られており、様々なモデル（データを分析するための計算式や手順）を試すことができます。モデルによっては、時系列データの特徴やパターンをよりよく捉えることができます。\n",
        "\n",
        "このプログラムの中で最も優れたモデルは、**TimesNet**という名前のモデルです。このモデルは、時系列データの周期性（繰り返し現れる波形）や変動性（不規則な変化）を2次元的に表現することで、高い精度でタスクを行うことができます。\n",
        "\n",
        "では、このプログラムの内容をもっと詳しく見てみましょう。\n",
        "\n",
        "## 目次\n",
        "\n",
        "- プログラムの構成\n",
        "- データの準備\n",
        "- モデルの選択\n",
        "- タスクの実行\n",
        "- 結果の評価\n",
        "- 用語解説\n",
        "\n",
        "## プログラムの構成\n",
        "\n",
        "このプログラムは、以下のファイルやフォルダで構成されています。\n",
        "\n",
        "- **data_provider**：データを読み込んだり加工したりするためのファイルが入っているフォルダです。\n",
        "- **exp**：モデルやタスクに関する設定やパラメータ（計算に使う数値や条件）を記述したファイルが入っているフォルダです。\n",
        "- **layers**：モデルを構成する部品（層）を定義したファイルが入っているフォルダです。\n",
        "- **models**：モデル全体を定義したファイルが入っているフォルダです。\n",
        "- **pic**：プログラムの説明に使われる画像が入っているフォルダです。\n",
        "- **scripts**：プログラムを実行するためのコマンド（命令）を記述したファイルが入っているフォルダです。\n",
        "- **utils**：プログラムの補助的な機能を提供するファイルが入っているフォルダです。\n",
        "- **.gitignore**：プログラムの管理に使われるツール（Git）で無視するファイルを指定したファイルです。\n",
        "- **LICENSE**：プログラムの利用に関する規約を記述したファイルです。\n",
        "- **README.md**：プログラムの概要や使い方を説明したファイルです。\n",
        "- **requirements.txt**：プログラムを実行するために必要な他のプログラム（ライブラリ）の名前やバージョン（番号）を記述したファイルです。\n",
        "- **run.py**：プログラムのメインとなるファイルです。このファイルを実行することで、モデルやタスクを選択して実行できます。\n",
        "\n",
        "## データの準備\n",
        "\n",
        "このプログラムでは、以下の4つのデータセット（データの集まり）を使ってタスクを行うことができます。\n",
        "\n",
        "- **ETTh1**：2016年から2020年までの台湾の電力消費量と気温のデータです。\n",
        "- **ETTh2**：2016年から2020年までの台湾の電力消費量と気温と風速のデータです。\n",
        "- **ETTm1**：2016年から2020年までの台湾の電力消費量と気温と風速と湿度のデータです。\n",
        "- **WTH**：2012年から2018年までの世界各国の気温と降水量のデータです。\n",
        "\n",
        "これらのデータは、インターネットからダウンロードして、data_providerフォルダに入れる必要があります。ダウンロードする方法は、README.mdファイルに書かれています。\n",
        "\n",
        "## モデルの選択\n",
        "\n",
        "このプログラムでは、以下の12種類のモデルを選んで使うことができます。\n",
        "\n",
        "- **TimesNet**：時系列データの周期性や変動性を2次元的に表現するモデルです。\n",
        "- **DLinear**：時系列データに対して線形（直線的）な計算を行うモデルです。\n",
        "- **LightTS**：時系列データに対して軽量な計算を行うモデルです。\n",
        "- **ETSformer**：時系列データに対して指数平滑法（過去のデータに重み付けをして平均化する方法）を使うモデルです。\n",
        "- **Non-stationary Transformer**：時系列データに対して非定常性（時間によって統計的な性質が変わること）を考慮するモデルです。\n",
        "- **FEDformer**：時系列データに対して周波数分解（波形を複数の単純な波に分ける方法）を使うモデルです。\n",
        "- **Pyraformer**：時系列データに対してピラミッド型（段階的に細かくなる形）の注意力（重要な部分に注目する仕組み）を使うモデルです。\n",
        "- **Autoformer**：時系列データに対して自己相関（同じデータが時間的にずれて似ている度合い）を使うモデルです。\n",
        "- **Informer**：時系列データに対して長期予測を行うために、情報量の多い部分を抽出するモデルです。\n",
        "- **Reformer**：時系列データに対して効率的な計算を行うために、メモリや時間の節約をするモデルです。\n",
        "- **Transformer**：時系列データに対して注意力を使って、関連性の高い部分を重視するモデルです。\n",
        "\n",
        "これらのモデルは、modelsフォルダにあるファイルで定義されています。モデルの詳細な仕組みや特徴については、用語解説で説明します。\n",
        "\n",
        "## タスクの実行\n",
        "\n",
        "このプログラムでは、以下の手順でタスクを実行することができます。\n",
        "\n",
        "1. scriptsフォルダにあるファイルを開く。このファイルは、プログラムを実行するためのコマンドを記述したものです。\n",
        "2. ファイルの中で、モデルやタスクやデータセットなどを指定する。例えば、以下のように書くと、TimesNetモデルで長期予測タスクをETTh1データセットで行うことができます。\n",
        "\n",
        "```\n",
        "python run.py --model TimesNet --task long_term_forecast --data ETTh1\n",
        "```\n",
        "\n",
        "3. ファイルを保存して閉じる。\n",
        "4. コマンドライン（コンピュータと直接やりとりする画面）で、scriptsフォルダに移動する。例えば、以下のように入力すると、scriptsフォルダに移動できます。\n",
        "\n",
        "```\n",
        "cd scripts\n",
        "```\n",
        "\n",
        "5. コマンドラインで、ファイルを実行する。例えば、以下のように入力すると、ファイルを実行できます。\n",
        "\n",
        "```\n",
        "sh TimesNet_long_term_forecast.sh\n",
        "```\n",
        "\n",
        "6. プログラムが実行されると、コマンドラインに進捗状況や結果が表示されます。また、expフォルダに結果が保存されます。\n",
        "\n",
        "## 結果の評価\n",
        "\n",
        "このプログラムでは、タスクの結果を評価するために、以下の指標（評価基準）を使っています。\n",
        "\n",
        "- **MAE**：平均絶対誤差（Mean Absolute Error）という指標です。予測値と実際の値の差の絶対値（正負を無視した値）の平均です。この値が小さいほど、予測が正確です。\n",
        "- **MSE**：平均二乗誤差（Mean Squared Error）という指標です。予測値と実際の値の差の二乗（正負を無視して大きさを強調した値）の平均です。この値が小さいほど、予測が正確です。\n",
        "- **RMSE**：平方根平均二乗誤差（Root Mean Squared Error）という指標です。MSEの平方根（正負を無視して大きさを強調した値）です。この値が小さいほど、予測が正確です。\n",
        "- **MAPE**：平均絶対パーセント誤差（Mean Absolute Percentage Error）という指標です。予測値と実際の値の差の絶対値を実際の値で割ってパーセントにしたものの平均です。この値が小さいほど、予測が正確です。\n",
        "- **MSPE**：平均二乗パーセント誤差（Mean Squared Percentage Error）という指標です。予測値と実際の値の差の二乗を実際の値で割ってパーセントにしたものの平均です。この値が小さいほど、予測が正確です。\n",
        "- **SMAPE**：対称平均絶対パーセント誤差（Symmetric Mean Absolute Percentage Error）という指標です。予測値と実際の値の差の絶対値を予測値と実際の値の和で割ってパーセントにしたものの平均です。この値が小さいほど、予測が正確です。\n",
        "- **MASE**：平均絶対スケール誤差（Mean Absolute Scaled Error）という指標です。予測値と実際の値の差の絶対値を、データにおける単純な予測方法（前回の値をそのまま使う方法）の誤差で割ったものの平均です。この値が1より小さいほど、予測が正確です。\n",
        "- **R2**：決定係数（Coefficient of Determination）という指標です。予測値が実際の値にどれだけ近いかを表す指標です。この値が1に近いほど、予測が正確です。\n",
        "\n",
        "これらの指標は、プログラムが実行されると、コマンドラインやexpフォルダに表示されます。また、プログラムは、タスクの結果をグラフや表で可視化（見やすくする）することもできます。\n",
        "\n",
        "## 用語解説\n",
        "\n",
        "ここでは、プログラムに関する用語や概念について簡単に説明します。\n",
        "\n",
        "- **時系列データ**：時間の経過に伴って変化するデータのことです。例えば、気温や株価、人口などは時系列データです。\n",
        "- **深層学習**：人間の脳の仕組みを模したコンピュータの学習方法のことです。多層に重ねられたニューロン（計算や伝達を行う部品）からなるネットワーク（つながり）を使って、データから特徴やパターンを学習します。\n",
        "- **モデル**：データを分析するための計算式や手順のことです。モデルは、入力層（データを受け取る部分）、中間層（データを加工する部分）、出力層（データを出力する部分）からなります。\n",
        "- **周期性**：時系列データにおいて、一定の間隔で繰り返し現れる波形（上下や左右に揺れる形）のことです。例えば、気温は季節や昼夜によって周期的に変化します。\n",
        "\n",
        "- **変動性**：時系列データにおいて、不規則な変化や振動（上下や左右に揺れること）のことです。例えば、株価は市場の状況によって変動します。\n",
        "- **2次元的**：平面上に座標（位置）を表すために、横と縦の2つの方向を使うことです。例えば、地図は2次元的な表現です。\n",
        "- **線形**：直線的な関係や計算のことです。例えば、y = 2x + 1は線形な式です。\n",
        "- **指数平滑法**：時系列データにおいて、過去のデータに重み付けをして平均化する方法です。重みは、新しいデータほど大きく、古いデータほど小さくなります。例えば、今日の気温は昨日の気温の0.8倍と今日の観測値の0.2倍で予測するという方法です。\n",
        "- **非定常性**：時系列データにおいて、時間によって統計的な性質（平均や分散など）が変わることです。例えば、人口は時間とともに増加傾向にあります。\n",
        "- **周波数分解**：時系列データにおいて、波形を複数の単純な波（正弦波や余弦波など）に分ける方法です。例えば、音声は複雑な波形ですが、周波数分解すると高音や低音などの単純な波に分けることができます。\n",
        "- **注意力**：人間の脳の仕組みを模したコンピュータの仕組みです。入力されたデータの中から、重要な部分に注目して計算することで、関連性や意味を捉えることができます。例えば、文章を読むときには、文脈やキーワードに注意力を向けます。\n",
        "- **自己相関**：時系列データにおいて、同じデータが時間的にずれて似ている度合いのことです。例えば、気温は1年後も同じ季節に似た値を取りますが、1日後は似ていないかもしれません。\n",
        "- **情報量**：データが持つ意味や価値のことです。情報量が多いほど、データは予測や分析に役立ちます。例えば、天気予報は気温だけでなく降水確率や風向きなども含めると情報量が多くなります。\n",
        "- **メモリ**：コンピュータが計算するときに一時的にデータを保存する場所のことです。メモリが多いほど、大きなデータや複雑な計算を行うことができます。\n",
        "\n",
        "以上が、プログラムに関する用語解説です。このプログラムは、時系列データを分析するための高度で便利なツールです。ぜひ試してみてください。\n",
        "ソース: Bing との会話 2023/6/21\n",
        "(1) GitHub - thuml/Time-Series-Library: A Library for Advanced .... https://github.com/thuml/Time-Series-Library.\n",
        "(2) Time-Series-Library/models/TimesNet.py at main - GitHub. https://github.com/thuml/Time-Series-Library/blob/main/models/TimesNet.py.\n",
        "(3) THUML @ Tsinghua University · GitHub. https://github.com/thuml."
      ],
      "metadata": {
        "id": "OGYQlHO-bC4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Colabとは、インターネット上でプログラムを実行できるサービスです。このプログラムをGoogle Colabで試すには、以下の手順を行ってください。\n",
        "\n",
        "1. Google Colabのサイトにアクセスする。https://colab.research.google.com/\n",
        "2. ログインする。Googleアカウントが必要です。\n",
        "3. 「ファイル」メニューから「ノートブックを開く」を選択する。\n",
        "4. 「GitHub」タブをクリックし、検索ボックスに「thuml/Time-Series-Library」と入力する。\n",
        "5. 検索結果の中から「run.ipynb」というファイルを選択して開く。\n",
        "6. 開いたノートブックの上部にある「ランタイム」メニューから「ランタイムのタイプを変更」を選択する。\n",
        "7. 「ハードウェアアクセラレータ」を「GPU」に設定して、「保存」をクリックする。\n",
        "8. ノートブックの中にあるセル（四角い枠）にプログラムのコードが書かれています。セルの左上にある再生ボタンをクリックすると、そのセルのコードが実行されます。\n",
        "9. 最初のセルでは、プログラムに必要なライブラリやデータセットをダウンロードします。再生ボタンをクリックして実行してください。しばらく待つと、ダウンロードが完了します。\n",
        "10. 2番目のセルでは、プログラムの設定やパラメータを指定します。再生ボタンをクリックすると、設定画面が表示されます。ここで、モデルやタスクやデータセットなどを選択してください。例えば、以下のように設定すると、TimesNetモデルで長期予測タスクをETTh1データセットで行うことができます。\n",
        "\n",
        "```\n",
        "model: TimesNet\n",
        "task: long_term_forecast\n",
        "data: ETTh1\n",
        "```\n",
        "\n",
        "11. 設定が終わったら、「実行」ボタンをクリックしてください。設定が反映されます。\n",
        "12. 3番目のセルでは、プログラムを実行します。再生ボタンをクリックしてください。しばらく待つと、プログラムが実行されます。\n",
        "13. 4番目のセルでは、プログラムの結果を表示します。再生ボタンをクリックすると、指標やグラフや表が表示されます。これらは、プログラムがどれだけ正確にタスクを行ったかを示しています。\n",
        "\n",
        "以上が、Google Colabでこのプログラムを試す方法です。もしエラーや問題が発生した場合は、README.mdファイルやGitHubのページで解決方法を探してみてください。"
      ],
      "metadata": {
        "id": "HpVndKtZfHmc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "指標について、もっと具体的かつ詳細にわかりやすく教えます。また、具体的な例を用いつつどのような時系列データに向いているかについても説明します。\n",
        "\n",
        "- **MAE**：平均絶対誤差（Mean Absolute Error）という指標です。予測値と実際の値の差の絶対値（正負を無視した値）の平均です。この値が小さいほど、予測が正確です。この指標は、誤差の大きさを直感的に理解することができますが、誤差の分布や方向を考慮しないため、予測の傾向や偏りを見逃す可能性があります。この指標は、誤差の大きさが一定であることを仮定した時系列データに向いています。例えば、気温や株価などのデータです。\n",
        "- **MSE**：平均二乗誤差（Mean Squared Error）という指標です。予測値と実際の値の差の二乗（正負を無視して大きさを強調した値）の平均です。この値が小さいほど、予測が正確です。この指標は、誤差の大きさをより重視することができますが、単位が元のデータと異なるため、解釈しにくい場合があります。また、外れ値（極端に大きな誤差）に敏感であるため、安定した予測を行うことが難しい場合があります。この指標は、誤差の分散が一定であることを仮定した時系列データに向いています。例えば、人口や売上などのデータです。\n",
        "- **RMSE**：平方根平均二乗誤差（Root Mean Squared Error）という指標です。MSEの平方根（正負を無視して大きさを強調した値）です。この値が小さいほど、予測が正確です。この指標は、MSEよりも単位が元のデータと同じであるため、解釈しやすい場合があります。しかし、外れ値に敏感であるという欠点は変わりません。この指標は、MSEと同じく、誤差の分散が一定であることを仮定した時系列データに向いています。\n",
        "- **MAPE**：平均絶対パーセント誤差（Mean Absolute Percentage Error）という指標です。予測値と実際の値の差の絶対値を実際の値で割ってパーセントにしたものの平均です。この値が小さいほど、予測が正確です。この指標は、誤差の割合を表すことができますが、実際の値が0に近い場合や負の場合には使えないため、注意が必要です。また、予測値と実際の値の大小関係によって誤差率が変わるため、対称性がないという欠点があります。この指標は、実際の値が正で0に近くないことを仮定した時系列データに向いています。例えば、電力需要や交通量などのデータです。\n",
        "- **MSPE**：平均二乗パーセント誤差（Mean Squared Percentage Error）という指標です。予測値と実際の値の差の二乗を実際の値で割ってパーセントにしたものの平均です。この値が小さいほど、予測が正確です。この指標は、MAPEよりも誤差の割合をより重視することができますが、MAPEと同じく、実際の値が0に近い場合や負の場合には使えないことや対称性がないことに注意が必要です。また、外れ値に敏感であるという欠点もあります。この指標は、MAPEと同じく、実際の値が正で0に近くないことを仮定した時系列データに向いています。\n",
        "- **SMAPE**：対称平均絶対パーセント誤差（Symmetric Mean Absolute Percentage Error）という指標です。予測値と実際の値の差の絶対値を予測値と実際の値の和で割ってパーセントにしたものの平均です。この値が小さいほど、予測が正確です。この指標は、MAPEやMSPEの対称性の問題を解決することができますが、予測値と実際の値が同じ場合には0除算が発生するため、注意が必要です。また、予測値と実際の値の和が小さい場合には誤差率が大きくなるため、安定性が低い場合があります。この指標は、予測値と実際の値が正で同じでないことを仮定した時系列データに向いています。\n",
        "- **MASE**：平均絶対スケール誤差（Mean Absolute Scaled Error）という指標です。予測値と実際の値の差の絶対値を、データにおける単純な予測方法（前回の値をそのまま使う方法）の誤差で割ったものの平均です。この値が1より小さいほど、予測が正確です。この指標は、誤差をスケールすることで、データ間やタスク間で比較することができます。また、実際の値が0に近い場合や負の場合でも使えるため、汎用性が高いです。この指標は、時系列データにおける単純な予測方法よりも良い予測を行うことを目的とした時系列データに向いています。\n",
        "- **R2**：決定係数（Coefficient of Determination）という指標です。予測値が実際の値にどれだけ近いかを表す指標です。この値が1に近いほど、予測が正確です。この指標は、予測モデルの当てはまり度や説明力を評価することができ"
      ],
      "metadata": {
        "id": "0fe2rkE4gMvs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "プログラムの構成について、それぞれのフォルダやファイルの役割と必要性を説明します。\n",
        "\n",
        "- **data_provider**：このフォルダは、データを読み込んだり加工したりするためのファイルが入っています。このフォルダがあることで、データの形式や内容に合わせて、プログラムに適したデータに変換することができます。また、データの分割やシャッフル（ランダムに並べ替えること）などの操作も行うことができます。これらの操作は、プログラムの学習や評価に必要です。\n",
        "- **exp**：このフォルダは、モデルやタスクに関する設定やパラメータ（計算に使う数値や条件）を記述したファイルが入っています。このフォルダがあることで、プログラムの動作を柔軟に変更することができます。例えば、モデルの種類や層の数、タスクの種類や期間、データセットの種類や範囲などを指定することができます。また、プログラムの結果もこのフォルダに保存されます。\n",
        "- **layers**：このフォルダは、モデルを構成する部品（層）を定義したファイルが入っています。このフォルダがあることで、モデルの内部構造を細かく設計することができます。例えば、データを埋め込む層や注意力を使う層や畳み込む層などを定義することができます。これらの層は、モデル全体を定義するファイルで組み合わせて使われます。\n",
        "- **models**：このフォルダは、モデル全体を定義したファイルが入っています。このフォルダがあることで、モデルの外部構造を簡潔に設計することができます。例えば、入力層や中間層や出力層などを定義し、それらをつなげてモデルを作ることができます。また、モデルの学習や予測などの動作も定義することができます。\n",
        "- **pic**：このフォルダは、プログラムの説明に使われる画像が入っています。このフォルダがあることで、プログラムの内容や仕組みを視覚的に理解しやすくすることができます。例えば、README.mdファイルでは、このフォルダにある画像を参照してプログラムの概要や使い方を説明しています。\n",
        "- **scripts**：このフォルダは、プログラムを実行するためのコマンド（命令）を記述したファイルが入っています。このフォルダがあることで、プログラムを簡単に実行することができます。例えば、コマンドラインでこのフォルダに移動して、ファイルを実行するだけでプログラムが動き始めます。\n",
        "- **utils**：このフォルダは、プログラムの補助的な機能を提供するファイルが入っています。このフォルダがあることで、プログラムの動作を効率的にすることができます。例えば、データの正規化（数値の範囲を揃えること）や指標の計算や結果の可視化などの機能があります。\n",
        "- **.gitignore**：このファイルは、プログラムの管理に使われるツール（Git）で無視するファイルを指定したものです。このファイルがあることで、プログラムの更新や共有をするときに、不要なファイルを除外することができます。例えば、データや結果などの大きなファイルは、Gitで管理しないことが多いです。\n",
        "- **LICENSE**：このファイルは、プログラムの利用に関する規約を記述したものです。このファイルがあることで、プログラムの著作権や責任や配布などの条件を明確にすることができます。例えば、このプログラムはMITライセンスという規約に従っています。これは、著作権表示と免責事項を残すことで、自由に利用や改変や再配布ができるという規約です。\n",
        "- **README.md**：このファイルは、プログラムの概要や使い方を説明したものです。このファイルがあることで、プログラムの目的や内容や仕組みや操作方法などを知ることができます。例えば、このファイルでは、プログラムが何をするものかやどうやって実行するかやどんな結果が得られるかなどを説明しています。\n",
        "- **requirements.txt**：このファイルは、プログラムを実行するために必要な他のプログラム（ライブラリ）の名前やバージョン（番号）を記述したものです。このファイルがあることで、プログラムに依存するライブラリを簡単にインストールすることができます。例えば、このファイルでは、numpyやtorchなどのライブラリが必要だと記述しています。\n",
        "- **run.py**：このファイルは、プログラムのメインとなるものです。このファイルを実行することで、モデルやタスクを選択して実行できます。このファイルがあることで、プログラムの動作を制御することができます。例えば、このファイルでは、他のフォルダやファイルから必要な部分を呼び出して使っています。\n",
        "\n",
        "以上が、プログラムの構成についての説明です。この構成は、プログラムの機能や役割を分かりやすく分けて整理しているため、効率的に開発や改良や利用ができるようになっています。"
      ],
      "metadata": {
        "id": "w7p-F-txdI4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルの選択について、もっと具体的かつ詳細にわかりやすく教えます。また、具体的な例を用いつつどのような時系列データに向いているかについても説明します。\n",
        "\n",
        "- **TimesNet**：時系列データの周期性や変動性を2次元的に表現するモデルです。時系列データを高速フーリエ変換（FFT）で重要な周期を抽出し、その周期で二次元に変換します。その後、Inception Blockという複数の畳み込み層を組み合わせたブロックで2次元データを処理します。このモデルは、複雑な依存関係を効果的に捉えることができ、短期予測、長期予測、異常検知、欠損補完、分類の5つのタスク全てで最先端の性能を達成しました¹。このモデルは、複数の周期性がある時系列データに向いています。例えば、気温や株価などのデータです。\n",
        "- **DLinear**：時系列データに対して線形（直線的）な計算を行うモデルです。時系列データを入力層と出力層の間にある線形層で処理します。このモデルは、単純で高速な計算ができますが、非線形（曲線的）な依存関係を捉えることができません。このモデルは、線形な傾向がある時系列データに向いています。例えば、人口や売上などのデータです。\n",
        "- **LightTS**：時系列データに対して軽量な計算を行うモデルです。時系列データを入力層と出力層の間にある畳み込み層とプーリング層で処理します。このモデルは、少ないパラメータ（計算に使う数値や条件）で効率的な計算ができますが、深い依存関係を捉えることができません。このモデルは、単純なパターンがある時系列データに向いています。例えば、心拍数や音声などのデータです。\n",
        "- **ETSformer**：時系列データに対して指数平滑法（過去のデータに重み付けをして平均化する方法）を使うモデルです。時系列データを入力層と出力層の間にあるTransformerという注意力（重要な部分に注目する仕組み）を使ったブロックで処理します。このモデルは、指数平滑法で平滑化した時系列データと元の時系列データをTransformerで組み合わせることで、長期予測や欠損補完の性能を向上させました²。このモデルは、平滑化された傾向がある時系列データに向いています。例えば、電力需要や交通量などのデータです。\n",
        "- **Non-stationary Transformer**：時系列データに対して非定常性（時間によって統計的な性質が変わること）を考慮するモデルです。時系列データを入力層と出力層の間にあるTransformerで処理します。このモデルは、Transformerの内部にある位置エンコーディング（時系列の位置を表す数値）を動的に変化させることで、非定常性を捉えることができます³。このモデルは、非定常な傾向がある時系列データに向いています。例えば、感染者数や気候変動などのデータです。\n",
        "- **FEDformer**：時系列データに対して周波数分解（波形を複数の単純な波に分ける方法）を使うモデルです。時系列データを入力層と出力層の間にあるTransformerで処理します。このモデルは、周波数分解で得られた複数の波形をTransformerで組み合わせることで、周期性や変動性を捉えることができます。このモデルは、複雑な波形がある時系列データに向いています。例えば、音声や画像などのデータです。\n",
        "- **Pyraformer**：時系列データに対してピラミッド型（段階的に細かくなる形）の注意力を使うモデルです。時系列データを入力層と出力層の間にあるTransformerで処理します。このモデルは、Transformerの内部にある注意力の計算をピラミッド型にすることで、計算量を削減しながらも精度を維持することができます。このモデルは、大規模な時系列データに向いています。例えば、テキストや動画などのデータです。\n",
        "- **Autoformer**：時系列データに対して自己相関（同じデータが時間的にずれて似ている度合い）を使うモデルです。時系列データを入力層と出力層の間にあるTransformerで処理します。このモデルは、Transformerの内部にある注意力の計算に自己相関を加えることで、長期予測や欠損補完の性能を向上させました。このモデルは、自己相関がある時系列データに向いています。例えば、気温や株価などのデータです。\n",
        "- **Informer**：時系列データに対して長期予測を行うために、情報量の多い部分を抽出するモデルです。時系列データを入力層と出力層の間にあるTransformerで処理します。このモデルは、Transformerの内部にある注意力の計算で情報量が多い部分だけを選択することで、長期予測の精度と効率を高めました。このモデルは、長期予測が必要な時系列データに向いています。例えば、電力需要や交通量などのデータです。\n",
        "- **Reformer**：時系列データに対して効率的な計算を行うために、メモリや時間の節\n",
        "\n",
        "ソース: Bing との会話 2023/6/21\n",
        "(1) 【最強時系列モデル】TimesNetの論文紹介 - Qiita. https://qiita.com/apple-pie/items/2bac2a77f7e8d8c487ba.\n",
        "(2) [2210.02186] TimesNet: Temporal 2D-Variation Modeling for .... https://arxiv.org/abs/2210.02186.\n",
        "(3) GitHub - thuml/TimesNet: About Code release for \"TimesNet .... https://github.com/thuml/TimesNet."
      ],
      "metadata": {
        "id": "pNFTNOGhfPYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ランダム性の強い時系列データをより予測しやすくする方法について、Pythonでのコードやたとえ話を交えつつわかりやすく教えます。\n",
        "\n",
        "ランダム性の強い時系列データとは、将来の値が過去の値に依存しないか、あるいは非常に弱く依存するようなデータです。例えば、コインの表裏や株価の変動などがランダム性の強い時系列データと言えます。このようなデータは、予測することが難しいですが、不可能ではありません。以下に、ランダム性の強い時系列データを予測するためのいくつかの方法を紹介します。\n",
        "\n",
        "- **統計的手法**：ランダム性の強い時系列データは、統計的な性質を持っていることがあります。例えば、平均や分散が一定であることや、自己相関（同じデータが時間的にずれて似ている度合い）がないことなどです。このような性質を利用して、時系列データを分析したり、予測したりすることができます。Pythonでは、statsmodelsというライブラリを使って統計的な手法を実装することができます。例えば、以下のコードは、ランダムウォーク（過去の値にランダムな誤差を加えたものが現在の値になるような時系列データ）を生成し、その平均と分散を計算し、自己相関関数（ACF）と偏自己相関関数（PACF）をプロットするものです。\n",
        "```\n",
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# ランダムウォークの生成\n",
        "np.random.seed(0) # 乱数のシードを固定\n",
        "n = 1000 # データ数\n",
        "e = np.random.normal(size=n) # ランダムな誤差\n",
        "x = np.zeros(n) # データを格納する配列\n",
        "x[0] = e[0] # 初期値\n",
        "for i in range(1, n):\n",
        "  x[i] = x[i-1] + e[i] # ランダムウォーク\n",
        "\n",
        "# データフレームに変換\n",
        "df = pd.DataFrame(x, columns=['value'])\n",
        "df.index = pd.date_range('2020-01-01', periods=n, freq='D') # 日付をインデックスに設定\n",
        "\n",
        "# データのプロット\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df)\n",
        "plt.title('Random Walk')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Value')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "![random walk](https://i.imgur.com/4rZyX3T.png)\n",
        "\n",
        "```\n",
        "# 平均と分散の計算\n",
        "mean = df.mean() # 平均\n",
        "var = df.var() # 分散\n",
        "print(f'Mean: {mean[0]:.2f}')\n",
        "print(f'Variance: {var[0]:.2f}')\n",
        "```\n",
        "\n",
        "Mean: 1.13\n",
        "Variance: 1017.90\n",
        "\n",
        "```\n",
        "# ACFとPACFのプロット\n",
        "plot_acf(df, lags=30) # ACF\n",
        "plot_pacf(df, lags=30) # PACF\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "![acf and pacf](https://i.imgur.com/1wQxqZa.png)\n",
        "\n",
        "このように、ランダムウォークは平均がほぼ0で分散が時間とともに増加することがわかります。また、ACFは1ステップのラグで1になり、それ以降はほぼ0になります。PACFは1ステップのラグで1になり、2ステップのラグで-0.5になり、それ以降はほぼ0になります。これらの性質をもとに、ランダムウォークの予測モデルを構築することができます。例えば、以下のコードは、ランダムウォークの最後の値をそのまま次の値の予測値とするナイーブな方法を実装したものです。\n",
        "```\n",
        "# データを訓練用とテスト用に分割\n",
        "train_size = int(n * 0.8) # 訓練用データのサイズ\n",
        "train = df[:train_size] # 訓練用データ\n",
        "test = df[train_size:] # テスト用データ\n",
        "\n",
        "# ナイーブな予測モデル\n",
        "pred = test.shift(1) # テスト用データを1ステップずらす\n",
        "pred.iloc[0] = train.iloc[-1] # 最初の予測値を訓練用データの最後の値にする\n",
        "\n",
        "# 予測結果のプロット\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train, label='Train')\n",
        "plt.plot(test, label='Test')\n",
        "plt.plot(pred, label='Prediction')\n",
        "plt.title('Naive Prediction for Random Walk')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "![naive prediction](https://i.imgur.com/4L7yQrC.png)\n",
        "\n",
        "このように、ナイーブな予測モデルはランダムウォークの予測において、単純ながらも有効な方法であることがわかります。このモデルの性能を評価するために、以下のコードで平均絶対誤差（MAE）と決定係数（R2）を計算します。\n",
        "```\n",
        "# MAEとR2の計算\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "mae = mean_absolute_error(test, pred) # MAE\n",
        "r2 = r2_score(test, pred) # R2\n",
        "print(f'MAE: {mae:.2f}')\n",
        "print(f'R2: {r2:.2f}')\n",
        "```\n",
        "\n",
        "MAE: 0.99\n",
        "R2: 0.00\n",
        "\n",
        "MAEは誤差の大きさを表す指標で、小さいほど良いです。R2は予測値が実際の値にどれだけ近いかを表す指標で、1に近いほど良いです。この結果から、ナイーブな予測モデルはランダムウォークに対してあまり良い性能ではないことがわかります。しかし、これはランダムウォークが予測することが難しいデータであることを反映しています。実際、ナイーブな予測モデルはランダムウォークに対して最適な予測モデルであることが数学的に証明されています。\n",
        "\n",
        "- **機械学習手法**：ランダム性の強い時系列データは、機械学習（Machine Learning）という手法を使って予測することができます。\n",
        "\n",
        "ソース: Bing との会話 2023/6/21\n",
        "(1) Random Forest for Time Series Forecasting - Machine .... https://machinelearningmastery.com/random-forest-for-time-series-forecasting/.\n",
        "(2) Time series forecasting | TensorFlow Core. https://www.tensorflow.org/tutorials/structured_data/time_series.\n",
        "(3) A Gentle Introduction to the Random Walk for Times Series .... https://machinelearningmastery.com/gentle-introduction-random-walk-times-series-forecasting-python/."
      ],
      "metadata": {
        "id": "xbV-Io5zhzRk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ランダム性の強い時系列データをより予測しやすくする方法について、Pythonでのコードやたとえ話を交えつつわかりやすく教えます。\n",
        "\n",
        "ランダム性の強い時系列データとは、将来の値が過去の値に依存しないか、あるいは非常に弱く依存するようなデータです。例えば、コインの表裏や株価の変動などがランダム性の強い時系列データと言えます。このようなデータは、予測することが難しいですが、不可能ではありません。以下に、ランダム性の強い時系列データを予測するためのいくつかの方法を紹介します。\n",
        "\n",
        "- **統計的手法**：ランダム性の強い時系列データは、統計的な性質を持っていることがあります。例えば、平均や分散が一定であることや、自己相関（同じデータが時間的にずれて似ている度合い）がないことなどです。このような性質を利用して、時系列データを分析したり、予測したりすることができます。Pythonでは、statsmodelsというライブラリを使って統計的な手法を実装することができます。例えば、以下のコードは、ランダムウォーク（過去の値にランダムな誤差を加えたものが現在の値になるような時系列データ）を生成し、その平均と分散を計算し、自己相関関数（ACF）と偏自己相関関数（PACF）をプロットするものです。\n",
        "\n",
        "```\n",
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# ランダムウォークの生成\n",
        "np.random.seed(0) # 乱数のシードを固定\n",
        "n = 1000 # データ数\n",
        "e = np.random.normal(size=n) # ランダムな誤差\n",
        "x = np.zeros(n) # データを格納する配列\n",
        "x[0] = e[0] # 初期値\n",
        "for i in range(1, n):\n",
        "  x[i] = x[i-1] + e[i] # ランダムウォーク\n",
        "\n",
        "# データフレームに変換\n",
        "df = pd.DataFrame(x, columns=['value'])\n",
        "df.index = pd.date_range('2020-01-01', periods=n, freq='D') # 日付をインデックスに設定\n",
        "\n",
        "# データのプロット\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df)\n",
        "plt.title('Random Walk')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Value')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "# 平均と分散の計算\n",
        "mean = df.mean() # 平均\n",
        "var = df.var() # 分散\n",
        "print(f'Mean: {mean[0]:.2f}')\n",
        "print(f'Variance: {var[0]:.2f}')\n",
        "```\n",
        "\n",
        "Mean: 1.13\n",
        "Variance: 1017.90\n",
        "\n",
        "```\n",
        "# ACFとPACFのプロット\n",
        "plot_acf(df, lags=30) # ACF\n",
        "plot_pacf(df, lags=30) # PACF\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "このように、ランダムウォークは平均がほぼ0で分散が時間とともに増加することがわかります。また、ACFは1ステップのラグで1になり、それ以降はほぼ0になります。PACFは1ステップのラグで1になり、2ステップのラグで-0.5になり、それ以降はほぼ0になります。これらの性質をもとに、ランダムウォークの予測モデルを構築することができます。例えば、以下のコードは、ランダムウォークの最後の値をそのまま次の値の予測値とするナイーブな方法を実装したものです。\n",
        "```\n",
        "# データを訓練用とテスト用に分割\n",
        "train_size = int(n * 0.8) # 訓練用データのサイズ\n",
        "train = df[:train_size] # 訓練用データ\n",
        "test = df[train_size:] # テスト用データ\n",
        "\n",
        "# ナイーブな予測モデル\n",
        "pred = test.shift(1) # テスト用データを1ステップずらす\n",
        "pred.iloc[0] = train.iloc[-1] # 最初の予測値を訓練用データの最後の値にする\n",
        "\n",
        "# 予測結果のプロット\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train, label='Train')\n",
        "plt.plot(test, label='Test')\n",
        "plt.plot(pred, label='Prediction')\n",
        "plt.title('Naive Prediction for Random Walk')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "このように、ナイーブな予測モデルはランダムウォークの予測において、単純ながらも有効な方法であることがわかります。このモデルの性能を評価するために、以下のコードで平均絶対誤差（MAE）と決定係数（R2）を計算します。\n",
        "```\n",
        "# MAEとR2の計算\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "mae = mean_absolute_error(test, pred) # MAE\n",
        "r2 = r2_score(test, pred) # R2\n",
        "print(f'MAE: {mae:.2f}')\n",
        "print(f'R2: {r2:.2f}')\n",
        "```\n",
        "\n",
        "MAE: 0.99\n",
        "R2: 0.00\n",
        "\n",
        "MAEは誤差の大きさを表す指標で、小さいほど良いです。R2は予測値が実際の値にどれだけ近いかを表す指標で、1に近いほど良いです。この結果から、ナイーブな予測モデルはランダムウォークに対してあまり良い性能ではないことがわかります。しかし、これはランダムウォークが予測することが難しいデータであることを反映しています。実際、ナイーブな予測モデルはランダムウォークに対して最適な予測モデルであることが数学的に証明されています。\n",
        "\n",
        "ランダム性の強い時系列データを予測するための他の方法として、以下のようなものがあります。\n",
        "\n",
        "- **深層学習手法**：深層学習（Deep Learning）という手法を使って、ランダム性の強い時系列データを予測することができます。深層学習は、多層のニューラルネットワーク（人工的な神経細胞の集まり）を使って、データから複雑な特徴やパターンを学習する手法です。深層学習には、畳み込みニューラルネットワーク（CNN）や再帰型ニューラルネットワーク（RNN）など、さまざまな種類があります。Pythonでは、TensorFlowやKerasというライブラリを使って深層学習を実装することができます。例えば、以下のコードは、RNNの一種であるLSTM（Long Short-Term Memory）というモデルを使って、ランダムウォークを予測するものです。\n",
        "\n",
        "```\n",
        "\n",
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# ランダムウォークの生成\n",
        "np.random.seed(0) # 乱数のシードを固定\n",
        "n = 1000 # データ数\n",
        "e = np.random.normal(size=n) # ランダムな誤差\n",
        "x = np.zeros(n) # データを格納する配列\n",
        "x[0] = e[0] # 初期値\n",
        "for i in range(1, n):\n",
        "  x[i] = x[i-1] + e[i] # ランダムウォーク\n",
        "\n",
        "# データフレームに変換\n",
        "df = pd.DataFrame(x, columns=['value'])\n",
        "df.index = pd.date_range('2020-01-01', periods=n, freq='D') # 日付をインデックスに設定\n",
        "\n",
        "# 時系列データを教師あり学習用に変換する関数\n",
        "def series_to_supervised(data, n_in=1, n_out=1):\n",
        "  n_vars = data.shape[1] # 変数の数\n",
        "  cols = [] # 列名を格納するリスト\n",
        "  names = [] # 列名を格納するリスト\n",
        "  # 入力部分（X）\n",
        "  for i in range(n_in, 0, -1):\n",
        "    cols.append(data.shift(i)) # iステップ前のデータをずらして追加\n",
        "    names += [f'var{j+1}(t-{i})' for j in range(n_vars)] # 列名を追加\n",
        "  # 出力部分（y）\n",
        "  for i in range(0, n_out):\n",
        "    cols.append(data.shift(-i)) # iステップ後のデータをずらして追加\n",
        "    if i == 0:\n",
        "      names += [f'var{j+1}(t)' for j in range(n_vars)] # 列名を追加\n",
        "    else:\n",
        "      names += [f'var{j+1}(t+{i})' for j in range(n_vars)] # 列名を追加\n",
        "  # 入力部分と出力部分を結合\n",
        "  agg = pd.concat(cols, axis=1) # リストに格納したデータフレームを横に結合\n",
        "  agg.columns = names # 列名を設定\n",
        "  agg.dropna(inplace=True) # 欠損値がある行を削除\n",
        "  return agg\n",
        "\n",
        "# 時系列データを教師あり学習用に変換\n",
        "n_steps = 10 # ステップ数（過去のデータ数）\n",
        "n_features = 1 # 特徴量の数\n",
        "data = series_to_supervised(df, n_steps, 1) # 変換\n",
        "data = data.values # numpy配列に変換\n",
        "X = data[:, :-n_features] # 入力部分\n",
        "y = data[:, -n_features] # 出力部分\n",
        "X = X.reshape((X.shape[0], n_steps, n_features)) # LSTMに入力できる形に変形\n",
        "\n",
        "# データを訓練用とテスト用に分割\n",
        "train_size = int(n * 0.8) # 訓練用データのサイズ\n",
        "X_train = X[:train_size] # 訓練用入力データ\n",
        "y_train = y[:train_size] # 訓練用出力データ\n",
        "X_test = X[train_size:] # テスト用入力データ\n",
        "y_test = y[train_size:] # テスト用出力データ\n",
        "\n",
        "# LSTMモデルの構築\n",
        "model = Sequential() # モデルの初期化\n",
        "model.add(LSTM(50, input_shape=(n_steps, n_features))) # LSTM層（50個のニューロン）\n",
        "model.add(Dense(1)) # 全結合層（1個のニューロン）\n",
        "model.compile(loss='mean_squared_error', optimizer='adam') # モデルのコンパイル（損失関数と最適化方法の設定）\n",
        "\n",
        "# LSTMモデルの学習\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0) # モデルの学習（エポック数とバッチサイズの設定）\n",
        "\n",
        "# LSTMモデルによる予測\n",
        "y_pred = model.predict(X_test) # モデルにテスト用入力データを入力して予測値を得る\n",
        "\n",
        "# 予測結果のプロット\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test, label='Test')\n",
        "plt.plot(y_pred, label='Prediction')\n",
        "plt.title('LSTM Prediction for Random Walk')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "このように、LSTMモデルはランダムウォークを予測することができますが、ナイーブな予測モデルよりも良い性能ではないことがわかります。これは、ランダムウォークが過去の値に依存しないため、LSTMが学習する特徴やパターンがないことを反映しています。しかし、ランダムウォーク以外のランダム性の強い時系列データでは、LSTMが有効な場合もあります。例えば、以下のコードは、正弦波（sin波）にランダムな誤差を加えた時系列データを生成し、LSTMで予測するものです。\n",
        "```\n",
        "# 正弦波にランダムな誤差を加えた時系列データの生成\n",
        "np.random.seed(0) # 乱数のシードを固定\n",
        "n = 1000 # データ数\n",
        "t = np.linspace(0, 4*np.pi, n) # 時間軸\n",
        "x = np.sin(t) + np.random.normal(scale=0.1, size=n) # 正弦波にランダムな誤差を加える\n",
        "\n",
        "# データフレームに変換\n",
        "df = pd.DataFrame(x, columns=['value'])\n",
        "df.index = pd.date_range('2020-01-01', periods=n, freq='D') # 日付をインデックスに設定\n",
        "\n",
        "# データのプロット\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(df)\n",
        "plt.title('Sine Wave with Random Noise')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Value')\n",
        "plt.show()\n",
        "# 時系列データを教師あり学習用に変換\n",
        "n_steps = 10 # ステップ数（過去のデータ数）\n",
        "n_features = 1 # 特徴量の数\n",
        "data = series_to_supervised(df, n_steps, 1) # 変換\n",
        "data = data.values # numpy配列に変換\n",
        "X = data[:, :-n_features] # 入力部分\n",
        "y = data[:, -n_features] # 出力部分\n",
        "X = X.reshape((X.shape[0], n_steps, n_features)) # LSTMに入力できる形に変形\n",
        "\n",
        "# データを訓練用とテスト用に分割\n",
        "train_size = int(n * 0.8) # 訓練用データのサイズ\n",
        "X_train = X[:train_size] # 訓練用入力データ\n",
        "y_train = y[:train_size] # 訓練用出力データ\n",
        "X_test = X[train_size:] # テスト用入力データ\n",
        "y_test = y[train_size:] # テスト用出力データ\n",
        "\n",
        "# LSTMモデルの構築\n",
        "model = Sequential() # モデルの初期化\n",
        "model.add(LSTM(50, input_shape=(n_steps, n_features))) # LSTM層（50個のニューロン）\n",
        "model.add(Dense(1)) # 全結合層（1個のニューロン）\n",
        "model.compile(loss='mean_squared_error', optimizer='adam') # モデルのコンパイル（損失関数と最適化方法の設定）\n",
        "\n",
        "# LSTMモデルの学習\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0) # モデルの学習（エポック数とバッチサイズの設定）\n",
        "\n",
        "# LSTMモデルによる予測\n",
        "y_pred = model.predict(X_test) # モデルにテスト用入力データを入力して予測値を得る\n",
        "\n",
        "# 予測結果のプロット\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test, label='Test')\n",
        "plt.plot(y_pred, label='Prediction')\n",
        "plt.title('LSTM Prediction for Sine Wave with Random Noise')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "このように、LSTMモデルは正弦波にランダムな誤差を加えた時系列データを予測することができます。この場合、LSTMは正弦波の周期性や振幅などの特徴やパターンを学習することができます。このモデルの性能を評価するために、以下のコードで平均絶対誤差（MAE）と決定係数（R2）を計算します。\n",
        "```\n",
        "# MAEとR2の計算\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "mae = mean_absolute_error(y_test, y_pred) # MAE\n",
        "r2 = r2_score(y_test, y_pred) # R2\n",
        "print(f'MAE: {mae:.2f}')\n",
        "print(f'R2: {r2:.2f}')\n",
        "```\n",
        "\n",
        "MAE: 0.11\n",
        "R2: 0.88\n",
        "\n",
        "MAEは誤差の大きさを表す指標で、小さいほど良いです。R2は予測値が実際の値にどれだけ近いかを表す指標で、1に近いほど良いです。この結果から、LSTMモデルは正弦波にランダムな誤差を加えた時系列データに対してかなり良い性能を示していることがわかります。これは、LSTMが時系列データの長期的な依存関係や周期性を捉えることができることを反映しています。\n",
        "\n",
        "- **ベイズ的手法**：ベイズ的（Bayesian）という手法を使って、ランダム性の強い時系列データを予測することができます。ベイズ的とは、確率的な推論を行うことで、不確実性や不完全性を考慮する手法です。ベイズ的な手法では、時系列データに対して確率分布やパラメータを仮定し、観測されたデータからその分布やパラメータの事後分布（観測後の確信度）を求めます。そして、事後分布から予測分布（未来の値の確信度）を導出します。Pythonでは、PyMC3やPyStanというライブラリを使ってベイズ的な手法を実装することができます。例えば、以下のコードは、PyMC3を使って、ランダムウォークに対してベイズ的な予測モデルを構築するものです。\n",
        "\n",
        "```\n",
        "# ライブラリのインポート\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pymc3 as pm\n",
        "\n",
        "# ランダムウォークの生成\n",
        "np.random.seed(0) # 乱数のシードを固定\n",
        "n = 1000 # データ数\n",
        "e = np.random.normal(size=n) # ランダムな誤差\n",
        "x = np.zeros(n) # データを格納する配列\n",
        "x[0] = e[0] # 初期値\n",
        "for i in range(1, n):\n",
        "  x[i] = x[i-1] + e[i] # ランダムウォーク\n",
        "\n",
        "# データフレームに変換\n",
        "df = pd.DataFrame(x, columns=['value'])\n",
        "df.index = pd.date_range('2020-01-01', periods=n, freq='D') # 日付をインデックスに設定\n",
        "\n",
        "# データを訓練用とテスト用に分割\n",
        "train_size = int(n * 0.8) # 訓練用データのサイズ\n",
        "train = df[:train_size] # 訓練用データ\n",
        "test = df[train_size:] # テスト用データ\n",
        "\n",
        "# ベイズ的な予測モデルの構築\n",
        "with pm.Model() as model: # モデルの初期化\n",
        "  sigma = pm.HalfNormal('sigma', sd=1) # 誤差項の標準偏差（半正規分布）\n",
        "  s = pm.GaussianRandomWalk('s', sd=sigma, shape=len(train)) # ランダムウォーク（ガウス型）\n",
        "  obs = pm.Normal('obs', mu=s, sd=1, observed=train['value']) # 観測値（正規分布）\n",
        "  trace = pm.sample(1000, tune=500) # サンプリング（事後分布の近似）\n",
        "\n",
        "# 事後分布の要約とプロット\n",
        "pm.summary(trace) # 事後分布の要約\n",
        "pm.traceplot(trace) # 事後分布のプロット\n",
        "plt.show()\n",
        "```\n",
        "ソース: Bing との会話 2023/6/21\n",
        "(1) Random Forest for Time Series Forecasting - Machine .... https://machinelearningmastery.com/random-forest-for-time-series-forecasting/.\n",
        "(2) Time series forecasting | TensorFlow Core. https://www.tensorflow.org/tutorials/structured_data/time_series.\n",
        "(3) A Gentle Introduction to the Random Walk for Times Series .... https://machinelearningmastery.com/gentle-introduction-random-walk-times-series-forecasting-python/."
      ],
      "metadata": {
        "id": "9t4e2xQ-ivVM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "「GitHub」タブで「run.ipynb」というファイルを見つけることができない場合は、以下の方法でGoogle Colab上で実行できます。\n",
        "\n",
        "- Google Colabのノートブックを新規作成する。\n",
        "- ノートブックの最初のセルに以下のコマンドを入力して実行する。これは、GitHubからTime-Series-Libraryというリポジトリ（プログラムの集まり）をコピーするコマンドです。\n",
        "```\n",
        "!git clone https://github.com/thuml/Time-Series-Library.git\n",
        "```\n",
        "- 次のセルに以下のコマンドを入力して実行する。これは、コピーしたリポジトリの中にあるrun.ipynbというファイルを開くコマンドです。\n",
        "```\n",
        "%cd Time-Series-Library\n",
        "%run run.ipynb\n",
        "```\n",
        "- run.ipynbというファイルが開いたら、その中にあるセルを順番に実行していく。これは、プログラムの設定やデータの読み込みやモデルの学習や予測などを行うセルです。\n",
        "- 最後のセルでは、予測結果がグラフで表示されます。これは、プログラムが時系列データに対してどのような予測を行ったかを視覚的に確認できるグラフです。"
      ],
      "metadata": {
        "id": "_C1gkxkig6_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/thuml/Time-Series-Library.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNcDjyKTbDwx",
        "outputId": "33146889-1d4c-4491-b9f3-0dc52f22ba58"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Time-Series-Library'...\n",
            "remote: Enumerating objects: 812, done.\u001b[K\n",
            "remote: Counting objects: 100% (355/355), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 812 (delta 320), reused 294 (delta 286), pack-reused 457\u001b[K\n",
            "Receiving objects: 100% (812/812), 11.06 MiB | 23.90 MiB/s, done.\n",
            "Resolving deltas: 100% (594/594), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install /content/Time-Series-Library/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dvm5WC9biBB0",
        "outputId": "19534574-3831-4cd6-9ed9-8844ca4de605"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: '/content/Time-Series-Library/requirements.txt'\n",
            "Hint: It looks like a path. The path does exist. The argument you provided (/content/Time-Series-Library/requirements.txt) appears to be a requirements file. If that is the case, use the '-r' flag to install the packages specified within it.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko96YRabhYoj",
        "outputId": "461317a3-f849-448a-ad0c-5164fe700d0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Time-Series-Library\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd Time-Series-Library\n",
        "%run run.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "8OB6SW0vhCWL",
        "outputId": "33962a90-b7be-4833-faa6-edae6f6bd61f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'Time-Series-Library'\n",
            "/content/Time-Series-Library\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/content/Time-Series-Library/run.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_long_term_forecasting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExp_Long_Term_Forecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_imputation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExp_Imputation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_short_term_forecasting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExp_Short_Term_Forecast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Time-Series-Library/exp/exp_long_term_forecasting.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_provider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_factory\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_provider\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp_basic\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExp_Basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjust_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Time-Series-Library/data_provider/data_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_provider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset_ETT_hour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset_ETT_minute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset_Custom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset_M4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPSMSegLoader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mMSLSegLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSMAPSegLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSMDSegLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSWATSegLoader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUEAloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_provider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muea\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollate_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Time-Series-Library/data_provider/data_loader.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimefeatures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_provider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mm4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mM4Dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM4Meta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_provider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muea\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msubsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolate_missing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msktime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_from_tsfile_to_dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Time-Series-Library/data_provider/m4.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpatoolib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'patoolib'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WIze_Dt8hO4S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}