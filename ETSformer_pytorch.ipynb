{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNsMw4KBZ8+ts9qj7bXI9T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumajirou/-daily-test/blob/main/ETSformer_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # **ライブラリのインストール**\n",
        "#einopsライブラリは、テンソルを操作したり並べ替えたりするための関数やクラスを多数提供する。\n",
        "\n",
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQvYy3EnqPhE",
        "outputId": "5211b847-6ad1-4b4a-cde6-36b71cf1d5f5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 509 kB/s \n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "D-OxhSmen391"
      },
      "outputs": [],
      "source": [
        "#@title # **ライブラリのインポート**\n",
        "#mathモジュールには、数学の定数π（約3.14）を表すπをはじめ、様々な数学関数が含まれています\n",
        "from math import pi\n",
        "\n",
        "#コレクションモジュールには、データのコレクションを扱うのに便利なデータ型が多数用意されており、namedtupleは、名前付きフィールドを持つtupleのサブクラスを作成することができる\n",
        "from collections import namedtuple\n",
        "\n",
        "#torchモジュールは、Pythonの深層学習ライブラリとして人気のあるPyTorchの一部です。これはテンソル（多次元配列）を扱い、それに対して様々な操作を行うための関数やクラスを提供します。\n",
        "import torch\n",
        "\n",
        "#nn.functionalモジュールもPyTorchの一部で、テンソルにニューラルネットワークの演算を適用するための関数を多数提供しています\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#einsum関数は、アインシュタイン和算を行うためのユーティリティ関数であり、複数のインデックスに対する和をテンソル表記で簡潔に表現することができる。\n",
        "from torch import nn, einsum\n",
        "\n",
        "#scipy.fftpackモジュールは高速フーリエ変換 (FFT) を扱うための関数を提供します\n",
        "#next_fast_len関数は、FFTの次の高速な長さを返します。\n",
        "#これは、FFTアルゴリズムを使って計算するのがより速い長さです。\n",
        "from scipy.fftpack import next_fast_len\n",
        "\n",
        "#einopsライブラリは、テンソルを操作したり並べ替えたりするための関数やクラスを多数提供する。\n",
        "#rearrange関数はテンソルの次元を並べ替えることができ、\n",
        "#repeat関数はテンソルを1つまたは複数の次元に沿って繰り返すことができる。\n",
        "from einops import rearrange, repeat\n",
        "\n",
        "#Rearrangeクラスは、入力にrearrange関数を適用するPyTorchレイヤーです。\n",
        "from einops.layers.torch import Rearrange\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title  # **constants(定数)**\n",
        "\n",
        "Intermediates = namedtuple('Intermediates', ['growth_latents', 'seasonal_latents', 'level_output'])\n",
        "print(Intermediates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0MmfWawqMZt",
        "outputId": "b7e3685e-3dde-4d8a-fa0e-a1f918906f43"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.Intermediates'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ChatGPTによるconstants(定数)コードの解説**\n",
        "\n",
        "\n",
        "- Intermediatesは、growth_latents、seasonal_latents、level_outputの3つのフィールドを持つ名前付きタプルのように見えます。\n",
        "- 名前付きタプルはフィールドに名前を付けたタプルのサブクラスで、Pythonの有効な識別子を使うことができます。\n",
        "- 名前付きタプルは、辞書に似ていますが、より簡潔な構文と属性によってフィールドにアクセスする機能を持つ、単純なレコードを表現する便利な方法です。\n",
        "\n",
        "例えば、次のような名前付きタプルを作成することができます。"
      ],
      "metadata": {
        "id": "l2sx6G16qo3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ##**constants(定数)コードの説明のため生成したChatGPTのコード**\n",
        "from collections import namedtuple\n",
        "\n",
        "Intermediates = namedtuple('Intermediates', ['growth_latents', 'seasonal_latents', 'level_output'])\n",
        "\n",
        "intermediates = Intermediates(growth_latents=1, seasonal_latents=2, level_output=3)\n",
        "\n",
        "print(intermediates.growth_latents)  # prints 1\n",
        "print(intermediates.seasonal_latents)  # prints 2\n",
        "print(intermediates.level_output)  # prints 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehHRoqY7rYFU",
        "outputId": "d2a7580f-0c5d-46f6-89a8-7626402c6fc9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "また、通常のタプルと同様に、インデックスを使用して名前付きタプルのフィールドにアクセスすることができます。"
      ],
      "metadata": {
        "id": "pMETZYuRrLFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## **constants(定数)コードの説明のため生成したChatGPTのコード**\n",
        "print(intermediates[0])  # prints 1\n",
        "print(intermediates[1])  # prints 2\n",
        "print(intermediates[2])  # prints 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPMtyBb9rdI5",
        "outputId": "3b1f1b8d-bd78-4f8c-e080-fa38909cece8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 名前付きタプルは不変であり、作成後にそのフィールドの値を変更することはできません。\n",
        "- フィールドの値を変更する必要がある場合は、更新された値で新しい名前付きタプルを作成することができます。"
      ],
      "metadata": {
        "id": "BzyIvD8mrLSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title # **helper functions(ヘルパー関数)**\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n"
      ],
      "metadata": {
        "id": "vBDmjJmfqbPj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ChatGPTによるhelper functions(ヘルパー関数)コードの解説**\n",
        "- これは、与えられた値がNoneでなければTrueを、そうでなければFalseを返す単純なユーティリティ関数のように見える。\n",
        "\n",
        "例えば、こんな感じです。"
      ],
      "metadata": {
        "id": "VYXUBLh7sLEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## **helper functions(ヘルパー関数)コードの説明のため生成したChatGPTのコード**\n",
        "val = None\n",
        "print(exists(val))  # False\n",
        "\n",
        "val = \"hello\"\n",
        "print(exists(val))  # True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygTmYhj6sNU7",
        "outputId": "b6f2b192-6f74-4e4f-9f56-cfd610efc1b9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- この関数は、0,\"\",[],{}のような値に対しても真を返すことに注意してください、\n",
        "- なぜならこれらはすべてPythonでは非Noneの値とみなされるからです。\n",
        "- Noneだけをチェックしたいのであれば、代わりにis演算子を使用することができます。"
      ],
      "metadata": {
        "id": "bUuidnQbsLYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## **helper functions(ヘルパー関数)コードの説明のため生成したChatGPTのコード**\n",
        "\n",
        "val = 0\n",
        "print(val is None)  # False\n",
        "\n",
        "val = None\n",
        "print(val is None)  # True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-rVgOqCsN-B",
        "outputId": "6c98570b-9704-466e-cdbb-90263e534d4a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@title # **fourier helpers(フーリエヘルパー関数)**\n",
        "\n",
        "def fourier_extrapolate(signal, start, end):\n",
        "    device = signal.device\n",
        "    fhat = torch.fft.fft(signal)\n",
        "    fhat_len = fhat.shape[-1]\n",
        "    time = torch.linspace(start, end - 1, end - start, device = device, dtype = torch.complex64)\n",
        "    freqs = torch.linspace(0, fhat_len - 1, fhat_len, device = device, dtype = torch.complex64)\n",
        "    res = fhat[..., None, :] * (1.j * 2 * pi * freqs[..., None, :] * time[..., :, None] / fhat_len).exp() / fhat_len\n",
        "    return res.sum(dim = -1).real\n"
      ],
      "metadata": {
        "id": "ijUGZe-zrgNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ChatGPTによるfourier helpers(フーリエヘルパー関数)コードの解説**\n",
        "- この関数は、離散フーリエ変換（DFT）とその逆である逆DFT（IDFT）を実装し、与えられた信号シグナルを 開始から 終了までの時間範囲に渡って外挿するようなものである。\n",
        "\n",
        "- DFTは信号を周波数成分に分解する数学的変換であり、IDFTはその逆で周波数成分から信号を再構成する変換である。\n",
        "- DFTは次のように定義される。\n",
        "\n",
        " - $X[k] = \\sum_{n=0}^{N-1} x[n] e^{-j \\frac{2}{N} kn}$\n",
        "\n",
        "- ここで、$x[n]$は入力信号、$X[k]$は出力周波数スペクトル、$N$は信号のサンプル数である。IDFTは逆変換として定義される。\n",
        "\n",
        " - $x[n] = \\frac{1}{N}\\X[k] e^{j ∕frac{2∕pi}{N} kn}$\n",
        "\n",
        "- この関数では，入力信号の周波数スペクトルを複素数値のテンソルとして返すtorch.fft.fft関数を用いて DFT を計算します。\n",
        "- これは $(1.j * 2 * pi * freqs[..., None, :] * time[..., :, None] / fhat_len)$ として定義され、最後の次元で sum(dim = -1) を使って合計を取ることによって計算されます。\n",
        "- そして、結果の実数部はrealを使って返されます。\n",
        "\n",
        "- 外挿は時間軸の範囲を始点から 終点まで拡張することで行われ、これにより再構成された信号は入力信号の元の範囲を超えて拡張される。\n",
        "- 拡張された時間軸はtorch.linspace関数を使って作成され、与えられた始点と終点の間に等間隔の値を生成します\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ChlskKdquZlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# classes\n",
        "\n",
        "def InputEmbedding(time_features, model_dim, kernel_size = 3, dropout = 0.):\n",
        "    return nn.Sequential(\n",
        "        Rearrange('b n d -> b d n'),\n",
        "        nn.Conv1d(time_features, model_dim, kernel_size = kernel_size, padding = kernel_size // 2),\n",
        "        nn.Dropout(dropout),\n",
        "        Rearrange('b d n -> b n d'),\n",
        "    )\n",
        "\n",
        "def FeedForward(dim, mult = 4, dropout = 0.):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(dim, dim * mult),\n",
        "        nn.Sigmoid(),\n",
        "        nn.Dropout(dropout),\n",
        "        nn.Linear(dim * mult, dim),\n",
        "        nn.Dropout(dropout)\n",
        "    )\n",
        "\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.ff = FeedForward(dim, **kwargs)\n",
        "        self.post_norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.post_norm(x + self.ff(x))\n",
        "\n",
        "# encoder related classes\n",
        "\n",
        "## multi-head exponential smoothing attention\n",
        "\n",
        "def conv1d_fft(x, weights, dim = -2, weight_dim = -1):\n",
        "    # Algorithm 3 in paper\n",
        "\n",
        "    N = x.shape[dim]\n",
        "    M = weights.shape[weight_dim]\n",
        "\n",
        "    fast_len = next_fast_len(N + M - 1)\n",
        "\n",
        "    f_x = torch.fft.rfft(x, n = fast_len, dim = dim)\n",
        "    f_weight = torch.fft.rfft(weights, n = fast_len, dim = weight_dim)\n",
        "\n",
        "    f_v_weight = f_x * rearrange(f_weight.conj(), '... -> ... 1')\n",
        "    out = torch.fft.irfft(f_v_weight, fast_len, dim = dim)\n",
        "    out = out.roll(-1, dims = (dim,))\n",
        "\n",
        "    indices = torch.arange(start = fast_len - N, end = fast_len, dtype = torch.long, device = x.device)\n",
        "    out = out.index_select(dim, indices)\n",
        "    return out\n",
        "\n",
        "class MHESA(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        dim,\n",
        "        heads = 8,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.initial_state = nn.Parameter(torch.randn(heads, dim // heads))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.alpha = nn.Parameter(torch.randn(heads))\n",
        "\n",
        "        self.project_in = nn.Linear(dim, dim)\n",
        "        self.project_out = nn.Linear(dim, dim)\n",
        "\n",
        "    def naive_Aes(self, x, weights):\n",
        "        n, h = x.shape[-2], self.heads\n",
        "\n",
        "        # in appendix A.1 - Algorithm 2\n",
        "\n",
        "        arange = torch.arange(n, device = x.device)\n",
        "\n",
        "        weights = repeat(weights, '... l -> ... t l', t = n)\n",
        "        indices = repeat(arange, 'l -> h t l', h = h, t = n)\n",
        "\n",
        "        indices = (indices - rearrange(arange + 1, 't -> 1 t 1')) % n\n",
        "\n",
        "        weights = weights.gather(-1, indices)\n",
        "        weights = self.dropout(weights)\n",
        "\n",
        "        # causal\n",
        "\n",
        "        weights = weights.tril()\n",
        "\n",
        "        # multiply\n",
        "\n",
        "        output = einsum('b h n d, h m n -> b h m d', x, weights)\n",
        "        return output\n",
        "\n",
        "    def forward(self, x, naive = False):\n",
        "        b, n, d, h, device = *x.shape, self.heads, x.device\n",
        "\n",
        "        # linear project in\n",
        "\n",
        "        x = self.project_in(x)\n",
        "\n",
        "        # split out heads\n",
        "\n",
        "        x = rearrange(x, 'b n (h d) -> b h n d', h = h)\n",
        "\n",
        "        # temporal difference\n",
        "\n",
        "        x = torch.cat((\n",
        "            repeat(self.initial_state, 'h d -> b h 1 d', b = b),\n",
        "            x\n",
        "        ), dim = -2)\n",
        "\n",
        "        x = x[:, :, 1:] - x[:, :, :-1]\n",
        "\n",
        "        # prepare exponential alpha\n",
        "\n",
        "        alpha = self.alpha.sigmoid()\n",
        "        alpha = rearrange(alpha, 'h -> h 1')\n",
        "\n",
        "        # arange == powers\n",
        "\n",
        "        arange = torch.arange(n, device = device)\n",
        "        weights = alpha * (1 - alpha) ** torch.flip(arange, dims = (0,))\n",
        "\n",
        "        if naive:\n",
        "            output = self.naive_Aes(x, weights)\n",
        "        else:\n",
        "            output = conv1d_fft(x, weights)\n",
        "\n",
        "        # get initial state contribution\n",
        "\n",
        "        init_weight = (1 - alpha) ** (arange + 1)\n",
        "        init_output = rearrange(init_weight, 'h n -> h n 1') * rearrange(self.initial_state, 'h d -> h 1 d')\n",
        "\n",
        "        output = output + init_output\n",
        "\n",
        "        # merge heads\n",
        "\n",
        "        output = rearrange(output, 'b h n d -> b n (h d)')\n",
        "        return self.project_out(output)\n",
        "\n",
        "## frequency attention\n",
        "\n",
        "class FrequencyAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        K = 4,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.K = K\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        freqs = torch.fft.rfft(x, dim = 1)\n",
        "\n",
        "        # get amplitudes\n",
        "\n",
        "        amp = freqs.abs()\n",
        "        amp = self.dropout(amp)\n",
        "\n",
        "        # topk amplitudes - for seasonality, branded as attention\n",
        "\n",
        "        topk_amp, _ = amp.topk(k = self.K, dim = 1, sorted = True)\n",
        "\n",
        "        # mask out all freqs with lower amplitudes than the lowest value of the topk above\n",
        "\n",
        "        topk_freqs = freqs.masked_fill(amp < topk_amp[:, -1:], 0.+0.j)\n",
        "\n",
        "        # inverse fft\n",
        "\n",
        "        return torch.fft.irfft(topk_freqs, dim = 1)\n",
        "\n",
        "## level module\n",
        "\n",
        "class Level(nn.Module):\n",
        "    def __init__(self, time_features, model_dim):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.Tensor([0.]))\n",
        "        self.to_growth = nn.Linear(model_dim, time_features)\n",
        "        self.to_seasonal = nn.Linear(model_dim, time_features)\n",
        "\n",
        "    def forward(self, x, latent_growth, latent_seasonal):\n",
        "        # following equation in appendix A.2\n",
        "\n",
        "        n, device = x.shape[1], x.device\n",
        "\n",
        "        alpha = self.alpha.sigmoid()\n",
        "\n",
        "        arange = torch.arange(n, device = device)\n",
        "        powers = torch.flip(arange, dims = (0,))\n",
        "\n",
        "        # Aes for raw time series signal with seasonal terms (from frequency attention) subtracted out\n",
        "\n",
        "        seasonal =self.to_seasonal(latent_seasonal)\n",
        "        Aes_weights = alpha * (1 - alpha) ** powers\n",
        "        seasonal_normalized_term = conv1d_fft(x - seasonal, Aes_weights)\n",
        "\n",
        "        # auxiliary term\n",
        "\n",
        "        growth = self.to_growth(latent_growth)\n",
        "        growth_smoothing_weights = (1 - alpha) ** powers\n",
        "        growth_term = conv1d_fft(growth, growth_smoothing_weights)\n",
        "\n",
        "        return seasonal_normalized_term + growth_term\n",
        "\n",
        "# decoder classes\n",
        "\n",
        "class LevelStack(nn.Module):\n",
        "    def forward(self, x, num_steps_forecast):\n",
        "        return repeat(x[:, -1], 'b d -> b n d', n = num_steps_forecast)\n",
        "\n",
        "class GrowthDampening(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        heads = 8\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.heads = heads\n",
        "        self.dampen_factor = nn.Parameter(torch.randn(heads))\n",
        "\n",
        "    def forward(self, growth, *, num_steps_forecast):\n",
        "        device, h = growth.device, self.heads\n",
        "\n",
        "        dampen_factor = self.dampen_factor.sigmoid()\n",
        "\n",
        "        # like level stack, it takes the last growth for forecasting\n",
        "\n",
        "        last_growth = growth[:, -1]\n",
        "        last_growth = rearrange(last_growth, 'b l (h d) -> b l 1 h d', h = h)\n",
        "\n",
        "        # prepare dampening factors per head and the powers\n",
        "\n",
        "        dampen_factor = rearrange(dampen_factor, 'h -> 1 1 1 h 1')\n",
        "        powers = (torch.arange(num_steps_forecast, device = device) + 1)\n",
        "        powers = rearrange(powers, 'n -> 1 1 n 1 1')\n",
        "\n",
        "        # following Eq(2) in the paper\n",
        "\n",
        "        dampened_growth = last_growth * (dampen_factor ** powers).cumsum(dim = 2)\n",
        "        return rearrange(dampened_growth, 'b l n h d -> b l n (h d)')\n",
        "\n",
        "# main class\n",
        "\n",
        "class ETSFormer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        model_dim,\n",
        "        time_features = 1,\n",
        "        embed_kernel_size = 3,\n",
        "        layers = 2,\n",
        "        heads = 8,\n",
        "        K = 4,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert (model_dim % heads) == 0, 'model dimension must be divisible by number of heads'\n",
        "        self.model_dim = model_dim\n",
        "        self.time_features = time_features\n",
        "\n",
        "        self.embed = InputEmbedding(time_features, model_dim, kernel_size = embed_kernel_size, dropout = dropout)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([])\n",
        "\n",
        "        for ind in range(layers):\n",
        "            is_last_layer = ind == (layers - 1)\n",
        "\n",
        "            self.encoder_layers.append(nn.ModuleList([\n",
        "                FrequencyAttention(K = K, dropout = dropout),\n",
        "                MHESA(dim = model_dim, heads = heads, dropout = dropout),\n",
        "                FeedForwardBlock(dim = model_dim) if not is_last_layer else None,\n",
        "                Level(time_features = time_features, model_dim = model_dim)\n",
        "            ]))\n",
        "\n",
        "        self.growth_dampening_module = GrowthDampening(dim = model_dim, heads = heads)\n",
        "\n",
        "        self.latents_to_time_features = nn.Linear(model_dim, time_features)\n",
        "        self.level_stack = LevelStack()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        *,\n",
        "        num_steps_forecast = 0,\n",
        "        return_latents = False\n",
        "    ):\n",
        "        one_time_feature = x.ndim == 2\n",
        "\n",
        "        if one_time_feature:\n",
        "            x = rearrange(x, 'b n -> b n 1')\n",
        "\n",
        "        z = self.embed(x)\n",
        "\n",
        "        latent_growths = []\n",
        "        latent_seasonals = []\n",
        "\n",
        "        for freq_attn, mhes_attn, ff_block, level in self.encoder_layers:\n",
        "            latent_seasonal = freq_attn(z)\n",
        "            z = z - latent_seasonal\n",
        "\n",
        "            latent_growth = mhes_attn(z)\n",
        "            z = z - latent_growth\n",
        "\n",
        "            if exists(ff_block):\n",
        "                z = ff_block(z)\n",
        "\n",
        "            x = level(x, latent_growth, latent_seasonal)\n",
        "\n",
        "            latent_growths.append(latent_growth)\n",
        "            latent_seasonals.append(latent_seasonal)\n",
        "\n",
        "        latent_growths = torch.stack(latent_growths, dim = -2)\n",
        "        latent_seasonals = torch.stack(latent_seasonals, dim = -2)\n",
        "\n",
        "        latents = Intermediates(latent_growths, latent_seasonals, x)\n",
        "\n",
        "        if num_steps_forecast == 0:\n",
        "            return latents\n",
        "\n",
        "        latent_seasonals = rearrange(latent_seasonals, 'b n l d -> b l d n')\n",
        "        extrapolated_seasonals = fourier_extrapolate(latent_seasonals, x.shape[1], x.shape[1] + num_steps_forecast)\n",
        "        extrapolated_seasonals = rearrange(extrapolated_seasonals, 'b l d n -> b l n d')\n",
        "\n",
        "        dampened_growths = self.growth_dampening_module(latent_growths, num_steps_forecast = num_steps_forecast)\n",
        "        level = self.level_stack(x, num_steps_forecast = num_steps_forecast)\n",
        "\n",
        "        summed_latents = dampened_growths.sum(dim = 1) + extrapolated_seasonals.sum(dim = 1)\n",
        "        forecasted = level + self.latents_to_time_features(summed_latents)\n",
        "\n",
        "        if one_time_feature:\n",
        "            forecasted = rearrange(forecasted, 'b n 1 -> b n')\n",
        "\n",
        "        if return_latents:\n",
        "            return forecasted, latents\n",
        "\n",
        "        return forecasted\n",
        "\n",
        "# classification wrapper\n",
        "\n",
        "class MultiheadLayerNorm(nn.Module):\n",
        "    def __init__(self, dim, heads = 1, eps = 1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.g = nn.Parameter(torch.ones(heads, 1, dim))\n",
        "        self.b = nn.Parameter(torch.zeros(heads, 1, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        std = torch.var(x, dim = -1, unbiased = False, keepdim = True).sqrt()\n",
        "        mean = torch.mean(x, dim = -1, keepdim = True)\n",
        "        return (x - mean) / (std + self.eps) * self.g + self.b\n",
        "\n",
        "class ClassificationWrapper(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        etsformer,\n",
        "        num_classes = 10,\n",
        "        heads = 16,\n",
        "        dim_head = 32,\n",
        "        level_kernel_size = 3,\n",
        "        growth_kernel_size = 3,\n",
        "        seasonal_kernel_size = 3,\n",
        "        dropout = 0.\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert isinstance(etsformer, ETSFormer)\n",
        "        self.etsformer = etsformer\n",
        "        model_dim = etsformer.model_dim\n",
        "        time_features = etsformer.time_features\n",
        "\n",
        "        inner_dim = dim_head * heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.queries = nn.Parameter(torch.randn(heads, dim_head))\n",
        "\n",
        "        self.growth_to_kv = nn.Sequential(\n",
        "            Rearrange('b n d -> b d n'),\n",
        "            nn.Conv1d(model_dim, inner_dim * 2, growth_kernel_size, bias = False, padding = growth_kernel_size // 2),\n",
        "            Rearrange('... (kv h d) n -> ... (kv h) n d', kv = 2, h = heads),\n",
        "            MultiheadLayerNorm(dim_head, heads = 2 * heads),\n",
        "        )\n",
        "\n",
        "        self.seasonal_to_kv = nn.Sequential(\n",
        "            Rearrange('b n d -> b d n'),\n",
        "            nn.Conv1d(model_dim, inner_dim * 2, seasonal_kernel_size, bias = False, padding = seasonal_kernel_size // 2),\n",
        "            Rearrange('... (kv h d) n -> ... (kv h) n d', kv = 2, h = heads),\n",
        "            MultiheadLayerNorm(dim_head, heads = 2 * heads),\n",
        "        )\n",
        "\n",
        "        self.level_to_kv = nn.Sequential(\n",
        "            Rearrange('b n t -> b t n'),\n",
        "            nn.Conv1d(time_features, inner_dim * 2, level_kernel_size, bias = False, padding = level_kernel_size // 2),\n",
        "            Rearrange('b (kv h d) n -> b (kv h) n d', kv = 2, h = heads),\n",
        "            MultiheadLayerNorm(dim_head, heads = 2 * heads),\n",
        "        )\n",
        "\n",
        "        self.to_out = nn.Linear(inner_dim, model_dim)\n",
        "\n",
        "        self.to_logits = nn.Sequential(\n",
        "            nn.LayerNorm(model_dim),\n",
        "            nn.Linear(model_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, timeseries):\n",
        "        latent_growths, latent_seasonals, level_output = self.etsformer(timeseries)\n",
        "\n",
        "        latent_growths = latent_growths.mean(dim = -2)\n",
        "        latent_seasonals = latent_seasonals.mean(dim = -2)\n",
        "\n",
        "        # queries, key, values\n",
        "\n",
        "        q = self.queries * self.scale\n",
        "\n",
        "        kvs = torch.cat((\n",
        "            self.growth_to_kv(latent_growths),\n",
        "            self.seasonal_to_kv(latent_seasonals),\n",
        "            self.level_to_kv(level_output)\n",
        "        ), dim = -2)\n",
        "\n",
        "        k, v = kvs.chunk(2, dim = 1)\n",
        "\n",
        "        # cross attention pooling\n",
        "\n",
        "        sim = einsum('h d, b h j d -> b h j', q, k)\n",
        "        sim = sim - sim.amax(dim = -1, keepdim = True).detach()\n",
        "\n",
        "        attn = sim.softmax(dim = -1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        out = einsum('b h j, b h j d -> b h d', attn, v)\n",
        "        out = rearrange(out, 'b ... -> b (...)')\n",
        "\n",
        "        out = self.to_out(out)\n",
        "\n",
        "        # project to logits\n",
        "\n",
        "        return self.to_logits(out)"
      ],
      "metadata": {
        "id": "jrKp7JaVt6c-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cMJTvk0qn9az"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}