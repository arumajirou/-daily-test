{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumajirou/-daily-test/blob/main/annotated_diffusion_%E6%B3%A8%E9%87%88%E4%BB%98%E3%81%8D_%E6%99%AE%E5%8F%8A%E3%83%A2%E3%83%87%E3%83%AB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5a94671",
      "metadata": {
        "id": "c5a94671"
      },
      "source": [
        "<h1>\n",
        "\t注釈付き 普及モデル\n",
        "</h1>\n",
        "\n",
        "\n",
        "<div class=\"author-card\">\n",
        "    <a href=\"/nielsr\">\n",
        "        <img class=\"avatar avatar-user\" src=\"https://avatars.githubusercontent.com/u/48327001?v=4\" width=\"100\" title=\"Gravatar\">\n",
        "        <div class=\"bfc\">\n",
        "            <code>nielsr</code>\n",
        "            <span class=\"fullname\">Niels Rogge</span>\n",
        "        </div>\n",
        "    </a>\n",
        "    <a href=\"/kashif\">\n",
        "        <img class=\"avatar avatar-user\" src=\"https://avatars.githubusercontent.com/u/8100?v=4\" width=\"100\" title=\"Gravatar\">\n",
        "        <div class=\"bfc\">\n",
        "            <code>kashif</code>\n",
        "            <span class=\"fullname\">Kashif Rasul</span>\n",
        "        </div>\n",
        "    </a>\n",
        "    \n",
        "</div>\n",
        "\n",
        "<script async defer src=\"https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js\"></script>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "290edb0b",
      "metadata": {
        "id": "290edb0b"
      },
      "source": [
        "このブログでは、DenoisingDiffusion ProbabilisticModels（DDPM、ノイズ除去拡散確率モデル ,拡散モデル、スコアベース生成モデル、または単に[オートエンコーダー](https://benanne.github.io/2022/01/31/diffusion.html)としても知られています）について深く見ていきます。研究者は、**（無）条件画像／音声／動画生成**のために、これらのモデルを使って顕著な結果を出すことができました。代表的な例としては、**OpenAIの[GLIDE](https://arxiv.org/abs/2112.10741)や [DALL-E 2](https://openai.com/dall-e-2/)**、**ハイデルベルク大学の[Latent Diffusion](https://github.com/CompVis/latent-diffusion)**、**Google Brainの[ImageGen](https://imagen.research.google/)など**があります（記事執筆時）。\n",
        "\n",
        "[(Ho et al., 2020)](https://arxiv.org/abs/2006.11239)によるオリジナルのDDPMの論文を、[Phil Wangの実装](https://github.com/lucidrains/denoising-diffusion-pytorch)-それ自体は[オリジナルのTensorFlowの実装](https://github.com/hojonathanho/diffusion)をベースに、PyTorchでステップバイステップで実装しながら見ていきます。なお、生成モデリングのための拡散の考え方は、実は[(Sohl-Dickstein et al., 2015)](https://arxiv.org/abs/1503.03585)で既に紹介されていました。しかし、このアプローチを独自に改良したのは、[(Song et al., 2019)](https://arxiv.org/abs/1907.05600) (at Stanford University)、そして[(Ho et al., 2020)](https://arxiv.org/abs/2006.11239) (at Google Brain) まででした。\n",
        "\n",
        "なお、拡散モデルには[いくつかの視点](https://twitter.com/sedielem/status/1530894256168222722?s=20&t=mfv4afx1GcNQU5fZklpACw)がある。ここでは、離散時間（潜在変数モデル）の視点を採用しますが、他の視点もぜひチェックしてみてください。\n",
        "\n",
        "では、さっそく本題に入りましょう。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yVijwSUfKhfY",
      "metadata": {
        "id": "yVijwSUfKhfY"
      },
      "source": [
        "まず必要なライブラリをインストールし、インポートします（PyTorchが インストールされていることが前提です）。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M7G7djASKdWg",
      "metadata": {
        "id": "M7G7djASKdWg"
      },
      "source": [
        "<p align=\"center\">\n",
        "<img src='https://drive.google.com/uc?id=11C3cBUfz7_vrkj_4CWCyePaQyr-0m85_' width=500>\n",
        "</p>\n",
        "\n",
        "まず必要なライブラリをインストールし、インポートします（PyTorchが インストールされていることが前提です）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1f2d714",
      "metadata": {
        "id": "a1f2d714"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U einops datasets matplotlib tqdm\n",
        "\n",
        "import math\n",
        "from inspect import isfunction\n",
        "from functools import partial\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from einops import rearrange\n",
        "\n",
        "import torch\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fe49a34",
      "metadata": {
        "id": "6fe49a34"
      },
      "source": [
        "# **普及モデルとは？**\n",
        "正規化フロー、$GAN$、$VAE$などの他の生成モデルと比較すると、（ノイズ除去）拡散モデルはそれほど複雑ではありません：\n",
        "\n",
        "これらはすべて、ある単純な分布からノイズをデータサンプルに変換するものです。これはここでも同じで、 ニューラルネットワークが純粋なノイズから出発して徐々にデータをノイズ除去することを学習するのです。\n",
        "\n",
        "イメージのためにもう少し詳しく説明すると、セットアップは2つのプロセスで構成されています。\n",
        "\n",
        "- 固定された（あるいはあらかじめ定義された）任意の前方拡散プロセス $q$で，画像にガウスノイズを徐々に加え，最終的に純粋なノイズにする\n",
        "- 学習型逆変換拡散処理 $p θ$ ここでニューラルネットワークは、純粋なノイズから始まり、実際の画像に行き着くまで、徐々に画像をノイズ除去するように学習されます。\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1t5dUyJwgy2ZpDAqHXw7GhUAp2FE5BWHA\" width=\"600\" />\n",
        "</p>\n",
        "\n",
        "$t$でインデックスされる順方向および逆方向のプロセスは、ある有限時間ステップ数 $T$の間起こります（$DDPM$ の作者は $T= 1000$ を使用しています）。 $t= 0$でスタートし、データ分布から実画像 $x 0$ をサンプルし（$ImageNet$からの猫の画像としましょう）、前進過程は各タイムステップ $t$ でガウス分布からノイズをサンプルし、前のタイムステップの画像に追加しています。十分に大きな$T$と、各タイムステップでノイズを加えるための行儀の良いスケジュールがあれば、漸進的なプロセスによって、 $t= T$で [等方性ガウス分布](https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic)と呼ばれるものが出来上がります。\n",
        "\n",
        "より数学的な形式では\n",
        "最終的には、ニューラルネットワークが最適化する必要のある、扱いやすい損失関数が必要なので、これをもっと正式に書きましょう。\n",
        "\n",
        "させる  \\\\(q(\\mathbf{x}_0)\\\\) は、実データの分布、例えば「実画像」の分布である。この分布からサンプリングして画像を得ることができる。  $x0∼q(x0)$ .ここで、前方拡散過程を定義する  $q(xt|xt-1)$ 各時間ステップでガウスノイズを加える  t 既知の分散スケジュールに従って  $0<β1<β2<$...$<βT<1$ かわりに\n",
        "$q(xt|xt-1)=N(xt;1-βt-----√xt-1,βtI)$. \n",
        "\n",
        "正規分布（ガウス分布とも呼ばれる）は2つのパラメータで定義されることを思い出してください。  $μ$ と分散  $σ2≥0$ .基本的に、時間ステップにおけるそれぞれの新しい（少しノイズの多い）画像は  t が描画されます。 条件付ガウス分布をもって  $μt=1-βt-----√xt-1$ と  σ2t=βt をサンプリングすることによって行うことができます。  $ϵ ∼N(0,I)$ を設定し  $xt=1-βt-----√xt-1+βt--√ϵ$ .\n",
        "\n",
        "$β  t$ は 各タイムステップで一定ではない ことに注意してくださいt (したがって添え字) --- 実際には、いわゆる 「分散スケジュール」を 定義 します。これは、これから見るように線形、二次、余弦などです (学習率スケジュールに少し似ています)。\n",
        "\n",
        "からスタートするわけです。  x0 ということになり、結局は  $x1,...,xt,...,xT$で、ここで  $xT$ は、スケジュールを適切に設定すれば、純粋なガウスノイズとなる。\n",
        "\n",
        "今、条件付き分布が分かっていれば  $p(xt-1|xt)$ というランダムなガウシアンノイズのサンプリングによって、このプロセスを逆に実行することができます。  $xT$ そして，それを徐々に「ノイズ除去」して，最終的に実際の分布からのサンプルを得ます。  $x0$.\n",
        "\n",
        "しかし、私たちは知らない  $p(xt-1|xt)$ .この条件付き確率を計算するためには、すべての可能な画像の分布を知る必要があるため、難易度が高いのです。そこで、ニューラルネットワークを活用して じゅうきょひどうと呼ぶことにしましょう。  $pθ(xt-1|xt)$ である。  $θ$ はニューラルネットワークのパラメータであり、勾配降下法により更新される。\n",
        "\n",
        "さて、それでは逆過程の（条件付き）確率分布を表現するためのニューラルネットワークが必要ですね。この逆過程もガウシアンだとすると、どんなガウシアン分布も2つのパラメータで定義されることを思い出してください。\n",
        "\n",
        "によってパラメタライズされる平均値です。  $μθ$ ;\n",
        "によってパラメタライズされる分散です。  $Σθ$ ;\n",
        "ということで、そのプロセスをパラメトリックにすると\n",
        "$$ p_\\theta (\\mathbf{x}_{t-1} | \\mathbf{x}_t) = \\mathcal{N}(\\mathbf{x}_{t-1}; \\mu_\\theta(\\mathbf{x}_{t},t), \\Sigma_\\theta (\\mathbf{x}_{t},t))$$\n",
        "ここで、平均と分散もノイズレベルに条件付けされる  $t$ ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d747688",
      "metadata": {
        "id": "2d747688"
      },
      "source": [
        "したがって、ニューラルネットワークは平均と分散を学習・表現する必要があります。しかし、DDPMの作者は分散を固定 し、 この条件付き確率分布の平均値のみを学習（表現） させることにした。論文より\n",
        "> First, we set \\\\(\\Sigma_\\theta ( \\mathbf{x}_t, t) = \\sigma^2_t \\mathbf{I}\\\\) to untrained time dependent constants. Experimentally, both \\\\(\\sigma^2_t = \\beta_t\\\\) and \\\\(\\sigma^2_t  = \\tilde{\\beta}_t\\\\) (see paper) had similar results. \n",
        "\n",
        "\n",
        "まず 、 未調教の時間依存定数に 、$Sigma_theta$${x}_t$, $t$ = \\sigma^2_t \\mathbf{I}}) を設定します。 実験的には 、♪ \\(sigma^2_t = \\beta_t) と ♪ \\(sigma^2_t = \\tilde{beta}_t) (論文参照)の両方で同様の 結果が得られています。\n",
        "\n",
        "これはその後、Improved diffusion modelsの論文で改良され、ニューラルネットワークが平均以外に、この逆行列の分散も学習するようになりました。\n",
        "\n",
        "そこで、ニューラルネットワークはこの条件付き確率分布の平均値だけを学習・表現すればよいと仮定して、話を続ける。\n",
        "\n",
        "目的関数の定義（平均値の再パラメタイズによる）\n",
        "後方プロセスの平均を学習するための目的関数を導出するために、著者 らは\\(q\\) と \\(p_theta\\) の組み合わせが variational auto-encoder (VAE) として 見られることを観察 します(Kingma et al., 2013)。したがって、変分下界 （ELBOともいう）は、グランドトゥルースデータサンプル \\(\\mathbf{x}_0}) に関する負の対数尤度を最小化するために使用できる （ELBOに関する詳細は、VAE論文を参照すること） 。このプロセスの ELBOは、各タイムステップでの損失 和(L = L_0 + L_1 + ... + L_Thesis) であることが判明しました。Forward \\(q) process and backward processの 構成から 、 損失の 各項(not for \\(L_0 001))は実は 2つのガウス分布間のKL divergenceで、明示的に平均に関するL2損失として書くことができます!\n",
        "\n",
        "Sohl-Dickstein et al.が示した ように、構築された前進過 程(forward process)の直接的な 結果として、( sums of Gaussians is also Gaussian) \\(\\mathbf{x}_0) を条件として任意のノイズレベルで サンプリング できるのです。これは とても便利な ことで、\" Sample \\(\\mathbf{x}_t}\" のために、 何度も \"the\\(q) \" を 適用 する必要がありません。 q(\\mathbf{x}_t | \\mathbf{x}_0) = \\cal{N}(\\mathbf{x}_t; \\sqrt{Thinbar{Thinalpha}_t}) であることが分かります。\\mathbf{x}_0, (1- \\bar{alpha}_t) \\mathbf{I})$$.\n",
        "\n",
        "with \\(\\alpha_t := 1 - \\beta_t\\) and \\(\\bar{alpha}t := \\Pi_{s=1}^{t} \\alpha_s :).この式を \"nice property \"と呼ぶことにしましょう。 これは、ガウスノイズをサンプリングして、それを適当にスケーリングして 、\\(\\mathbf{x}_0}) に加え れば、 そのまま \\(\\mathbf{x}_t})が得られることを意味します。 なお 、この\\(Ⓐ) は既知の 分散スケジュール である \\(Ⓐ) の 関数 なので、 これも既知で事前に計算 することが可能です。これにより、学習中に 損失関 数(loss function)のランダムな項を最適化 することができます ( 言い換えれば、 学習中に ランダムに (tenta)を サンプリングして、 (L_tenta)を 最適化 することができます)。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68574c28",
      "metadata": {
        "id": "68574c28"
      },
      "source": [
        "この特性のもう一つの利点は、Ho et al.は、(いくつかの数学の後、読者にこの素晴らしいブログ記事を参照してもらっていますが)代わりに平均を再パラメトリック化して、ニューラルネットワークに追加されたノイズを学習(予測)させることができます(via a network \\(mathbf{epsilon}_theta(\\mathbf{x}_t,を学習し、損失を構成するKL項に ノイズ レベルⒶ(t)Ⓑ が 加わることを予測します。つまり、このニューラルネットワークは（直接的な）平均予測器ではなく、ノイズ予測器となる。平均は次のように計算される。\n",
        "\n",
        "Ъ⑅⑅⑅⑅⑅⑅⑅⑅⑅⑅⑅⑅⑅⑅ㄘ\\left( \\mathbf{x}_t - \\frac{beta_t}{Thanksqrt{1- \\bar{alpha}_t}} {Thanksqrt{1- | }} )\\mathbf{mu}_theta(\\mathbf{x}_t, t) \\right)$$$\\mathbf{mu}_theta(\\mathbf{x}_t, t) = \\frac{1}{sqrt{1}{alpha_t}} (\\mathbf{x}_t, t)\\left( \\mathbf{x}_t - \\frac{beta_t}{Thanksqrt{1- \\bar{alpha}_t}} {Thanksqrt{1- | }} )\\mathbf{epsilon}_theta(\\mathbf{x}_t, t) \\right)$$.\n",
        "\n",
        "最終的な目的関数  Lt の場合、次のようになります（ランダムな時間ステップの場合）。  t 所定  ϵ∼N(0,I) ):"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5153024b",
      "metadata": {
        "id": "5153024b"
      },
      "source": [
        "ここで 、${x}_0$ は初期（実画像、無汚染）画像で 、 固定前進プロセス で与えられた 直接ノイズレベル $t$である。${epsilon}$ は時間ステップでサンプリングされた純粋なノイズ であり、 [\\(mathbf{epsilon}_theta (\\mathbf{x}_t, t)\\] は我々のニューラルネットである。 ニューラルネットワークは、ガウシアンノイズと予測値の平均二乗誤差(MSE)を用いて最適化されています。\n",
        "\n",
        "これで学習アルゴリズムは次のようになる。\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1LJsdkZ3i1J32lmi9ONMqKFg5LMtpSfT4\" width=\"400\" />\n",
        "</p>\n",
        "\n",
        "つまりは\n",
        "\n",
        "われわれは無作為にサンプルを取る  x0 実際の未知で複雑な可能性のあるデータ分布から  q(x0) \n",
        "ノイズレベルをサンプリングする  t 満遍なく  1 と  T (を使用します（例：ランダムな時間ステップ）。\n",
        "ガウス分布からノイズをサンプリングし，このノイズによって入力をレベル  t 上記で定義した nice プロパティを使って\n",
        "このノイズを予測するようにニューラルネットワークを学習させる。  xt に適用されるノイズです。  $x0$ 予定に基づ$く  βt$ \n",
        "実際には、確率的勾配降下法を用いてニューラルネットワークを最適化するため、これらの作業はすべてデータのバッチで行われる。\n",
        "\n",
        "ニューラルネット\n",
        "ニューラルネットワークは特定の時間ステップでノイズの入った画像を取り込み、予測されたノイズを返す必要がある。予測されるノイズは入力画像と同じサイズ/解像度を持つテンソルであることに注意されたい。つまり、技術的には同じ形のテンソルを取り込み、出力するネットワークである。これにはどのようなニューラルネットワークを使えばよいのだろうか。\n",
        "\n",
        "ここで典型的に使われているのは、典型的な「深層学習入門」のチュートリアルで覚えているであろう、オートエンコーダーと非常によく似たものです。オートエンコーダーは、エンコーダーとデコーダーの間に、いわゆる「ボトルネック」層があります。エンコーダーはまず画像を「ボトルネック」と呼ばれる小さな隠れ表現にエンコードし、次にデコーダーがその隠れ表現を実際の画像にデコードして戻す。これにより、ネットワークはボトルネック層に最も重要な情報のみを保持するようになる。\n",
        "\n",
        "アーキテクチャの面では、DDPMの作者は、（Ronnebergerら、2015）（当時、医療画像のセグメンテーションのために最先端の結果を達成した）によって導入されたU-Netを採用した。このネットワークは、他のオートエンコーダーと同様に、ネットワークが最も重要な情報のみを学習するようにする中央のボトルネックで構成されています。重要なのは、エンコーダーとデコーダーの間に残留接続を導入し、勾配流を大幅に改善したことです（He et al., 2015のResNetに着想を得ています）。<p align=\"center\">\n",
        "    <img src=\"https://drive.google.com/uc?id=1_Hej_VTgdUWGsxxIuyZACCGjpbCGIUi6\" width=\"400\" />\n",
        "</p>\n",
        "\n",
        "このように、U-Netモデルは、まず入力をダウンサンプリング（空間分解能を小さくする）し、その後、アップサンプリングを行う。\n",
        "\n",
        "以下、順を追って、このネットワークを実装していきます。\n",
        "\n",
        "ネットワークヘルパー\n",
        "まず、ニューラルネットワークを実装する際に使用されるいくつかのヘルパー関数とクラスを定義します。重要なのはResidualモジュールを定義することで、これは単に特定の関数の出力に入力を追加する（言い換えれば、特定の関数に残差接続を追加する）だけです。\n",
        "\n",
        "また、アップサンプリングとダウンサンプリングの操作の別名を定義しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68907f5d",
      "metadata": {
        "id": "68907f5d"
      },
      "outputs": [],
      "source": [
        "def exists(x):\n",
        "    return x is not None\n",
        "\n",
        "def default(val, d):\n",
        "    if exists(val):\n",
        "        return val\n",
        "    return d() if isfunction(d) else d\n",
        "\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return self.fn(x, *args, **kwargs) + x\n",
        "\n",
        "def Upsample(dim):\n",
        "    return nn.ConvTranspose2d(dim, dim, 4, 2, 1)\n",
        "\n",
        "def Downsample(dim):\n",
        "    return nn.Conv2d(dim, dim, 4, 2, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "592aa765",
      "metadata": {
        "id": "592aa765"
      },
      "source": [
        "ポジションエンベッディング\n",
        "ニューラルネットワークのパラメータは時間（ノイズレベル）にわたって共有されるので、著者らは正弦波位置エンコーディングを採用して、  t  、Transformer（Vaswani et al., 2017）に触発されてエンコード しています。これにより、ニューラルネットワークは、バッチ内のすべての画像について、どの特定の時間ステップ（ノイズレベル）で動作しているのかを「知る」ことができます。\n",
        "\n",
        "SinusoidalPositionEmbeddingsモジュールは、形状（batch_size, 1）のテンソルを入力として受け取り（つまり、バッチ内のいくつかのノイズ画像のノイズレベル）、これを形状（batch_size, dim）のテンソルに変換します（dimは位置埋め込みの次元数です）。これは、後に見るように、各残差ブロックに追加される。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ed0757b",
      "metadata": {
        "id": "5ed0757b"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionEmbeddings(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = math.log(10000) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff47fbb",
      "metadata": {
        "id": "9ff47fbb"
      },
      "source": [
        "ResNet/ConvNeXTブロック\n",
        "次に、U-Netモデルのコアビルディングブロックを定義します。DDPMの著者らはWide ResNetブロック（Zagoruyko et al., 2016）を採用しましたが、Phil Wangは、後者が画像ドメインで大きな成功を収めたことから、ConvNeXTブロック（Liu et al., 2022）のサポートも追加することにしました。最終的なU-Netアーキテクチャにおいて、どちらかを選択することができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c2d1219",
      "metadata": {
        "id": "2c2d1219"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, dim, dim_out, groups = 8):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(dim, dim_out, 3, padding = 1)\n",
        "        self.norm = nn.GroupNorm(groups, dim_out)\n",
        "        self.act = nn.SiLU()\n",
        "\n",
        "    def forward(self, x, scale_shift = None):\n",
        "        x = self.proj(x)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        if exists(scale_shift):\n",
        "            scale, shift = scale_shift\n",
        "            x = x * (scale + 1) + shift\n",
        "\n",
        "        x = self.act(x)\n",
        "        return x\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    \"\"\"https://arxiv.org/abs/1512.03385\"\"\"\n",
        "    \n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):\n",
        "        super().__init__()\n",
        "        self.mlp = (\n",
        "            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out))\n",
        "            if exists(time_emb_dim)\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.block1 = Block(dim, dim_out, groups=groups)\n",
        "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        h = self.block1(x)\n",
        "\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            time_emb = self.mlp(time_emb)\n",
        "            h = rearrange(time_emb, \"b c -> b c 1 1\") + h\n",
        "\n",
        "        h = self.block2(h)\n",
        "        return h + self.res_conv(x)\n",
        "    \n",
        "class ConvNextBlock(nn.Module):\n",
        "    \"\"\"https://arxiv.org/abs/2201.03545\"\"\"\n",
        "\n",
        "    def __init__(self, dim, dim_out, *, time_emb_dim=None, mult=2, norm=True):\n",
        "        super().__init__()\n",
        "        self.mlp = (\n",
        "            nn.Sequential(nn.GELU(), nn.Linear(time_emb_dim, dim))\n",
        "            if exists(time_emb_dim)\n",
        "            else None\n",
        "        )\n",
        "\n",
        "        self.ds_conv = nn.Conv2d(dim, dim, 7, padding=3, groups=dim)\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.GroupNorm(1, dim) if norm else nn.Identity(),\n",
        "            nn.Conv2d(dim, dim_out * mult, 3, padding=1),\n",
        "            nn.GELU(),\n",
        "            nn.GroupNorm(1, dim_out * mult),\n",
        "            nn.Conv2d(dim_out * mult, dim_out, 3, padding=1),\n",
        "        )\n",
        "\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, time_emb=None):\n",
        "        h = self.ds_conv(x)\n",
        "\n",
        "        if exists(self.mlp) and exists(time_emb):\n",
        "            assert exists(time_emb), \"time embedding must be passed in\"\n",
        "            condition = self.mlp(time_emb)\n",
        "            h = h + rearrange(condition, \"b c -> b c 1 1\")\n",
        "\n",
        "        h = self.net(h)\n",
        "        return h + self.res_conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d9a24c",
      "metadata": {
        "id": "51d9a24c"
      },
      "source": [
        "アテンションモジュール\n",
        "次に、DDPMの作者が畳み込みブロックの間に追加したアテンションモジュールを定義します。アテンションは、NLPやビジョンからタンパク質折り畳みまで、AIの様々な領域で大きな成功を収めた有名なTransformerアーキテクチャ（Vaswani et al.、2017）のビルディングブロックです。Phil Wangは注意の2つの変種を採用しています：1つは通常のマルチヘッド自己注意（Transformerで使用）、もう1つは線形注意変種）、その時間およびメモリ要件は、通常の注意の2次関数に対して、シーケンス長に線形にスケールします。\n",
        "\n",
        "アテンションメカニズムの詳細については、ジェイ・アラマー氏の素晴らしいブログ記事を参照してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07bbd544",
      "metadata": {
        "id": "07bbd544"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "        q = q * self.scale\n",
        "\n",
        "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
        "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
        "        attn = sim.softmax(dim=-1)\n",
        "\n",
        "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
        "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
        "        return self.to_out(out)\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=32):\n",
        "        super().__init__()\n",
        "        self.scale = dim_head**-0.5\n",
        "        self.heads = heads\n",
        "        hidden_dim = dim_head * heads\n",
        "        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n",
        "\n",
        "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), \n",
        "                                    nn.GroupNorm(1, dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.shape\n",
        "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
        "        q, k, v = map(\n",
        "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
        "        )\n",
        "\n",
        "        q = q.softmax(dim=-2)\n",
        "        k = k.softmax(dim=-1)\n",
        "\n",
        "        q = q * self.scale\n",
        "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
        "\n",
        "        out = torch.einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
        "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
        "        return self.to_out(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a8031b0",
      "metadata": {
        "id": "9a8031b0"
      },
      "source": [
        "グループ正規化\n",
        "DDPMの作者は、U-Netの畳み込み層／注意層をグループノーマライズでインターリーブしています（Wu et al.、2018）。以下では、PreNormクラスを定義し、このクラスは、さらに見るように、注意層の前にgroupnormを適用するために使用されます。なお、トランスフォーマーでは、注目の前と後のどちらに正規化を適用するかという議論があります。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e2ce68f",
      "metadata": {
        "id": "5e2ce68f"
      },
      "outputs": [],
      "source": [
        "class PreNorm(nn.Module):\n",
        "    def __init__(self, dim, fn):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.norm = nn.GroupNorm(1, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.norm(x)\n",
        "        return self.fn(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06b3fad0",
      "metadata": {
        "id": "06b3fad0"
      },
      "source": [
        "条件付きU-NET\n",
        "さて、すべてのビルディングブロック（位置埋め込み、ResNet/ConvNeXTブロック、注意、グループ正規化）を定義したところで、ニューラルネットワーク全体を定義することになります。ネットワークの仕事は  ϵθ(xt,t) は、ノイズの多い画像＋ノイズレベルを一括して取り込み、入力に加えられたノイズを出力するものです。より正式には\n",
        "\n",
        "ネットワークは、形状(batch_size, num_channels, height, width)と形状(batch_size, 1) のノイズレベルのバッチを入力として受け取り、形状(batch_size, num_channels, height, width) のテンソルを返します。\n",
        "ネットワークは以下のように構築されています。\n",
        "\n",
        "まず、ノイズの多い画像に対して畳み込み層を適用し、ノイズレベルに対する位置埋め込みを計算する\n",
        "次に、一連のダウンサンプリングステージが適用される。各ダウンサンプリングステージは、2つのResNet/ConvNeXTブロック + groupnorm + attention + residual connection + ダウンサンプリングオペレーションから構成されます。\n",
        "は、ResNet または ConvNeXT ブロックが適用され、注意とインターリーブされる。\n",
        "次に、一連のアップサンプリングステージが適用される。各アップサンプリングステージは、2つのResNet/ConvNeXTブロック + groupnorm + attention + residual connection + upsample operationで構成されます。\n",
        "最後に、ResNet/ConvNeXTブロックと畳み込み層が適用されます。\n",
        "最終的にニューラルネットワークは、レゴブロックのように層を積み上げていきます（ただし、その仕組みを理解することが重要です）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a159023",
      "metadata": {
        "id": "3a159023"
      },
      "outputs": [],
      "source": [
        "class Unet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        init_dim=None,\n",
        "        out_dim=None,\n",
        "        dim_mults=(1, 2, 4, 8),\n",
        "        channels=3,\n",
        "        with_time_emb=True,\n",
        "        resnet_block_groups=8,\n",
        "        use_convnext=True,\n",
        "        convnext_mult=2,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # 次元の決定\n",
        "        self.channels = channels\n",
        "\n",
        "        init_dim = default(init_dim, dim // 3 * 2)\n",
        "        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding=3)\n",
        "\n",
        "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "        \n",
        "        if use_convnext:\n",
        "            block_klass = partial(ConvNextBlock, mult=convnext_mult)\n",
        "        else:\n",
        "            block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
        "\n",
        "        # 埋め込み時間\n",
        "        if with_time_emb:\n",
        "            time_dim = dim * 4\n",
        "            self.time_mlp = nn.Sequential(\n",
        "                SinusoidalPositionEmbeddings(dim),\n",
        "                nn.Linear(dim, time_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(time_dim, time_dim),\n",
        "            )\n",
        "        else:\n",
        "            time_dim = None\n",
        "            self.time_mlp = None\n",
        "\n",
        "        # 層\n",
        "        self.downs = nn.ModuleList([])\n",
        "        self.ups = nn.ModuleList([])\n",
        "        num_resolutions = len(in_out)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.downs.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
        "                        Downsample(dim_out) if not is_last else nn.Identity(),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
        "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
        "\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
        "            is_last = ind >= (num_resolutions - 1)\n",
        "\n",
        "            self.ups.append(\n",
        "                nn.ModuleList(\n",
        "                    [\n",
        "                        block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
        "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
        "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
        "                        Upsample(dim_in) if not is_last else nn.Identity(),\n",
        "                    ]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        out_dim = default(out_dim, channels)\n",
        "        self.final_conv = nn.Sequential(\n",
        "            block_klass(dim, dim), nn.Conv2d(dim, out_dim, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, time):\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
        "\n",
        "        h = []\n",
        "\n",
        "        # 低解像度処理\n",
        "        for block1, block2, attn, downsample in self.downs:\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            h.append(x)\n",
        "            x = downsample(x)\n",
        "\n",
        "        # ボトルネック\n",
        "        x = self.mid_block1(x, t)\n",
        "        x = self.mid_attn(x)\n",
        "        x = self.mid_block2(x, t)\n",
        "\n",
        "        # 高解像度処理\n",
        "        for block1, block2, attn, upsample in self.ups:\n",
        "            x = torch.cat((x, h.pop()), dim=1)\n",
        "            x = block1(x, t)\n",
        "            x = block2(x, t)\n",
        "            x = attn(x)\n",
        "            x = upsample(x)\n",
        "\n",
        "        return self.final_conv(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a30368b2",
      "metadata": {
        "id": "a30368b2"
      },
      "source": [
        "denoise_modelは上で定義したU-Netになります。我々は、真のノイズと予測されたノイズの間にHuberロスを採用することにします。\n",
        "\n",
        "PyTorchデータセットの定義 + DataLoader\n",
        "ここでは、通常のPyTorchデータセットを定義します 。このデータセットは、Fashion-MNIST, CIFAR-10, ImageNet などの実データセットからの画像を 、[-1,1] に直線的にスケーリングしたものから単純に構成されて います。\n",
        "\n",
        "各画像は同じサイズにリサイズされます。面白いのは、画像もランダムに水平方向に反転されることです。論文より\n",
        "\n",
        "CIFAR10では、学習時にランダムな水平フリップを使用しました。フリップを使用した場合と使用しない場合の両方を試した結果、フリップを使用することでサンプルの品質が若干向上することが分かりました。\n",
        "\n",
        "ここでは、🤗Datasetsライブラリを用いて、Fashion MNISTデータセットをハブから簡単に読み込むことができます。このデータセットは、既に同じ解像度、すなわち28x28を持つ画像から構成されています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d751df2",
      "metadata": {
        "id": "5d751df2"
      },
      "outputs": [],
      "source": [
        "def cosine_beta_schedule(timesteps, s=0.008):\n",
        "    \"\"\"\n",
        "    cosine schedule as proposed in https://arxiv.org/abs/2102.09672\n",
        "    \"\"\"\n",
        "    steps = timesteps + 1\n",
        "    x = torch.linspace(0, timesteps, steps)\n",
        "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
        "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
        "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
        "    return torch.clip(betas, 0.0001, 0.9999)\n",
        "\n",
        "def linear_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start, beta_end, timesteps)\n",
        "\n",
        "def quadratic_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2\n",
        "\n",
        "def sigmoid_beta_schedule(timesteps):\n",
        "    beta_start = 0.0001\n",
        "    beta_end = 0.02\n",
        "    betas = torch.linspace(-6, 6, timesteps)\n",
        "    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bfc3841",
      "metadata": {
        "id": "6bfc3841"
      },
      "source": [
        "まず始めに、リニアスケジュールで  $T=200$ の時間ステップと様々な変数を定義します。  $βt$ というように、分散の累積積が必要です。  $α¯t$ .以下の各変数は単なる1次元のテンソルであり、以下の値を格納する。  $t$ まで  $T$ .また、重要なことに、我々は 抜粋関数を使用することで、適切な  $t$ のインデックスを一括で取得することができます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc57b01f",
      "metadata": {
        "id": "cc57b01f"
      },
      "outputs": [],
      "source": [
        "timesteps = 200\n",
        "\n",
        "# β線スケジュールを定義する\n",
        "betas = linear_beta_schedule(timesteps=timesteps)\n",
        "\n",
        "#　アルファを定義する\n",
        "alphas = 1. - betas\n",
        "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
        "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
        "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
        "\n",
        "# 拡散 q(x_t | x_{t-1}) などの計算を行う\n",
        "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
        "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)\n",
        "\n",
        "# 事後q(x_{t-1} | x_t, x_0)の計算結果\n",
        "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
        "\n",
        "def extract(a, t, x_shape):\n",
        "    batch_size = t.shape[0]\n",
        "    out = a.gather(-1, t.cpu())\n",
        "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48f8a004",
      "metadata": {
        "id": "48f8a004"
      },
      "source": [
        "We'll illustrate with a cats image how noise is added at each time step of the diffusion process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f13b16",
      "metadata": {
        "id": "c9f13b16",
        "lines_to_next_cell": 0
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import requests\n",
        "\n",
        "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
        "image = Image.open(requests.get(url, stream=True).raw)\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bde062f",
      "metadata": {
        "id": "4bde062f"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=17FXnvCTl96lDhqZ_io54guXO8hM-rsQ2\" width=\"400\" />\n",
        "\n",
        "ノイズはPillow Imageではなく、PyTorchのテンソルに加えられる。まず、PIL画像からPyTorchテンソル（その上にノイズを加えることができる）へ、またはその逆を可能にする画像変換を定義します。\n",
        "\n",
        "これらの変換は非常に簡単で、まず画像を正規化するために  255 (にあるような）。  $[0,1]$の範囲）であることを確認し、それらが  $[-1,1]$ の範囲です。DPPMの論文より。\n",
        "\n",
        "の整数からなる画像データを想定しています。  ${0,1,...,255}$ に線形に拡大される。  $[-1,1]$ .これにより、ニューラルネットワークの反転処理は、標準的な正規の事前分布から始まる一貫したスケールの入力で動作することが保証されます。  $p(xT)$ . \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71aba861",
      "metadata": {
        "id": "71aba861"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize\n",
        "\n",
        "image_size = 128\n",
        "transform = Compose([\n",
        "    Resize(image_size),\n",
        "    CenterCrop(image_size),\n",
        "    ToTensor(), # HWC 形式の Numpy 配列に変換し、255 で割る。\n",
        "    Lambda(lambda t: (t * 2) - 1),\n",
        "    \n",
        "])\n",
        "\n",
        "x_start = transform(image).unsqueeze(0)\n",
        "x_start.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e143cc62",
      "metadata": {
        "id": "e143cc62"
      },
      "source": [
        "<div class=\"output stream stdout\">\n",
        "\n",
        "    Output:\n",
        "    ----------------------------------------------------------------------------------------------------\n",
        "    torch.Size([1, 3, 128, 128])\n",
        "\n",
        "</div>\n",
        "\n",
        "We also define the reverse transform, which takes in a PyTorch tensor containing values in $[-1, 1]$ and turn them back into a PIL image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b98e91ff",
      "metadata": {
        "id": "b98e91ff"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "reverse_transform = Compose([\n",
        "     Lambda(lambda t: (t + 1) / 2),\n",
        "     Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
        "     Lambda(lambda t: t * 255.),\n",
        "     Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
        "     ToPILImage(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6869ab7",
      "metadata": {
        "id": "f6869ab7"
      },
      "source": [
        "Let's verify this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "366a770c",
      "metadata": {
        "id": "366a770c"
      },
      "outputs": [],
      "source": [
        "reverse_transform(x_start.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f38d30ab",
      "metadata": {
        "id": "f38d30ab"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1WT22KYvqJbHFdYYfkV7ohKNO4alnvesB\" width=\"100\" />\n",
        "\n",
        "ここで、論文と同様に前方拡散過程を定義することができる。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3752480",
      "metadata": {
        "id": "f3752480"
      },
      "outputs": [],
      "source": [
        "# forward diffusion\n",
        "def q_sample(x_start, t, noise=None):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x_start.shape\n",
        "    )\n",
        "\n",
        "    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e82bac28",
      "metadata": {
        "id": "e82bac28"
      },
      "source": [
        "Let's test it on a particular time step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bd64f89",
      "metadata": {
        "id": "6bd64f89"
      },
      "outputs": [],
      "source": [
        "def get_noisy_image(x_start, t):\n",
        "  # ノイズを加える\n",
        "  x_noisy = q_sample(x_start, t=t)\n",
        "\n",
        "  # PILの画像に戻す\n",
        "  noisy_image = reverse_transform(x_noisy.squeeze())\n",
        "\n",
        "  return noisy_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52d22667",
      "metadata": {
        "id": "52d22667"
      },
      "outputs": [],
      "source": [
        "# 時間がかかる\n",
        "t = torch.tensor([40])\n",
        "\n",
        "get_noisy_image(x_start, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "003e1d95",
      "metadata": {
        "id": "003e1d95"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1Ra33wxuw3QxPlUG0iqZGtxgKBNdjNsqz\" width=\"100\" />\n",
        "\n",
        "Let's visualize this for various time steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e27c37d",
      "metadata": {
        "id": "8e27c37d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 再現性のためシードを固定する\n",
        "torch.manual_seed(0)\n",
        "\n",
        "# source: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
        "def plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):\n",
        "    if not isinstance(imgs[0], list):\n",
        "        # 1列でも2次元のグリッドを作成する\n",
        "        imgs = [imgs]\n",
        "\n",
        "    num_rows = len(imgs)\n",
        "    num_cols = len(imgs[0]) + with_orig\n",
        "    fig, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)\n",
        "    for row_idx, row in enumerate(imgs):\n",
        "        row = [image] + row if with_orig else row\n",
        "        for col_idx, img in enumerate(row):\n",
        "            ax = axs[row_idx, col_idx]\n",
        "            ax.imshow(np.asarray(img), **imshow_kwargs)\n",
        "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "    if with_orig:\n",
        "        axs[0, 0].set(title='Original image')\n",
        "        axs[0, 0].title.set_size(8)\n",
        "    if row_title is not None:\n",
        "        for row_idx in range(num_rows):\n",
        "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "323102d0",
      "metadata": {
        "id": "323102d0"
      },
      "outputs": [],
      "source": [
        "plot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a205c24",
      "metadata": {
        "id": "4a205c24"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1QifsBnYiijwTqru6gur9C0qKkFYrm-lN\" width=\"800\" />\n",
        "    \n",
        "つまり、モデルが与えられた損失関数を次のように定義することができるようになったのである。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7725f6cf",
      "metadata": {
        "id": "7725f6cf"
      },
      "outputs": [],
      "source": [
        "def p_losses(denoise_model, x_start, t, noise=None, loss_type=\"l1\"):\n",
        "    if noise is None:\n",
        "        noise = torch.randn_like(x_start)\n",
        "\n",
        "    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)\n",
        "    predicted_noise = denoise_model(x_noisy, t)\n",
        "\n",
        "    if loss_type == 'l1':\n",
        "        loss = F.l1_loss(noise, predicted_noise)\n",
        "    elif loss_type == 'l2':\n",
        "        loss = F.mse_loss(noise, predicted_noise)\n",
        "    elif loss_type == \"huber\":\n",
        "        loss = F.smooth_l1_loss(noise, predicted_noise)\n",
        "    else:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc01c63b",
      "metadata": {
        "id": "cc01c63b"
      },
      "source": [
        "前方拡散プロセスの定義\n",
        "前方拡散処理は、実分布から時間ステップ数で徐々に画像にノイズを加える  T  . これは分散スケジュールに従って行わ れる。オリジナルのDDPMの作者は、線形スケジュールを採用していました。\n",
        "\n",
        "から線形に増加する定数に設定した。  β1=10-4  まで  βT=0.02 .\n",
        "\n",
        "しかし、(Nichol et al., 2021)では、コサインスケジュールを採用するとより良い結果が得られることが示された。\n",
        "\n",
        "以下、様々なスケジュールを定義していきます。  T のタイムステップと、それに対応する累積分散のような変数が必要です。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6134d691",
      "metadata": {
        "id": "6134d691"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# ハブからデータセットを読み込む\n",
        "dataset = load_dataset(\"fashion_mnist\")\n",
        "image_size = 28\n",
        "channels = 1\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db6f5875",
      "metadata": {
        "id": "db6f5875"
      },
      "source": [
        "次に、データセット全体にオンザフライで適用する関数を定義します。これには with_transform 関数を使います。この関数は基本的な画像の前処理を行うだけです。ランダムに水平反転させ、スケールを変え、最後に$[-1,1]$の範囲に値を持つようにします。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e78945",
      "metadata": {
        "id": "b3e78945"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 画像変換を定義する (例: torchvision を使用)\n",
        "transform = Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Lambda(lambda t: (t * 2) - 1)\n",
        "])\n",
        "\n",
        "# 関数の定義\n",
        "def transforms(examples):\n",
        "   examples[\"pixel_values\"] = [transform(image.convert(\"L\")) for image in examples[\"image\"]]\n",
        "   del examples[\"image\"]\n",
        "\n",
        "   return examples\n",
        "\n",
        "transformed_dataset = dataset.with_transform(transforms).remove_columns(\"label\")\n",
        "\n",
        "# データ読み込み機の作成\n",
        "dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52e8273b",
      "metadata": {
        "id": "52e8273b"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(dataloader))\n",
        "print(batch.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4325faf",
      "metadata": {
        "id": "e4325faf"
      },
      "source": [
        "<div class=\"output stream stdout\">\n",
        "\n",
        "    Output:\n",
        "    ----------------------------------------------------------------------------------------------------\n",
        "    dict_keys(['pixel_values'])\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cf98443",
      "metadata": {
        "id": "4cf98443"
      },
      "source": [
        "# **サンプリング**\n",
        "学習中にモデルからサンプリングする（進捗を把握するため）ので、そのためのコードを以下に定義します。サンプリングは論文中ではアルゴリズム2としてまとめられている。\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1ij80f8TNBDzpKtqHjk_sh8o5aby3lmD7\" width=\"500\" />\n",
        "\n",
        "拡散モデルから新しい画像を生成するには、拡散プロセスを反転させる必要があります。  T ここで、ガウス分布から純粋なノイズをサンプリングし、ニューラルネットワークを使って（学習した条件付き確率を使って）徐々にノイズを除去し、最終的に時間ステップ  t=0 .このように、より少ないノイズ除去率で画像を得ることができます。  xt-1 ノイズ予測器を用いて、平均の再パラメータ化を行うことで、このようなことが可能になります。分散は前もってわかっていることを忘れないでください。\n",
        "\n",
        "理想は、実際のデータ分布から得られたような画像に仕上げることです。\n",
        "\n",
        "これを実装したのが以下のコードです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7628fb3",
      "metadata": {
        "id": "f7628fb3"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def p_sample(model, x, t, t_index):\n",
        "    betas_t = extract(betas, t, x.shape)\n",
        "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
        "        sqrt_one_minus_alphas_cumprod, t, x.shape\n",
        "    )\n",
        "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
        "    \n",
        "    # 論文中の式11\n",
        "    # 我々のモデル（ノイズ予測器）を使って平均を予測する\n",
        "    model_mean = sqrt_recip_alphas_t * (\n",
        "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
        "    )\n",
        "\n",
        "    if t_index == 0:\n",
        "        return model_mean\n",
        "    else:\n",
        "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
        "        noise = torch.randn_like(x)\n",
        "        # アルゴリズム2行目 4．\n",
        "        return model_mean + torch.sqrt(posterior_variance_t) * noise \n",
        "\n",
        "# アルゴリズム2ですが、全画像を保存します。\n",
        "@torch.no_grad()\n",
        "def p_sample_loop(model, shape):\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    b = shape[0]\n",
        "    # 純粋なノイズからスタート（バッチ内の各例で）\n",
        "    img = torch.randn(shape, device=device)\n",
        "    imgs = []\n",
        "    \n",
        "    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):\n",
        "        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)\n",
        "        imgs.append(img.cpu().numpy())\n",
        "    return imgs\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, image_size, batch_size=16, channels=3):\n",
        "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f70235f8",
      "metadata": {
        "id": "f70235f8"
      },
      "source": [
        "上記のコードはオリジナルの実装を単純化したものであることに注意してください。私たちは、私たちの簡略化したコード（論文のアルゴリズム2に沿ったもの）が、オリジナルの複雑な実装と同じように動作することを発見しました。\n",
        "\n",
        "モデルの学習\n",
        "次に、通常のPyTorchの方法でモデルを学習させます。また、上記で定義したサンプルメソッドを用いて、生成された画像をペリディオカルに保存するためのロジックをいくつか定義します。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c1ad663",
      "metadata": {
        "id": "0c1ad663"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def num_to_groups(num, divisor):\n",
        "    groups = num // divisor\n",
        "    remainder = num % divisor\n",
        "    arr = [divisor] * groups\n",
        "    if remainder > 0:\n",
        "        arr.append(remainder)\n",
        "    return arr\n",
        "\n",
        "results_folder = Path(\"./results\")\n",
        "results_folder.mkdir(exist_ok = True)\n",
        "save_and_sample_every = 1000"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22e4c0fd",
      "metadata": {
        "id": "22e4c0fd"
      },
      "source": [
        "Below, we define the model, and move it to the GPU. We also define a standard optimizer (Adam)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5126e21",
      "metadata": {
        "id": "a5126e21"
      },
      "outputs": [],
      "source": [
        "from torch.optim import Adam\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = Unet(\n",
        "    dim=image_size,\n",
        "    channels=channels,\n",
        "    dim_mults=(1, 2, 4,)\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7444b0b",
      "metadata": {
        "id": "f7444b0b"
      },
      "source": [
        "Let's start training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92b12ed1",
      "metadata": {
        "id": "92b12ed1"
      },
      "outputs": [],
      "source": [
        "from torchvision.utils import save_image\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for step, batch in enumerate(dataloader):\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      batch_size = batch[\"pixel_values\"].shape[0]\n",
        "      batch = batch[\"pixel_values\"].to(device)\n",
        "\n",
        "      # アルゴリズム1 行目：バッチ内のすべての例に対して一様にtをサンプリングする\n",
        "      t = torch.randint(0, timesteps, (batch_size,), device=device).long()\n",
        "\n",
        "      loss = p_losses(model, batch, t, loss_type=\"huber\")\n",
        "\n",
        "      if step % 100 == 0:\n",
        "        print(\"Loss:\", loss.item())\n",
        "\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # 生成された画像の保存\n",
        "      if step != 0 and step % save_and_sample_every == 0:\n",
        "        milestone = step // save_and_sample_every\n",
        "        batches = num_to_groups(4, batch_size)\n",
        "        all_images_list = list(map(lambda n: sample(model, batch_size=n, channels=channels), batches))\n",
        "        all_images = torch.cat(all_images_list, dim=0)\n",
        "        all_images = (all_images + 1) * 0.5\n",
        "        save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = 6)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e617a66a",
      "metadata": {
        "id": "e617a66a"
      },
      "source": [
        "<div class=\"output stream stdout\">\n",
        "\n",
        "    Output:\n",
        "    ----------------------------------------------------------------------------------------------------\n",
        "    Loss: 0.46477368474006653\n",
        "    Loss: 0.12143351882696152\n",
        "    Loss: 0.08106148988008499\n",
        "    Loss: 0.0801810547709465\n",
        "    Loss: 0.06122320517897606\n",
        "    Loss: 0.06310459971427917\n",
        "    Loss: 0.05681884288787842\n",
        "    Loss: 0.05729678273200989\n",
        "    Loss: 0.05497899278998375\n",
        "    Loss: 0.04439849033951759\n",
        "    Loss: 0.05415581166744232\n",
        "    Loss: 0.06020551547408104\n",
        "    Loss: 0.046830907464027405\n",
        "    Loss: 0.051029372960329056\n",
        "    Loss: 0.0478244312107563\n",
        "    Loss: 0.046767622232437134\n",
        "    Loss: 0.04305662214756012\n",
        "    Loss: 0.05216279625892639\n",
        "    Loss: 0.04748568311333656\n",
        "    Loss: 0.05107741802930832\n",
        "    Loss: 0.04588869959115982\n",
        "    Loss: 0.043014321476221085\n",
        "    Loss: 0.046371955424547195\n",
        "    Loss: 0.04952816292643547\n",
        "    Loss: 0.04472338408231735\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8337c82",
      "metadata": {
        "id": "a8337c82"
      },
      "source": [
        "サンプリング（推論）\n",
        "モデルからサンプリングするには、上で定義したsample関数を使えばいいのです。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3d8a814",
      "metadata": {
        "id": "f3d8a814"
      },
      "outputs": [],
      "source": [
        "# サンプル画像64枚\n",
        "samples = sample(model, image_size=image_size, batch_size=64, channels=channels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_s-Al2lJ2c8T",
      "metadata": {
        "id": "_s-Al2lJ2c8T"
      },
      "outputs": [],
      "source": [
        "# その中から1枚表示する\n",
        "random_index = 5\n",
        "plt.imshow(samples[-1][random_index].reshape(image_size, image_size, channels), cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26ad579f",
      "metadata": {
        "id": "26ad579f"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1ytnzS7IW7ortC6ub85q7nud1IvXe2QTE\" width=\"300\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0k4H1fmlKvzR",
      "metadata": {
        "id": "0k4H1fmlKvzR"
      },
      "source": [
        "どうやら、いい感じのTシャツを生成できるようです！なお、今回学習させたデータセットは、かなり低解像度（28x28）であることに留意してください。\n",
        "\n",
        "また、ノイズ除去の過程をGIFで作成することもできます。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "spE1I9aVNwzZ",
      "metadata": {
        "id": "spE1I9aVNwzZ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.animation as animation\n",
        "\n",
        "random_index = 53\n",
        "\n",
        "fig = plt.figure()\n",
        "ims = []\n",
        "for i in range(timesteps):\n",
        "    im = plt.imshow(samples[i][random_index].reshape(image_size, image_size, channels), cmap=\"gray\", animated=True)\n",
        "    ims.append([im])\n",
        "\n",
        "animate = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)\n",
        "animate.save('diffusion.gif')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b02eb802",
      "metadata": {
        "id": "b02eb802"
      },
      "source": [
        "フォローアップの読み方\n",
        "なお、DDPMの論文では、拡散モデルが（無）条件付き画像生成の有望な方向性であることが示されました。これはその後、（非常に）改善され、特にテキスト条件付き画像生成のために改善されました。以下に、いくつかの重要な（しかし網羅的ではない）後続の研究を列挙する。\n",
        "\n",
        "Improved Denoising Diffusion Probabilistic Models(Nichol et al., 2021): 条件付き分布の分散を（平均以外に）学習することが性能向上に役立つことを発見した。\n",
        "高忠実度画像生成のためのカスケード拡散モデル(Ho et al., 2021): 高忠実度画像合成のために解像度を上げる画像を生成する複数の拡散モデルのパイプラインからなるカスケード拡散を導入する\n",
        "画像合成において拡散モデルがGANに勝る(Dhariwal et al., 2021): U-Netアーキテクチャを改善し、分類器ガイダンスを導入することで、拡散モデルが現在の最新鋭の生成モデルより優れた画像サンプル品質を達成できることを示す。\n",
        "Classifier-Free Diffusion Guidance(Ho et al., 2021): 条件付き拡散モデルと無条件拡散モデルを単一のニューラルネットワークで共同学習することにより、拡散モデルのガイドに分類器が不要であることを示す。\n",
        "CLIP Latentsによる階層的テキスト条件付き画像生成(DALL-E 2)(Ramesh et al., 2022): 事前にテキストキャプションをCLIP画像埋め込みに変換し、その後拡散モデルにより画像にデコードする\n",
        "フォトリアリスティックなテキストから画像への拡散モデルと深い言語理解（ImageGen）（Sahariaら、2022）：事前に学習した大規模な言語モデル（T5など）とカスケード拡散を組み合わせることで、テキストから画像の合成にうまく機能することを示しています。\n",
        "なお、このリストには、執筆時点（2022年6月7日）までの重要な作品しか含まれていません。\n",
        "\n",
        "今のところ、拡散モデルの主な（おそらく唯一の）欠点は、画像を生成するために複数のフォワードパスが必要なことだと思われます（GANのような生成モデルではそうではありません）。しかし、わずか10ステップのノイズ除去で忠実度の高い画像を生成する研究が進められています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YinXsM62JYjn",
      "metadata": {
        "id": "YinXsM62JYjn"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6fe49a34",
        "2d747688",
        "5153024b",
        "592aa765",
        "9ff47fbb",
        "51d9a24c",
        "9a8031b0",
        "06b3fad0",
        "a30368b2",
        "cc01c63b",
        "f70235f8",
        "b02eb802"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}