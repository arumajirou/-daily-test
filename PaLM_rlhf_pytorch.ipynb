{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOMqhrYTCsUPbJ1gQ7SrpOW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumajirou/-daily-test/blob/main/PaLM_rlhf_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PaLM + RLHF - Pytorch (ワイプ)**\n",
        "\n",
        "---\n",
        "RLHF (Reinforcement Learning with Human Feedback) をPaLMアーキテクチャの上に実装しています。RETROのように検索機能も追加する予定。\n",
        "\n"
      ],
      "metadata": {
        "id": "9Gp4XttcHu_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LoRA(低ランク適応) **\n",
        "\n",
        "**Low-Rank Adaptation of Large Language Models**\n",
        "\n",
        "\n",
        "**大規模言語モデルの低ランク適応**"
      ],
      "metadata": {
        "id": "XSjwyUWnKrKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **インストール**"
      ],
      "metadata": {
        "id": "04j9it8jH4FF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgA-lkihHnwa",
        "outputId": "62696242-eb0b-486e-eaee-ca4107ea8cca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting palm-rlhf-pytorch\n",
            "  Downloading PaLM_rlhf_pytorch-0.0.35-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from palm-rlhf-pytorch) (4.64.1)\n",
            "Collecting einops>=0.6\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 515 kB/s \n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.15.0-py3-none-any.whl (191 kB)\n",
            "\u001b[K     |████████████████████████████████| 191 kB 12.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6 in /usr/local/lib/python3.8/dist-packages (from palm-rlhf-pytorch) (1.13.0+cu116)\n",
            "Collecting beartype\n",
            "  Downloading beartype-0.11.0-py3-none-any.whl (702 kB)\n",
            "\u001b[K     |████████████████████████████████| 702 kB 64.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.6->palm-rlhf-pytorch) (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate->palm-rlhf-pytorch) (1.21.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate->palm-rlhf-pytorch) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate->palm-rlhf-pytorch) (21.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate->palm-rlhf-pytorch) (5.4.8)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->accelerate->palm-rlhf-pytorch) (3.0.9)\n",
            "Installing collected packages: einops, beartype, accelerate, palm-rlhf-pytorch\n",
            "Successfully installed accelerate-0.15.0 beartype-0.11.0 einops-0.6.0 palm-rlhf-pytorch-0.0.35\n"
          ]
        }
      ],
      "source": [
        "!pip install palm-rlhf-pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **使用方法**\n",
        "**初期訓練**\n",
        "PaLMは、他の自己回帰変換器と同様に"
      ],
      "metadata": {
        "id": "8zmfCCnAH68z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from palm_rlhf_pytorch import PaLM\n",
        "\n",
        "palm = PaLM(\n",
        "    num_tokens = 20000,\n",
        "    dim = 512,\n",
        "    depth = 12\n",
        ").cuda()\n",
        "\n",
        "seq = torch.randint(0, 20000, (1, 2048)).cuda()\n",
        "\n",
        "loss = palm(seq, return_loss = True)\n",
        "loss.backward()\n",
        "\n",
        "# 多くの訓練の後、順序配列を生成できるようになりました\n",
        "\n",
        "generated = palm.generate(2048) # (1, 2048)"
      ],
      "metadata": {
        "id": "v_2XtCxOHr6I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- そして、**人間の反応**に基づき、**報酬モデル(報酬計算式／報酬計算方法)を学習**させます。\n",
        "- 元の論文では、**事前学習した変換器**から**報酬モデル(報酬計算式／報酬計算方法)**を**過学習させず**に微調整することはできませんでしたが、まだオープンな研究なので、とにかく**LoRA(Low-Rank Adaptation ,低ランク適応)**で微調整する選択を与えています。"
      ],
      "metadata": {
        "id": "JE0gp0ggIU5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from palm_rlhf_pytorch import PaLM, RewardModel\n",
        "\n",
        "palm = PaLM(\n",
        "    num_tokens = 20000,\n",
        "    dim = 512,\n",
        "    depth = 12,\n",
        "    causal = False\n",
        ")\n",
        "\n",
        "reward_model = RewardModel(\n",
        "    palm,\n",
        "    num_binned_output = 5 # 1 から 5 までの評価を宣言する\n",
        ").cuda()\n",
        "\n",
        "# 疑似データ\n",
        "\n",
        "seq = torch.randint(0, 20000, (1, 1024)).cuda()\n",
        "prompt_mask = torch.zeros(1, 1024).bool().cuda() # 順序配列のどの部分がプロンプトで、どの部分が応答であるか\n",
        "labels = torch.randint(0, 5, (1,)).cuda()\n",
        "\n",
        "# 訓練\n",
        "\n",
        "loss = reward_model(seq, prompt_mask = prompt_mask, labels = labels)\n",
        "loss.backward()\n",
        "\n",
        "# 多くの訓練の後\n",
        "\n",
        "reward = reward_model(seq, prompt_mask = prompt_mask)"
      ],
      "metadata": {
        "id": "cjq_nfqaIn_F"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from palm_rlhf_pytorch import PaLM, RewardModel, RLHFTrainer\n",
        "\n",
        "# 事前に訓練されたPaLMをロードする\n",
        "\n",
        "palm = PaLM(\n",
        "    num_tokens=256,\n",
        "    dim=512,\n",
        "    depth=8\n",
        ").cuda()\n",
        "\n",
        "palm.load('./path/to/pretrained/palm.pt')\n",
        "\n",
        "# 事前訓練済みの報酬モードをロードします\n",
        "\n",
        "reward_model = RewardModel(\n",
        "    palm,\n",
        "    num_binned_output = 5\n",
        ").cuda()\n",
        "\n",
        "reward_model.load('./path/to/pretrained/reward_model.pt')\n",
        "\n",
        "# 強化学習のプロンプトのリストを準備する\n",
        "\n",
        "prompts = torch.randint(0, 256, (50000, 512)).cuda() # 50k prompts\n",
        "\n",
        "# すべてをトレーナーに渡してトレーニングする\n",
        "\n",
        "trainer = RLHFTrainer(\n",
        "    palm = palm,\n",
        "    reward_model = reward_model,\n",
        "    prompt_token_ids = prompts\n",
        ")\n",
        "\n",
        "trainer.train(num_episodes = 50000)\n",
        "\n",
        "# それで成功したら…\n",
        "# 10個のサンプルを生成し、報酬モデルを使用して最高のものを返します\n",
        "\n",
        "answer = trainer.generate(2048, prompt = prompts[0], num_samples = 10) # (<= 2048,)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "g7Gt-n5QIEl0",
        "outputId": "c903ab54-efc4-4cbb-b924-d9ad041676e0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b970a2eab118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m ).cuda()\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mpalm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./path/to/pretrained/palm.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 事前訓練済みの報酬モードをロードします\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/palm_rlhf_pytorch/palm_rlhf_pytorch.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JQnWBvkAM6bL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}