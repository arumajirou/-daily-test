{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMr4rIKRgPLsX81mQ2Xy//2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumajirou/-daily-test/blob/main/20230622.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qa5bSbYI4HuK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "こんにちは。このURLの内容を小学生にもわかりやすく教えるための文章を作成してみました。以下に示します。\n",
        "\n",
        "# 大規模言語モデルの出力スピードを最大24倍に高めるライブラリ「vLLM」が登場、メモリ効率を高める新たな仕組み「PagedAttention」とは？\n",
        "\n",
        "## 目次\n",
        "- 大規模言語モデルとは何か？\n",
        "- ライブラリとは何か？\n",
        "- vLLMとは何か？\n",
        "- PagedAttentionとは何か？\n",
        "- まとめ\n",
        "- 用語の説明\n",
        "\n",
        "## 大規模言語モデルとは何か？\n",
        "大規模言語モデルとは、人間の言葉をコンピューターに理解させるための仕組みです。コンピューターは、たくさんの文章や本を読んで、言葉の意味や使い方を学習します。この学習した知識をもとに、コンピューターは新しい文章を作ったり、質問に答えたり、会話したりすることができます。大規模言語モデルは、読んだ文章や本の量がとても多いほど、言葉の知識が豊富になります。しかし、その分、コンピューターが必要とするメモリや処理速度も増えてしまいます。メモリとは、コンピューターが一時的にデータを保存する場所です。処理速度とは、コンピューターが一秒間にどれだけの計算や操作ができるかを表す数値です。\n",
        "\n",
        "## ライブラリとは何か？\n",
        "ライブラリとは、コンピューターに特定の機能を提供するためのプログラムの集まりです。例えば、大規模言語モデルを使うためには、ライブラリが必要になります。ライブラリには、大規模言語モデルの種類や構造、学習方法や出力方法などが決められています。ライブラリを使うことで、プログラマーは自分で細かい設定や計算をしなくても、簡単に大規模言語モデルを利用することができます。しかし、ライブラリによっては、メモリや処理速度の効率が悪い場合もあります。その場合、コンピューターは大規模言語モデルを扱うのに苦労してしまいます。\n",
        "\n",
        "## vLLMとは何か？\n",
        "vLLMとは、新しい種類のライブラリです。vLLMは、「PagedAttention」という仕組みを使って、メモリや処理速度の効率を高めています。vLLMを使うことで、従来のライブラリよりも24倍も速く大規模言語モデルの出力ができるようになりました。出力とは、大規模言語モデルが作った文章や答えなどのことです。vLLMは、VicunaやKoalaやLLaMAなどの大規模言語モデルに対応しています。VicunaやKoalaやLLaMAは、それぞれ異なる目的や特徴を持った大規模言語モデルです。例えば、Vicunaは文章を要約したり、Koalaは文章を作ったり、LLaMAは質問に答えたりすることができます。\n",
        "\n",
        "## PagedAttentionとは何か？\n",
        "PagedAttentionとは、vLLMが使っている仕組みです。PagedAttentionは、メモリの使い方を工夫することで、大規模言語モデルの出力を速くすることができます。PagedAttentionの仕組みを説明するには、まず「アテンション」という概念を理解する必要があります。アテンションとは、大規模言語モデルが文章を作るときに使う技術です。アテンションでは、入力された文章の各単語に対して、重要度や関係性を数値で表します。この数値をキーと値と呼びます。キーと値は、メモリに保存されます。これをKVキャッシュと呼びます。KVキャッシュは、大規模言語モデルが新しい単語を選ぶときに参照されます。しかし、KVキャッシュは、入力された文章の長さによってサイズが変わります。サイズが大きくなると、メモリに入りきらなかったり、メモリの空きスペースが無駄になったりします。これがメモリの効率を悪くする原因です。\n",
        "\n",
        "PagedAttentionでは、この問題を解決するために、KVキャッシュを一定の長さごとに分割して扱います。これをブロックと呼びます。ブロックは、メモリの空きスペースに自由に配置できます。ブロックの場所は、ブロックテーブルというもので管理されます。ブロックテーブルは、どのブロックがどこにあるかを記録しています。ブロックテーブルを見ることで、大規模言語モデルは必要なブロックを素早く見つけることができます。また、新しいブロックが必要になったら、メモリの空きスペースに割り当てられます。これによって、メモリの無駄な使用や断片化を防ぐことができます。\n",
        "\n",
        "さらに、PagedAttentionでは、複数の出力を同時に行う場合にもメモリ効率を高めることができます。具体的には、同じ内容のKVキャッシュを複数回メモリに保存しないようにします。例えば、「こんにちは」という入力文から、「Hello」と「Bonjour」という出力文を同時に作る場合、KVキャッシュは同じ内容になります。従来のライブラリでは、このKVキャッシュを2回分メモリに保存してしまいますが、PagedAttentionでは1回分だけ保存します。そして、2つの出力文が同じKVキャッシュを参照するようにします。これによって、メモリの使用量を半分に減らすことができます。\n",
        "\n",
        "## まとめ\n",
        "- 大規模言語モデルとは、人間の言葉をコンピューターに理解させるための仕組みです。\n",
        "-\n",
        "\n",
        "ソース: Bing との会話 2023/6/22\n",
        "(1) 大規模言語モデルの出力スピードを最大24倍に高めるライブラリ .... https://gigazine.net/news/20230622-vllm-paged-attention/.\n",
        "(2) [B!] 大規模言語モデルの出力スピードを最大24倍に高める .... https://b.hatena.ne.jp/entry/s/gigazine.net/news/20230622-vllm-paged-attention/.\n",
        "(3) ItaruSugawara on Twitter: \"大規模言語モデルの出力スピードを .... https://twitter.com/ItaruSugawara/status/1671763126096920576."
      ],
      "metadata": {
        "id": "UcQjaiDQ4Imp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PagedAttentionとは、vLLMが使っている仕組みです。PagedAttentionは、メモリの使い方を工夫することで、大規模言語モデルの出力を速くすることができます。PagedAttentionの仕組みを説明するには、まず「アテンション」という概念を理解する必要があります。アテンションとは、大規模言語モデルが文章を作るときに使う技術です。アテンションでは、入力された文章の各単語に対して、重要度や関係性を数値で表します。この数値をキーと値と呼びます。キーと値は、メモリに保存されます。これをKVキャッシュと呼びます。KVキャッシュは、大規模言語モデルが新しい単語を選ぶときに参照されます。しかし、KVキャッシュは、入力された文章の長さによってサイズが変わります。サイズが大きくなると、メモリに入りきらなかったり、メモリの空きスペースが無駄になったりします。これがメモリの効率を悪くする原因です。\n",
        "\n",
        "PagedAttentionでは、この問題を解決するために、KVキャッシュを一定の長さごとに分割して扱います。これをブロックと呼びます。ブロックは、メモリの空きスペースに自由に配置できます。ブロックの場所は、ブロックテーブルというもので管理されます。ブロックテーブルは、どのブロックがどこにあるかを記録しています。ブロックテーブルを見ることで、大規模言語モデルは必要なブロックを素早く見つけることができます。また、新しいブロックが必要になったら、メモリの空きスペースに割り当てられます。これによって、メモリの無駄な使用や断片化を防ぐことができます。\n",
        "\n",
        "さらに、PagedAttentionでは、複数の出力を同時に行う場合にもメモリ効率を高めることができます。具体的には、同じ内容のKVキャッシュを複数回メモリに保存しないようにします。例えば、「こんにちは」という入力文から、「Hello」と「Bonjour」という出力文を同時に作る場合、KVキャッシュは同じ内容になります。従来のライブラリでは、このKVキャッシュを2回分メモリに保存してしまいますが、PagedAttentionでは1回分だけ保存します。そして、2つの出力文が同じKVキャッシュを参照するようにします。これによって、メモリの使用量を半分に減らすことができます。"
      ],
      "metadata": {
        "id": "VJPj3R874Lht"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Hau5uuo4JKl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}