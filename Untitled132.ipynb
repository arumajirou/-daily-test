{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMbp4wXpF6qU50F2UGC9Toc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumajirou/-daily-test/blob/main/Untitled132.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUcoeTi5K1SA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "こんにちは。この文章では、以下のURLにある内容を小学生でもわかるように説明します。\n",
        "\n",
        "https://github.com/facebookresearch/localrf\n",
        "\n",
        "このURLは、コンピューターで写真や動画から立体的な景色を作る方法についての研究のページです。この方法はNeRF（ナーフ）という名前がついています。NeRFはNeural Radiance Field（ニューラル・レディアンス・フィールド）の略です。ニューラルとは、コンピューターが人間の脳のように学習することです。レディアンスとは、光の明るさや色のことです。フィールドとは、空間の中にあるもののことです。つまり、NeRFはコンピューターが空間の中にある光の明るさや色を学習する方法ということです。\n",
        "\n",
        "では、具体的にどうやってコンピューターが学習するのでしょうか？まず、以下のような手順で説明します。\n",
        "\n",
        "- 目次\n",
        "  - NeRFでできること\n",
        "  - NeRFにおける問題の定式化\n",
        "  - ボリュームレンダリングによる画像化\n",
        "  - その他の工夫\n",
        "  - まとめ\n",
        "  - 用語の説明\n",
        "\n",
        "- NeRFでできること\n",
        "\n",
        "NeRFは、100枚くらいの写真や動画から、その景色の立体的な形を復元し、新しい角度から見た写真や動画を作り出すことができます。例えば、こんな感じです。\n",
        "\n",
        "![NeRFで作られた自由視点映像](https://techblog.leapmind.io/blog/wp-content/uploads/2021/02/nerf.gif)\n",
        "\n",
        "この写真や動画は、実際に撮影したものではありません。コンピューターが学習した景色を新しく作り出したものです。すごいでしょう？\n",
        "\n",
        "学習に使うのは写真や動画だけなので、カメラで撮ったものなら何でも現実の景色にも適用できます。例えば、インターネットから集めた観光地の写真から、自分がその場所に行ったような映像を作ることもできます。\n",
        "\n",
        "- NeRFにおける問題の定式化\n",
        "\n",
        "では、どうやってコンピューターが景色を学習するのでしょうか？景色を立体的な物体と考えてみましょう。立体的な物体は、たくさんの三角形や四角形で作られています。それぞれの三角形や四角形には色がついています。これをポリゴン（多角形）と呼びます。\n",
        "\n",
        "![ポリゴンで表現された物体](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Polygonal_mesh_of_a_dog.png/1200px-Polygonal_mesh_of_a_dog.png)\n",
        "\n",
        "しかし、立体的な物体を表現する方法はポリゴンだけではありません。NeRFでは、立体的な物体をニューラルネットワークというコンピューターが学習する仕組みで表現します。ニューラルネットワークは、入力と出力の関係を学習します。例えば、入力が「りんご」という言葉で、出力が「赤い」という色だとします。すると、コンピューターは「りんご」が「赤い」色を持つという関係を学習します。\n",
        "\n",
        "NeRFでは、入力が物体の中のある点の座標とカメラの向きで、出力がその点の色と密度（どれくらい透明か）です。座標とは、物体の中でどこにあるかを示す数字です。カメラの向きとは、その点を見るときにどこから見るかを示す数字です。例えば、入力が「(1, 2, 3)と(30, 45)」で、出力が「(255, 0, 0)と0.8」だとします。すると、コンピューターは「(1, 2, 3)という座標にある点は、(30, 45)というカメラの向きから見ると、(255, 0, 0)という色（赤色）で、0.8という密度（少し透明）を持つ」という関係を学習します。\n",
        "\n",
        "![NeRFの入出力](https://techblog.leapmind.io/blog/wp-content/uploads/2021/02/nerf_input_output.png)\n",
        "\n",
        "このようにして、コンピューターは物体の中にあるたくさんの点の色と密度を学習します。これがNeRFの基本的な考え方です。\n",
        "\n",
        "- ボリュームレンダリングによる画像化\n",
        "\n",
        "では、コンピューターが学習した物体から新しい角度から見た写真や動画を作るにはどうすればいいでしょうか？それにはボリュームレンダリングという方法を使います。ボリュームレンダリングとは、物体の中にある点の色と密度を使って画像を作る方法です。\n",
        "\n",
        "具体的には、カメラから物体に向かって光線（ひかりせん）を飛ばしてみます。光線は物体の中に入っていく途中でたくさんの点に当たります。それぞれの点には色と密度があります。色はそのまま光線の色になります。密度は光線がどれくらい透過するかを決めます。密度が大きいほど光線は透過しにくくなります。\n",
        "\n",
        "![ボリュームレンダリング](https://techblog.leapmind.io/blog/wp-content/uploads/2021/02/volume_rendering.png)\n",
        "\n",
        "このようにして、光線が当たった点の色と密度を足し合わせていきます。すると、カメラから見える一つの点の色が決まります。これをカメラから見えるすべての点について行うことで、画像が完成します。\n",
        "\n",
        "- その他の工夫\n",
        "\n",
        "NeRFでは、さらにいくつかの工夫をしています。ここでは簡単に紹介します。\n",
        "\n",
        "- Positional Encoding: 入力する座標やカメラの向きを高次元に変換することで、細かい形や色を学習しやすくする。\n",
        "- 階層的ボリュームサンプリング: 計算量を減らすために、粗いネットワークと細かいネットワークの2種類を使う。粗いネットワークで重要な\n",
        "\n",
        "かしこまりました。では、続きを説明します。\n",
        "\n",
        "- まとめ\n",
        "\n",
        "NeRFは、写真や動画から立体的な景色を作る方法です。ニューラルネットワークというコンピューターが学習する仕組みを使って、物体の中にある点の色と密度を学習します。その後、ボリュームレンダリングという方法で、新しい角度から見た画像を作り出します。NeRFは、現実の景色にも適用できるし、高精細な画像を生成できるという特徴があります。\n",
        "\n",
        "- 用語の説明\n",
        "\n",
        "- ニューラルネットワーク: コンピューターが人間の脳のように学習する仕組みです。入力と出力の関係を学習します。\n",
        "- レディアンス: 光の明るさや色のことです。\n",
        "- フィールド: 空間の中にあるもののことです。\n",
        "- 座標: 物体の中でどこにあるかを示す数字です。\n",
        "- カメラの向き: 物体を見るときにどこから見るかを示す数字です。\n",
        "- 色: 光の波長や反射率によって決まる性質です。赤や青や緑などがあります。\n",
        "- 密度: 物体がどれくらい透明かを示す数字です。0から1までの値で、0は完全に透明で、1は完全に不透明です。\n",
        "- ポリゴン: 三角形や四角形などの多角形です。立体的な物体を表現するときに使われます。\n",
        "- ボリュームレンダリング: 物体の中にある点の色と密度を使って画像を作る方法です。\n",
        "- 光線: 光が進む道筋です。カメラから物体に向かって飛ばします。\n",
        "- Positional Encoding: 入力する座標やカメラの向きを高次元に変換することです。高次元とは、たくさんの数字で表すことです。これをすると、細かい形や色を学習しやすくなります。\n",
        "- 階層的ボリュームサンプリング: 計算量を減らすために、粗いネットワークと細かいネットワークの2種類を使うことです。粗いネットワークで重要な領域を選んで、細かいネットワークで詳しく計算します。\n",
        "\n",
        "このURLは、先ほど説明した研究の詳細を書いた論文（ろんぶん）です。論文とは、研究者が自分たちの研究の内容や結果をまとめた文章です。論文は、他の研究者に自分たちの研究を伝えたり、新しい発見や知識を広めたりするために書かれます。\n",
        "\n",
        "この論文では、以下のようなことが書かれています。\n",
        "\n",
        "- 背景（はいけい）: これまでのNeRFの研究では、カメラの位置や向きが正確にわかっていることが前提（ぜんてい）でした。しかし、実際にはカメラの位置や向きがずれていることもあります。また、NeRFは一つの物体や場所にしか適用できませんでした。しかし、実際にはカメラが動くときには物体や場所が変わることもあります。\n",
        "- 目的（もくてき）: この研究では、カメラの位置や向きが正確にわからなくても、NeRFを使って立体的な景色を作る方法を提案（ていあん）します。また、カメラが動くときには物体や場所が変わることにも対応（たいおう）できるようにします。\n",
        "- 方法（ほうほう）: この研究では、以下のような方法を使っています。\n",
        "  - プログレッシブ・オプティマイゼーション: カメラの位置や向きとNeRFを同時（どうじ）に学習します。最初は粗い（あらい）解像度（かいぞうど）で学習して、徐々（じょじょ）に細かくしていきます。これにより、カメラの位置や向きがずれていても学習できます。\n",
        "  - ローカル・レディアンス・フィールド: NeRFを小さく分けて、カメラが見える範囲（はんい）だけ学習します。これにより、カメラが動くときに物体や場所が変わっても学習できます。\n",
        "- 結果（けっか）: この研究では、実際に撮影した動画やインターネットから集めた写真を使って、立体的な景色を作りました。その結果、カメラの位置や向きがずれていても、物体や場所が変わっても、高品質（こうひんしつ）な画像を生成（せいせい）できました。また、他の研究と比較（ひかく）しても優れた性能（せいのう）を示しました。\n",
        "\n",
        "以上で、URLにある内容の要約（ようやく）は終わりです。わかりましたか？\n",
        "\n",
        "ソース: Bing との会話 2023/6/18(1) localrf.github.io/localrf.pdf at master · localrf/localrf.github.io. https://github.com/localrf/localrf.github.io/blob/master/localrf.pdf アクセス日時 2023/6/18.\n",
        "(2) PDFリーダー | Adobe Acrobat Reader. https://www.adobe.com/jp/acrobat/pdf-reader.html アクセス日時 2023/6/18.\n",
        "(3) 【無料】PDFのソフト一覧 - 窓の杜. https://forest.watch.impress.co.jp/library/nav/genre/offc/document_pdf.html アクセス日時 2023/6/18.\n",
        "(4) レポート機能を使ってみよう. https://live.paloaltonetworks.com/twzvq79624/attachments/twzvq79624/Public-Sector-Information/14/1/NGFW_%E3%83%AC%E3%83%9D%E3%83%BC%E3%83%88%E6%A9%9F%E8%83%BD%E7%B4%B9%E4%BB%8B1.pdf アクセス日時 2023/6/18.\n",
        "(5) レポートの生成方法 | パロアルトネットワークスな日々. https://pansetech.net/report/ アクセス日時 2023/6/18.\n",
        "\n",
        "以上で、URLにある内容の説明は終わります。わかりましたか？\n",
        "ソース: Bing との会話 2023/6/18(1) 数式を使わずにやさしく解説するNeRF: Neural Radiance Fields. https://techblog.leapmind.io/blog/introduction-to-nerf-without-math/ アクセス日時 2023/6/18.\n",
        "(2) 「NeRF」の最新事例 自由視点映像やリアルタイムレンダリングも .... https://xtrend.nikkei.com/atcl/contents/technology/00007/00034/ アクセス日時 2023/6/18.\n",
        "(3) Neural Radiance Field (NeRF): A Gentle Introduction. https://datagen.tech/guides/synthetic-data/neural-radiance-field-nerf/ アクセス日時 2023/6/18.\n",
        "(4) NeRF From Nothing: A Tutorial with PyTorch | Towards Data .... https://towardsdatascience.com/its-nerf-from-nothing-build-a-vanilla-nerf-with-pytorch-7846e4c45666 アクセス日時 2023/6/18."
      ],
      "metadata": {
        "id": "RrkbLJRTK2GR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C-xqsH7YRlKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "こんにちは、私はニュースライターで小学校の先生でもあります。今日は、以下のURLにある内容を、小学生でもわかるように教えていきたいと思います。\n",
        "\n",
        "https://github.com/hiyouga/LLaMA-Efficient-Tuning\n",
        "\n",
        "このURLは、GitHubというウェブサイトの一部です。GitHubとは、プログラミングのコードを管理したり、共有したりするためのウェブサイトです。プログラミングとは、コンピューターに指示を出すための言語です。このURLにある内容は、LLaMAというプログラミングのコードで、大規模言語モデルというものを作ったり、使ったりするためのものです。\n",
        "\n",
        "では、まず目次から見ていきましょう。\n",
        "\n",
        "目次\n",
        "- 大規模言語モデルとは何か\n",
        "- LLaMAとは何か\n",
        "- LLaMAを使うために必要なもの\n",
        "- LLaMAを使ってできること\n",
        "- まとめ\n",
        "- 技術用語の説明\n",
        "\n",
        "## 大規模言語モデルとは何か\n",
        "大規模言語モデルとは、人間が使う言語をコンピューターが理解したり、生成したりするためのプログラムです。例えば、文章を読んで質問に答えたり、話題に合わせて会話したり、文章を要約したりすることができます。大規模言語モデルは、インターネット上にあるたくさんの文章を読んで学習します。そのため、パラメーター数というものが多いほど、より複雑なことができるようになります。パラメーター数とは、コンピューターが文章を理解したり生成したりするために使う変数（へんすう）の量（かず）のことです。変数とは、値（あたい）が変わることができるものです。例えば、「x=3」という式（しき）では、「x」が変数で、「3」が値です。「x」は「3」以外の値にもなることができます。\n",
        "\n",
        "## LLaMAとは何か\n",
        "LLaMAとは、Meta社という会社が開発した大規模言語モデルの一種です。Meta社とは、Facebook社という会社が改名した新しい会社です。Facebook社とは、SNS（ソーシャル・ネットワーキング・サービス）というウェブサイトを作った会社です。SNSとは、インターネット上で友だちや知り合いとつながったり、情報を交換したりするウェブサイトです。LLaMAは、パラメーター数によって4つのモデルがあります。\n",
        "- 7B（70億）\n",
        "- 13B（130億）\n",
        "- 30B（300億）\n",
        "- 65B（650億）\n",
        "\n",
        "これらの数字はパラメーター数を表しています。例えば、「7B」というのは、「7,000,000,000」という数字です。これは70億と読みます。「B」は「billion」という英単語で、「10億」を意味します。「M」は「million」という英単語で、「100万」を意味します。LLaMAは、WikipediaやCommon Crawl、C4というウェブサイトにある文章を読んで学習しています。Wikipediaとは、インターネット上にある百科事典（ひゃっかじてん）です。百科事典とは、さまざまなことについて書かれた本です。Common Crawlとは、インターネット上にあるウェブページを集めたデータベースです。ウェブページとは、インターネット上にある文章や画像や音声などの情報です。データベースとは、データを整理して保存したものです。データとは、情報をコンピューターが扱えるようにしたものです。C4とは、Common Crawlから選んだウェブページを整理したデータセットです。データセットとは、データをまとめたものです。\n",
        "\n",
        "## LLaMAを使うために必要なもの\n",
        "LLaMAを使うためには、以下のものが必要です。\n",
        "- コンピューター\n",
        "- GPU\n",
        "- Python\n",
        "- Node.js\n",
        "- その他のプログラム\n",
        "\n",
        "コンピューターとは、プログラミングのコードを実行したり、データを処理したりする機械です。実行とは、コードに書かれた指示をコンピューターが行うことです。処理とは、データに対して何らかの操作をすることです。GPUとは、グラフィックス・プロセッシング・ユニットの略で、コンピューターの中にある部品の一つです。GPUは、画像や動画などのグラフィックスを表示したり、計算したりするのに得意です。Pythonとは、プログラミングの言語の一種です。プログラミングの言語とは、コンピューターに指示を出すための言語です。Pythonは、簡単でわかりやすい言語であり、多くの人が使っています。Node.jsとは、JavaScriptというプログラミングの言語を使ってコンピューター上でプログラムを実行するためのものです。JavaScriptとは、ウェブページに動きや機能をつけるための言語です。その他のプログラムとは、LLaMAを使うために必要なもので、インターネットからダウンロードできます。\n",
        "\n",
        "## LLaMAを使ってできること\n",
        "LLaMAを使ってできることは以下のようなことです。\n",
        "- (Continually) pre-training\n",
        "- Supervised fine-tuning\n",
        "- Reward model training\n",
        "- PPO training (RLHF)\n",
        "- Evaluation (BLEU and ROUGE_CHINESE)\n",
        "- CLI demo\n",
        "- Web demo\n",
        "- Export model\n",
        "\n",
        "(Continually) pre-trainingとは、LLaMAが学習することです。LLaMAは、インターネット上にある文章を読んで学習しますが、その学習を続けることでより賢くなります。Supervised fine-tuningとは、LLaMAが特定のタスクに対応するように学習することです。例えば、「文章を読んで質問に答える」というタスクがあったら、そのタスクに関係する文章や質問や答えを用意してLLaMAに教えます。Reward model trainingとは、LLaMAが自分で作った文章が良いか悪いか判断するために学習することです。例えば、「面白い話を\n",
        "\n",
        "ソース: Bing との会話 2023/6/18(1) LLaMA - Hugging Face. https://huggingface.co/docs/transformers/main/en/model_doc/llama アクセス日時 2023/6/18.\n",
        "(2) 大規模言語モデル「LLaMA」を自分のPCで実行する - Qiita. https://qiita.com/mine820/items/4c10165cc92211aeed87 アクセス日時 2023/6/18.\n",
        "(3) Metaが大規模言語モデル「LLaMA」を発表、GPT-3に匹敵 .... https://gigazine.net/news/20230227-llama-large-language-model-meta-ai/ アクセス日時 2023/6/18.\n",
        "(4) Metaの大規模言語モデル「LLaMA-65B」のデータが4chanで流出. https://gigazine.net/news/20230306-llama-65b-leaked/ アクセス日時 2023/6/18."
      ],
      "metadata": {
        "id": "Ut0I2hfHRlwk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gzxYIa20K3Kf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}