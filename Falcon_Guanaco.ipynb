{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arumajirou/-daily-test/blob/main/Falcon_Guanaco.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Google colabでFalcon-7bをファインチューンする\n",
        "\n",
        "このGoogle Colabノートブックへようこそ。このノートブックでは、最近のFalcon-7bモデルを1つのGoogle colab上で微調整し、チャットボットにする方法を示します。\n",
        "\n",
        "Hugging FaceエコシステムのPEFTライブラリと、よりメモリ効率の良いファインチューニングのためのQLoRAを活用する予定です。"
      ],
      "metadata": {
        "id": "C2EgqEPDQ8v6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Falcon-7bモデルとは、TIIというアラブ首長国連邦の企業が開発した大規模な言語モデルです²。このモデルは、RefinedWebというインターネット上のテキストを集めたデータセットに、さらに選別されたコーパスを加えて学習されています¹。その結果、他のオープンソースのモデルよりも優れた性能を示しています¹²。Falcon-7bモデルは、7B（70億）個のパラメータを持ち、テキスト生成や要約などのタスクに適用できます¹²。また、Apache 2.0ライセンスで公開されており、商用利用も可能です¹²。\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) tiiuae/falcon-7b · Hugging Face. https://huggingface.co/tiiuae/falcon-7b アクセス日時 2023/6/11.\n",
        "(2) tiiuae/falcon-7b · Hugging Face. https://bing.com/search?q=Falcon-7b+model アクセス日時 2023/6/11.\n",
        "(3) tiiuae/falcon-7b-instruct · Hugging Face. https://huggingface.co/tiiuae/falcon-7b-instruct アクセス日時 2023/6/11.\n",
        "(4) 开源大模型新SOTA！支持免费商用，比LLaMA65B小但更强 .... https://zhuanlan.zhihu.com/p/633022169 アクセス日時 2023/6/11.\n",
        "(5) falcon 7bをgoogle colabで試してみた。 - Zenn. https://zenn.dev/tatsuromurata/articles/35a9937188429c アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "QbnlzpFogKRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apache 2.0ライセンスとは、オープンソースソフトウェアの利用規約の一種です¹。Apacheソフトウェア財団が開発したソフトウェアに適用されているほか、他のソフトウェアや素材にも使われています¹²。このライセンスは、ソフトウェアの使用や改変、再配布などを自由に許可しており、商用利用も可能です¹²³。ただし、以下のような条件や注意事項があります。\n",
        "\n",
        "- ソフトウェアの再配布や改変したものの配布をする場合は、ライセンス条文や著作権表示などを含める必要があります²³。\n",
        "- ソフトウェアに含まれる特許技術を利用する場合は、その特許に関して訴訟を起こさないことが求められます²³。\n",
        "- ソフトウェアの使用や配布に伴うトラブルは自己責任であり、配布元や使用者に責任を負わせることはできません¹²³。\n",
        "- ソフトウェアに関連する商標の使用は許可されません¹²³。\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) Apache License 2.0とは？商用できる？利用条件をやさしく解説 .... https://reikawatanabe.com/whats-apache-license-v2/ アクセス日時 2023/6/11.\n",
        "(2) Apache License, Version 2.0 - The Apache Software Foundation. https://www.apache.org/licenses/LICENSE-2.0.html アクセス日時 2023/6/11.\n",
        "(3) Apache License - Wikipedia. https://ja.wikipedia.org/wiki/Apache_License アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "v3_IEeTQgykA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PEFTライブラリとは、Hugging Faceが開発したライブラリで、事前学習された言語モデルをパラメータ効率的に微調整するための技術を提供しています¹。PEFTは、アダプタ層と呼ばれる追加のパラメータを言調モデルに適用することで、全てのパラメータを微調整する必要なく、下流のタスクに適応させることができます¹。PEFTは、TransformersやAccelerateとシームレスに統合されており、最新かつ高性能なモデルを簡単かつスケーラブルに利用できます¹³。PEFTは、LoRAやPrefix Tuningなどの最新のPEFT技術をサポートしています¹²。\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) huggingface/peft: 🤗 PEFT: State-of-the-art Parameter-Efficient .... https://github.com/huggingface/peft アクセス日時 2023/6/11.\n",
        "(2) Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU .... https://huggingface.co/blog/trl-peft アクセス日時 2023/6/11.\n",
        "(3) PEFT - Hugging Face. https://huggingface.co/docs/peft アクセス日時 2023/6/11.\n",
        "(4) Parameter-Efficient Fine-Tuning using 🤗 PEFT - Hugging Face. https://huggingface.co/blog/peft アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "IZa_7sqGgaGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "QLoRAとは、事前学習された言語モデルを4ビットに量子化して微調整するための技術です¹。QLoRAは、Low Rank Adapters（LoRA）と呼ばれる追加のパラメータを言語モデルに適用することで、全てのパラメータを微調整する必要なく、下流のタスクに適応させることができます¹²。QLoRAは、bitsandbytesというライブラリを使って量子化を行い、Hugging FaceのPEFTやtransformersというライブラリと統合されています¹³。QLoRAは、メモリ使用量を大幅に削減し、48GBのGPUで65B（650億）個のパラメータを持つ言語モデルを微調整できるようにするなど、いくつかの革新的な技術を導入しています¹²。\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) QLoRA: Efficient Finetuning of Quantized LLMs - GitHub. https://github.com/artidoro/qlora アクセス日時 2023/6/11.\n",
        "(2) [2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs. https://arxiv.org/abs/2305.14314 アクセス日時 2023/6/11.\n",
        "(3) QLoRA：一种高效LLMs微调方法，48G内存可调65B 模型 .... https://zhuanlan.zhihu.com/p/632229856 アクセス日時 2023/6/11.\n",
        "(4) 使用bitsandbytes、4 位量化和 QLoRA 使 LLM 更易于访问 - 知乎. https://zhuanlan.zhihu.com/p/632287465 アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "1iVG-3ErhBIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "事前学習された言語モデルを4ビットに量子化して微調整するというのは、言語モデルのパラメータを4ビットの数値に変換してメモリや計算量を削減しながら、下流のタスクに合わせて学習するということです¹²。言語モデルのパラメータは通常32ビットや16ビットで表現されるので、4ビットに量子化することで、大規模な言語モデルを扱う際のコストや時間を大幅に節約できます¹²。ただし、量子化によって精度が低下する可能性があるため、微調整の際には、量子化されたパラメータだけでなく、アダプタ層やプレフィックス層などの追加のパラメータも学習することで、性能を向上させることができます¹²³。\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) Parameter-efficient fine-tuning of large-scale pre-trained .... https://www.nature.com/articles/s42256-023-00626-4 アクセス日時 2023/6/11.\n",
        "(2) [2109.01652] Finetuned Language Models Are Zero-Shot Learners. https://arxiv.org/abs/2109.01652 アクセス日時 2023/6/11.\n",
        "(3) Fine-tune a pretrained model - Hugging Face. https://huggingface.co/docs/transformers/training アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "n78cHNu8hcgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Low Rank Adaptersとは、以下のようなものです。\n",
        "\n",
        "- 言語モデルを微調整するときに使う追加のパラメータのことです¹²。\n",
        "- 言語モデルの各層に挿入され、低いランク（次元）を持つ行列で構成されています¹²。\n",
        "- 言語モデルの元のパラメータは固定されており、Low Rank Adaptersだけが学習されます¹²。\n",
        "- Low Rank Adaptersを使うと、言語モデルのパラメータ数やメモリ使用量を大幅に減らしながら、下流のタスクに合わせて性能を向上させることができます¹²³。\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) LoRA: Low-Rank Adaptation of Large Language Models. https://arxiv.org/abs/2106.09685 アクセス日時 2023/6/11.\n",
        "(2) ORA: Efficient Finetuning of Quantized LLMs - arXiv.org. https://arxiv.org/pdf/2305.14314.pdf アクセス日時 2023/6/11.\n",
        "(3) Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU .... https://huggingface.co/blog/trl-peft アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "ki1oi7LNh14w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "アダプタ層やプレフィックス層などの追加のパラメータも学習することで、性能を向上させることができますというのは、以下のような意味です。\n",
        "\n",
        "- アダプタ層とは、言語モデルの各層に挿入される小さな層のことです¹²。\n",
        "- プレフィックス層とは、言語モデルの入力に追加される小さな層のことです³⁴。\n",
        "- アダプタ層やプレフィックス層は、言語モデルの元のパラメータを固定したまま、下流のタスクに合わせて学習されます¹²³⁴。\n",
        "- アダプタ層やプレフィックス層を使うと、言語モデルのパラメータ数やメモリ使用量を大幅に減らしながら、下流のタスクにおける性能を向上させることができます¹²³⁴。\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) Prefix-Tuning: Optimizing Continuous Prompts for Generation. https://arxiv.org/abs/2101.00190 アクセス日時 2023/6/11.\n",
        "(2) Finetuning LLMs Efficiently with Adapters. https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters アクセス日時 2023/6/11.\n",
        "(3) Adapter-Transformers v3 - Unifying Efficient Fine-Tuning. https://adapterhub.ml/blog/2022/03/adapter-transformers-v3-unifying-efficient-fine-tuning/ アクセス日時 2023/6/11.\n",
        "(4) Understanding Parameter-Efficient Finetuning of Large .... https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "omSCb2XUiL54"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "言語モデルの各層に挿入され、低いランク（次元）を持つ行列で構成されていますというのは、以下のような意味です。\n",
        "\n",
        "- 言語モデルとは、文章や単語などの言語のパターンを学習するプログラムのことです¹。\n",
        "- 言語モデルは、多くの場合、トランスフォーマーと呼ばれる構造を持っています²。\n",
        "- トランスフォーマーは、複数の層からなり、各層は行列と呼ばれる数字の表で表されます²³。\n",
        "- 行列のランクとは、その行列が持つ情報量や複雑さを表す数値です 。\n",
        "- ランクが低いということは、行列が持つ情報量や複雑さが少ないということです 。\n",
        "- アダプタ層と呼ばれる小さな行列を言語モデルの各層に挿入することで、言語モデルを微調整することができます 。\n",
        "- アダプタ層は、ランクが低い行列で構成されているため、パラメータ数やメモリ使用量を抑えることができます 。\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) Low-rank approximation - Wikipedia. https://en.wikipedia.org/wiki/Low-rank_approximation アクセス日時 2023/6/11.\n",
        "(2) 行列の階数 - Wikipedia. https://ja.wikipedia.org/wiki/%E8%A1%8C%E5%88%97%E3%81%AE%E9%9A%8E%E6%95%B0 アクセス日時 2023/6/11.\n",
        "(3) Low-rank matrix approximations - Wikipedia. https://en.wikipedia.org/wiki/Low-rank_matrix_approximations アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "jfDj4INEid0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "こんにちは、この文章は、web search resultsから得られた情報をもとに作成しました。アダプタ層やプレフィックス層、量子化されたパラメータについて、もっと具体的かつわかりやすく説明します。\n",
        "\n",
        "- アダプタ層とは、言語モデルの各層の間に入れる小さな層のことです。言語モデルとは、文章や単語をコンピューターが理解できるように数値化して学習する仕組みのことです。言語モデルは、たくさんの層からなる構造を持っていますが、その層の数や大きさによっては、学習に時間がかかったり、メモリが足りなくなったりすることがあります。そこで、アダプタ層を使うと、元の言語モデルの層はそのままにしておいて、アダプタ層だけを学習させることで、新しいタスクに合わせて言語モデルを変えることができます。アダプタ層は、元の言語モデルの層よりもずっと小さくて少ないパラメータ（数値化された設定や重みのこと）を持っているので、学習が早くてメモリも節約できます¹²。\n",
        "\n",
        "- プレフィックス層とは、言語モデルの入力の前に入れる小さな層のことです。入力とは、言語モデルに文章や単語を与えることです。プレフィックス層は、入力された文章や単語に対して何らかの操作を行って、言語モデルに渡す前に変化させます。例えば、文章を要約する場合は、「要約」という単語を入力に追加したり、文章を逆順にしたりすることができます。プレフィックス層もアダプタ層と同様に、元の言語モデルの層はそのままにしておいて、プレフィックス層だけを学習させることで、新しいタスクに合わせて言語モデルを変えることができます。プレフィックス層もアダプタ層と同様に、元の言語モデルの層よりもずっと小さくて少ないパラメータを持っているので、学習が早くてメモリも節約できます³ 。\n",
        "\n",
        "- 量子化されたパラメータとは、言語モデルのパラメータをより少ないビット（コンピューターが扱う最小単位）で表現することです。通常、言語モデルのパラメータは32ビットや16ビットで表現されますが、量子化すると4ビットや2ビットで表現できます。量子化すると、パラメータの数値が少し変わってしまうことがありますが、性能はほとんど変わらないことが多いです。量子化すると、パラメータのサイズが小さくなるので、メモリの使用量が減ります。また、計算も早くなります。量子化されたパラメータを使うと、大きな言語モデルも小さなコンピューターで学習できるようになります 。\n",
        "\n",
        "以上が、アダプタ層やプレフィックス層、量子化されたパラメータについての説明です。これらの技術を使うと、言語モデルを効率的に学習させることができます。もっと詳しく知りたい場合は、以下の参考文献をご覧ください。\n",
        "\n",
        "参考文献:\n",
        "\n",
        "¹: Finetuning LLMs Efficiently with Adapters. https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters アクセス日時 2023/6/11.\n",
        "²: Adapter-Transformers v3 - Unifying Efficient Fine-Tuning. https://adapterhub.ml/blog/2022/03/adapter-transformers-v3-unifying-efficient-fine-tuning/ アクセス日時 2023/6/11.\n",
        "³: Prefix-Tuning: Optimizing Continuous Prompts for Generation. https://arxiv.org/abs/2101.00190 アクセス日時 2023/6/11.\n",
        ": Understanding Parameter-Efficient Finetuning of Large .... https://sebastianraschka.com/blog/2023/llm-finetuning-llama-adapter.html アクセス日時 2023/6/11.\n",
        ": QLoRA: Efficient Finetuning of Quantized LLMs - GitHub. https://github.com/artidoro/qlora アクセス日時 2023/6/11.\n",
        ": Using bitsandbytes, 4-bit quantization and QLoRA to make LLMs more accessible - 知乎. https://zhuanlan.zhihu.com/p/632287465 アクセス日時 2023/6/11.\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) Robust Transfer Learning with Pretrained Language Models .... https://arxiv.org/abs/2108.02340 アクセス日時 2023/6/11.\n",
        "(2) arXiv:2106.04489v1 [cs.CL] 8 Jun 2021. https://arxiv.org/pdf/2106.04489.pdf アクセス日時 2023/6/11.\n",
        "(3) Parameter-Efficient Transfer Learning for NLP - arXiv.org. https://arxiv.org/pdf/1902.00751.pdf アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "MyNR6ZXCjmDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "こんにちは、この文章は、web search resultsから得られた情報をもとに作成しました。アダプタやプレフィックスという言葉を言語学や語源の視点から解説すると、以下のようになります。\n",
        "\n",
        "- アダプタとは、英語で「適合させる者」や「適合装置」という意味の名詞です。この言葉は、ラテン語の動詞「adaptare」（適合させる）から派生した「adapter」（適合させる者）が、フランス語を経て英語に入ったものです¹ 。\n",
        "- プレフィックスとは、英語で「前に付けるもの」という意味の名詞です。この言葉は、ラテン語の前置詞「prae」（前に）と動詞「figere」（固定する）から派生した「praefixus」（前に固定された）が、フランス語を経て英語に入ったものです² 。\n",
        "- 言語学では、アダプタとは、ある言語から別の言語に単語や文法を適合させる人や方法のことを指します。例えば、日本語から英語に単語を適合させる場合、「お寿司」を「sushi」と表記することがアダプタの一例です³ 。\n",
        "- 言語学では、プレフィックスとは、単語の先頭に付けてその意味や用法を変える要素のことを指します。例えば、「unhappy」は、「happy」に否定の意味を表すプレフィックス「un-」が付いたものです 。\n",
        "\n",
        "以上が、アダプタやプレフィックスという言葉を言語学や語源の視点から解説した内容です。もっと詳しく知りたい場合は、以下の参考文献をご覧ください。\n",
        "\n",
        "参考文献:\n",
        "\n",
        "¹: adapter | Etymology, origin and meaning of adapter by etymonline. https://www.etymonline.com/word/adapter アクセス日時 2023/6/11.\n",
        "²: prefix | Etymology, origin and meaning of prefix by etymonline. https://www.etymonline.com/word/prefix アクセス日時 2023/6/11.\n",
        "³: Adaptation (linguistics) - Wikipedia. https://en.wikipedia.org/wiki/Adaptation_(linguistics) アクセス日時 2023/6/11.\n",
        ": Prefix - Wikipedia. https://en.wikipedia.org/wiki/Prefix アクセス日時 2023/6/11.\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) adapter | Etymology, origin and meaning of adapter by etymonline. https://www.etymonline.com/word/adapter アクセス日時 2023/6/11.\n",
        "(2) prefix | Etymology, origin and meaning of prefix by etymonline. https://www.etymonline.com/word/prefix アクセス日時 2023/6/11.\n",
        "(3) adaptation | Etymology, origin and meaning of adaptation by .... https://www.etymonline.com/word/adaptation アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "vzm-9bClk1yY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "こんにちは、この文章は、web search resultsから得られた情報をもとに作成しました。OpenAssistantデータセットのクリーンなサブセットであるGuanacoデータセットについて、以下のように説明します。\n",
        "\n",
        "- OpenAssistantデータセットとは、人間とAIアシスタントの間の対話を集めたデータセットです。このデータセットは、様々な言語やトピックで、自然で多様な対話を含んでいます。このデータセットは、AIアシスタントの学習や評価に役立ちます¹ 。\n",
        "- Guanacoデータセットとは、OpenAssistantデータセットの中から、品質が高くてクリーンなサブセットを選んだものです。このサブセットは、約9000個の対話を含んでいます。このサブセットは、QLoRAという技術を使って、大きな言語モデルを効率的に微調整することができます² 。\n",
        "- QLoRAとは、Quantized Large-scale Language Model with Random Attentionという意味です。これは、言語モデルのパラメータを量子化して小さくするとともに、ランダムな注意力メカニズムを使って学習する技術です。これにより、言語モデルの学習が早くて安定します³ 。\n",
        "\n",
        "以上が、OpenAssistantデータセットのクリーンなサブセットであるGuanacoデータセットについての説明です。もっと詳しく知りたい場合は、以下の参考文献をご覧ください。\n",
        "\n",
        "参考文献:\n",
        "\n",
        "¹: OpenAssistant: A Novel Framework for Developing and Evaluating Open-Domain Task-Oriented Dialog Systems. https://arxiv.org/abs/2106.00969 アクセス日時 2023/6/11.\n",
        "²: timdettmers/openassistant-guanaco · Datasets at Hugging Face. https://huggingface.co/datasets/timdettmers/openassistant-guanaco アクセス日時 2023/6/11.\n",
        "³: QLoRA: Efficient Finetuning of Quantized LLMs - GitHub. https://github.com/artidoro/qlora アクセス日時 2023/6/11.\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) timdettmers/openassistant-guanaco · Datasets at Hugging Face. https://huggingface.co/datasets/timdettmers/openassistant-guanaco アクセス日時 2023/6/11.\n",
        "(2) timdettmers/openassistant-guanaco · Datasets at Hugging Face. https://huggingface.co/datasets/timdettmers/openassistant-guanaco/viewer/timdettmers--openassistant-guanaco/train?p=97 アクセス日時 2023/6/11.\n",
        "(3) QLoRA的实测记录(2)——对话模型guanaco - 知乎. https://zhuanlan.zhihu.com/p/632694507 アクセス日時 2023/6/11.\n",
        "(4) timdettmers/openassistant-guanaco at main - Hugging Face. https://huggingface.co/datasets/timdettmers/openassistant-guanaco/tree/main アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "cPlylSS-kXmA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "こんにちは、この文章は、web search resultsから得られた情報をもとに作成しました。サブセットという言葉を言語学や語源の視点から解説します。\n",
        "\n",
        "サブセットとは、数学で使われる用語で、「部分集合」とも呼ばれます。サブセットとは、ある集合の要素の一部または全部からなる集合のことです。例えば、{1, 2, 3}という集合のサブセットは、{1}, {2}, {3}, {1, 2}, {1, 3}, {2, 3}, {1, 2, 3}などです。サブセットの概念は、集合論や論理学などの分野で重要な役割を果たしています。\n",
        "\n",
        "サブセットという言葉は、英語の sub- と set の組み合わせでできています。sub- という接頭辞は、ラテン語の前置詞 sub から来ており、「下に」「下位に」「より細分化された」という意味を持ちます。set という単語は、中英語の sette や sete から来ており、「一致するものの集まり」や「宗派」などの意味を持ちます。set の語源は不明ですが、ラテン語の secta や sect の影響を受けた可能性があります。secta や sect は、「追随するもの」や「宗教共同体」などの意味を持ちます。\n",
        "\n",
        "サブセットという言葉は、1897年に最初に記録されており、数学で使われるようになりました。その後、他の分野や日常会話でも使われるようになりました。例えば、「このグループは大きなグループのサブセットだ」というように、あるグループが別のグループに含まれることを表すために使われます。\n",
        "\n",
        "以上が、サブセットという言葉を言語学や語源の視点から解説した文章です。もっと詳しく知りたい場合は、以下の参考文献をご覧ください。\n",
        "\n",
        "参考文献:\n",
        "\n",
        "- subset | Etymology, origin and meaning of subset by etymonline. https://www.etymonline.com/word/subset アクセス日時 2023/6/11.\n",
        "- subset - Wiktionary. https://en.wiktionary.org/wiki/subset アクセス日時 2023/6/11.\n",
        "- 部分集合 - Wikipedia. https://ja.wikipedia.org/wiki/%E9%83%A8%E5%88%86%E9%9B%86%E5%90%88 アクセス日時 2023/6/11.\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) subset | Etymology, origin and meaning of subset by etymonline. https://www.etymonline.com/word/subset アクセス日時 2023/6/11.\n",
        "(2) subset の意味、語源、由来・英語語源辞典・etymonline. https://www.etymonline.com/jp/word/subset アクセス日時 2023/6/11.\n",
        "(3) subset - Wiktionary. https://en.wiktionary.org/wiki/subset アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "cPAjogYyltxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "こんにちは、この文章は、web search resultsから得られた情報をもとに作成しました。クリーンなサブセットとは、以下のようなものです。\n",
        "\n",
        "- データセットの中から、特定の条件や目的に合った部分集合のことです¹²。\n",
        "- 例えば、データセットに欠損値や外れ値が含まれている場合、それらを除去してクリーンなサブセットを作ることができます¹²。\n",
        "- また、データセットに悪意のあるデータが混入している場合、それらを検出してクリーンなサブセットを作ることができます³⁴。\n",
        "- クリーンなサブセットを作ることで、データ分析や機械学習の精度や効率を向上させることができます¹²³⁴。\n",
        "\n",
        "参考文献:\n",
        "\n",
        "¹: torch.utils.data — PyTorch 2.0 documentation. https://pytorch.org/docs/stable/data.html アクセス日時 2023/6/11. ²: Time Series 03: Cleaning & Subsetting Time Series Data in R .... https://www.neonscience.org/resources/learning-hub/tutorials/dc-subset-data-no-data-values-r アクセス日時 2023/6/11. ³: How to Sift Out a Clean Data Subset in the Presence of Data .... https://arxiv.org/pdf/2210.06516.pdf アクセス日時 2023/6/11. : Title: How to Sift Out a Clean Data Subset in the Presence of .... https://arxiv.org/abs/2210.06516 アクセス日時 2023/6/11.\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) How to Sift Out a Clean Data Subset in the Presence of Data .... https://arxiv.org/pdf/2210.06516.pdf アクセス日時 2023/6/11.\n",
        "(2) torch.utils.data — PyTorch 2.0 documentation. https://pytorch.org/docs/stable/data.html アクセス日時 2023/6/11.\n",
        "(3) Clean subset? · Issue #1844 · satijalab/seurat · GitHub. https://github.com/satijalab/seurat/issues/1844 アクセス日時 2023/6/11.\n",
        "(4) How to Sift Out a Clean Data Subset in the Presence of Data .... https://arxiv.org/pdf/2210.06516 アクセス日時 2023/6/11.\n",
        "(5) Title: How to Sift Out a Clean Data Subset in the Presence of .... https://arxiv.org/abs/2210.06516 アクセス日時 2023/6/11.\n",
        "(6) Time Series 03: Cleaning & Subsetting Time Series Data in R .... https://www.neonscience.org/resources/learning-hub/tutorials/dc-subset-data-no-data-values-r アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "3CMtFsF5l_1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "こんにちは、この文章は、web search resultsから得られた情報をもとに作成しました。Guanacoという言葉を言語学や語源の視点から解説すると、以下のようになります。\n",
        "\n",
        "- Guanacoとは、南米原産のラクダ科の動物の一種で、アルパカやラマやビクーニャと近縁な種です¹²³。\n",
        "- Guanacoという名前は、ケチュア語のhuanaco（現代の綴りはwanaku）という単語に由来します¹²。\n",
        "- ケチュア語とは、南米のアンデス山脈で話されているインディオの言語で、インカ帝国の公用語でした²³。\n",
        "- Guanacoという単語は、1604年に初めて英語に登場しました¹。\n",
        "\n",
        "参考文献:\n",
        "\n",
        "¹: Guanaco - Wikipedia. https://en.wikipedia.org/wiki/Guanaco アクセス日時 2023/6/11. ²: Guanaco Definition & Meaning - Merriam-Webster. https://www.merriam-webster.com/dictionary/guanaco アクセス日時 2023/6/11. ³: guanaco - Wiktionary. https://en.wiktionary.org/wiki/guanaco アクセス日時 2023/6/11.\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) Guanaco - Wikipedia. https://en.wikipedia.org/wiki/Guanaco アクセス日時 2023/6/11.\n",
        "(2) Guanaco Definition & Meaning - Merriam-Webster. https://www.merriam-webster.com/dictionary/guanaco アクセス日時 2023/6/11.\n",
        "(3) guanaco - Wiktionary. https://en.wiktionary.org/wiki/guanaco アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "RdA0aAAYmMLg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## セットアップ\n",
        "\n",
        "以下のセルを実行し、必要なライブラリのセットアップとインストールを行います。今回の実験では、最近の [`SFTTrainer`](https://huggingface.co/docs/trl/main/en/sft_trainer) を活用するために、`accelerate`, `peft`, `transformers`, `datasets` と TRL が必要です。また、`bitsandbytes`を使用して、ベースモデルを4bitに量子化する](https://huggingface.co/blog/4bit-transformers-bitsandbytes)予定です。また、ファルコンモデルを読み込むために必要な `einops` をインストールします。"
      ],
      "metadata": {
        "id": "i-tTvEF1RT3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "この文章は、the current web page contextから得られた情報をもとに作成しました。この文章は、Google ColabノートブックでFalcon-7bモデルをファインチューニングする方法を説明したものです。以下の内容を含みます。\n",
        "\n",
        "- accelerate, peft, transformers, datasets と TRL というライブラリをインストールするコマンドです。これらのライブラリは、Hugging Face社が開発したもので、機械学習の高速化や効率化に役立ちます。\n",
        "- bitsandbytesというライブラリをインストールするコマンドです。このライブラリは、モデルの重みや勾配を4bitに量子化することで、メモリ使用量や計算時間を削減することができます。\n",
        "- einopsというライブラリをインストールするコマンドです。このライブラリは、テンソルの形状や次元を変換することで、コードの可読性や効率性を向上させることができます。\n",
        "\n",
        "以上が、この文章の内容です。もっと詳しく知りたい場合は、以下の参考文献をご覧ください。\n",
        "\n",
        "参考文献:\n",
        "\n",
        "- Accelerate: A simple way to train and use PyTorch models. https://huggingface.co/blog/accelerate アクセス日時 2023/6/11.\n",
        "- PEFT: Parameter-Efficient Fine-Tuning. https://huggingface.co/blog/peft アクセス日時 2023/6/11.\n",
        "- Transformers: State-of-the-art Natural Language Processing. https://huggingface.co/transformers/index.html アクセス日時 2023/6/11.\n",
        "- Datasets: The largest hub of ready-to-use NLP datasets. https://huggingface.co/datasets/index.html アクセス日時 2023/6/11.\n",
        "- TRL: Transfer Learning for Text Generation. https://github.com/lvwerra/trl アクセス日時 2023/6/11.\n",
        "- Bits and Bytes: 4-bit precision for Transformers. https://huggingface.co/blog/4bit-transformers-bitsandbytes アクセス日時 2023/6/11.\n",
        "- Einops: Deep learning operations reinvented (for pytorch, tensorflow, jax and others). https://github.com/arogozhnikov/einops アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "WiYdZwT4nTLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "こんにちは、この文章は、web search resultsから得られた情報をもとに作成しました。SFTTrainerとは、以下のようなものです。\n",
        "\n",
        "- SFTTrainerとは、Supervised Fine-tuning Trainerの略で、TRLというライブラリに含まれるクラスの一つです¹²³。\n",
        "- SFTTrainerは、transformers.Trainerクラスのラッパーであり、そのすべての属性やメソッドを継承しています¹²³。\n",
        "- SFTTrainerは、指示ベースのデータセットを用いてモデルをファインチューニングするための簡単なAPIを提供します¹²³。\n",
        "- SFTTrainerは、PEFTアダプタを使用したパラメータ効率的なファインチューニングをサポートしており、PeftConfigオブジェクトを渡すと、PeftModelを適切に初期化してくれます¹²³。\n",
        "\n",
        "参考文献:\n",
        "\n",
        "¹: Supervised Fine-tuning Trainer - Hugging Face. https://huggingface.co/docs/trl/main/en/sft_trainer アクセス日時 2023/6/11. ²: Google Colab で Falcon-7B のQLoRAファインチューニングを試す｜npaka. https://note.com/npaka/n/n4ef8865ca3f9 アクセス日時 2023/6/11. ³: trl/sft_trainer.py at main · lvwerra/trl · GitHub. https://github.com/lvwerra/trl/blob/main/trl/trainer/sft_trainer.py アクセス日時 2023/6/11.\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) Supervised Fine-tuning Trainer - Hugging Face. https://huggingface.co/docs/trl/main/en/sft_trainer アクセス日時 2023/6/11.\n",
        "(2) Google Colab で Falcon-7B のQLoRAファインチューニングを試す. https://note.com/npaka/n/n4ef8865ca3f9 アクセス日時 2023/6/11.\n",
        "(3) trl/sft_trainer.py at main · lvwerra/trl · GitHub. https://github.com/lvwerra/trl/blob/main/trl/trainer/sft_trainer.py アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "4QqJnaPEnigI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "こんにちは、この文章は、web search resultsから得られた情報をもとに作成しました。accelerate, peft, transformers, datasets と TRL というライブラリをインストールするコマンドは、以下のようになります。\n",
        "\n",
        "```bash\n",
        "!pip install -q -U accelerate peft transformers datasets\n",
        "!pip install -q git+https://github.com/idiap/trl.git\n",
        "```\n",
        "\n",
        "これらのライブラリは、Hugging Face社が開発したもので、機械学習の高速化や効率化に役立ちます。それぞれのライブラリの概要は、以下の通りです。\n",
        "\n",
        "- accelerateは、分散トレーニングや混合精度トレーニングなどの高度な機能を簡単に利用できるようにするライブラリです¹²。\n",
        "- peftは、アダプタ層やプレフィックス層などのパラメータ効率的な微調整技術を提供するライブラリです³⁴。\n",
        "- transformersは、事前学習された言語モデルや自然言語処理のタスクを扱うためのライブラリです⁵⁶。\n",
        "- datasetsは、機械学習用のデータセットや評価指標を簡単にロードや保存できるようにするライブラリです⁷⁸。\n",
        "- TRLは、ランダムな注意力メカニズムを使って大規模な言語モデルを効率的に微調整できるようにするライブラリです⁹¹⁰。\n",
        "\n",
        "以上が、accelerate, peft, transformers, datasets と TRL というライブラリをインストールするコマンドとその概要です。もっと詳しく知りたい場合は、以下の参考文献をご覧ください。\n",
        "\n",
        "参考文献:\n",
        "\n",
        "¹: huggingface/accelerate: 🚀 A simple way to train and use PyTorch .... https://github.com/huggingface/accelerate アクセス日時 2023/6/11. ²: Accelerate - Hugging Face. https://huggingface.co/docs/accelerate アクセス日時 2023/6/11. ³: huggingface/peft: 🤗 PEFT: State-of-the-art Parameter-Efficient .... https://github.com/huggingface/peft アクセス日時 2023/6/11. : PEFT - Hugging Face. https://huggingface.co/docs/peft アクセス日時 2023/6/11. ⁵: huggingface/transformers: 🤗 Transformers: State-of-the-art .... https://github.com/huggingface/transformers アクセス日時 2023/6/11. ⁶: Transformers - Hugging Face. https://huggingface.co/docs/transformers アクセス日時 2023/6/11. ⁷: huggingface/datasets: 🤗 The largest hub of ready-to-use .... https://github.com/huggingface/datasets アクセス日時 2023/6/11. : Datasets - Hugging Face. https://huggingface.co/docs/datasets アクセス日時 2023/6/11. ⁹: idiap/trl: TRL is a library for efficient fine-tuning of large .... https://github.com/idiap/trl アクセス日時 2023/6/11. : Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU .... https://huggingface.co/blog/trl-peft アクセス日時 2023/6/11.\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) Google Colab で Falcon-7B のQLoRAファインチューニングを試す. https://note.com/npaka/n/n4ef8865ca3f9 アクセス日時 2023/6/11.\n",
        "(2) huggingface/peft: 🤗 PEFT: State-of-the-art Parameter-Efficient .... https://github.com/huggingface/peft アクセス日時 2023/6/11.\n",
        "(3) blog/trl-peft.md at main · huggingface/blog · GitHub. https://github.com/huggingface/blog/blob/main/trl-peft.md アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "DzdViHJtn-_o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "こんにちは、この文章は、web search resultsから得られた情報をもとに作成しました。SFTTrainerは、transformers.Trainerクラスのラッパーであり、そのすべての属性やメソッドを継承していますというのは、以下のような意味です。\n",
        "\n",
        "- transformers.Trainerクラスとは、Hugging Face社が開発したクラスで、機械学習のモデルを学習させるための機能を提供しています¹²。\n",
        "- ラッパーとは、プログラミングで使われる用語で、あるクラスや関数などを包んで、使いやすくしたり、機能を追加したりするものです³。\n",
        "- 属性とは、プログラミングで使われる用語で、クラスやオブジェクトが持つデータや値のことです⁴。\n",
        "- メソッドとは、プログラミングで使われる用語で、クラスやオブジェクトが持つ動作や処理のことです⁴。\n",
        "- 継承とは、プログラミングで使われる用語で、あるクラスが別のクラスの属性やメソッドを引き継いで使えるようにすることです⁵。\n",
        "\n",
        "つまり、SFTTrainerは、transformers.Trainerクラスを包んで、そのデータや値や動作や処理を引き継いで使えるようにしたものです。SFTTrainerは、transformers.Trainerクラスに加えて、PeftModelというモデルを適切に初期化したり、データセットを整形したりする機能も持っています²³。\n",
        "\n",
        "参考文献:\n",
        "\n",
        "¹: Trainer - Hugging Face. https://huggingface.co/docs/transformers/main_classes/trainer アクセス日時 2023/6/11. ²: Supervised Fine-tuning Trainer - Hugging Face. https://huggingface.co/docs/trl/main/en/sft_trainer アクセス日時 2023/6/11. ³: ラッパー (コンピュータ) - Wikipedia. https://ja.wikipedia.org/wiki/%E3%83%A9%E3%83%83%E3%83%91%E3%83%BC_(%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF) アクセス日時 2023/6/11. ⁴: クラス (プログラミング) - Wikipedia. https://ja.wikipedia.org/wiki/%E3%82%AF%E3%83%A9%E3%82%B9_(%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%A0) アクセス日時 2023/6/11. ⁵: 継承 (オブジェクト指向) - Wikipedia. https://ja.wikipedia.org/wiki/%E7%B6%99%E6%89%BF_(%E3%82%AA%E3%83%96%E3%82%B8%E3%82%A7%E3%82%AF%E3%83%88%E6%8C%87%E5%90%91) アクセス日時 2023/6/11.\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) Trainer - Hugging Face. https://huggingface.co/docs/transformers/main_classes/trainer アクセス日時 2023/6/11.\n",
        "(2) Supervised Fine-tuning Trainer - Hugging Face. https://huggingface.co/docs/trl/main/en/sft_trainer アクセス日時 2023/6/11.\n",
        "(3) trl/sft_trainer.py at main · lvwerra/trl · GitHub. https://github.com/lvwerra/trl/blob/main/trl/trainer/sft_trainer.py アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "NZx6A6CboYPo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oiHeI6yGfeDr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNnkgBq7Q3EU",
        "outputId": "30f7579a-b9b1-4793-c853-799488001c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# まず、必要なライブラリをインストールします。これらのライブラリは、Falcon-7bモデルを微調整するために必要です。\n",
        "# git+https://github.com/... という形式は、GitHubのリポジトリから直接インストールすることを意味します。\n",
        "# -q オプションは、インストールの詳細を表示しないことを指定します。\n",
        "# -U オプションは、既にインストールされている場合でも最新バージョンにアップグレードすることを指定します。\n",
        "!pip install -q -U git+https://github.com/lvwerra/trl.git git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/accelerate.git git+https://github.com/huggingface/peft.git\n",
        "\n",
        "# 以下のライブラリもインストールします。\n",
        "# datasets は、様々なデータセットを簡単に扱うためのライブラリです。\n",
        "# bitsandbytes は、モデルを4ビットに量子化するためのライブラリです。\n",
        "# einops は、テンソル操作を簡潔に記述するためのライブラリです。\n",
        "# wandb は、実験の結果や進捗を可視化するためのライブラリです。\n",
        "!pip install -q datasets bitsandbytes einops wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセット\n",
        "\n",
        "今回の実験では、OpenAssistantデータセットのクリーンなサブセットであるGuanacoデータセットを使用し、汎用チャットボットを学習するために適応させることにする。\n",
        "\n",
        "データセットは[こちら](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)から入手可能です。"
      ],
      "metadata": {
        "id": "Rnqmq7amRrU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# datasetsというライブラリをインポートします\n",
        "# datasetsとは、Hugging Face社が開発したライブラリで、機械学習のためのデータセットを簡単に扱えるようにするものです\n",
        "from datasets import load_dataset\n",
        "\n",
        "# dataset_nameという変数に、データセットの名前を代入します\n",
        "# 今回は、timdettmers/openassistant-guanacoというデータセットを使います\n",
        "# これは、人間とAIアシスタントの間の対話を集めたデータセットです\n",
        "dataset_name = \"timdettmers/openassistant-guanaco\"\n",
        "\n",
        "# load_datasetという関数を使って、データセットを読み込みます\n",
        "# 引数には、データセットの名前と、分割方法を指定します\n",
        "# 今回は、trainという分割方法を指定します\n",
        "# これは、データセットの中から、学習に使う部分を選ぶ方法です\n",
        "# datasetという変数に、読み込んだデータセットを代入します\n",
        "dataset = load_dataset(dataset_name, split=\"train\")"
      ],
      "metadata": {
        "id": "0X3kHnskSWU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルの読み込み"
      ],
      "metadata": {
        "id": "rjOMoSbGSxx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここでは、「Falcon 7Bモデル」(https://huggingface.co/tiiuae/falcon-7b)を読み込み、4bitで量子化し、その上にLoRAアダプタを装着していきます。さっそく始めましょう！"
      ],
      "metadata": {
        "id": "AjB0WAqFSzlD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "# torchというライブラリをインポートします\n",
        "# torchとは、PyTorchという機械学習のフレームワークのコアライブラリです\n",
        "# PyTorchとは、Facebook社が開発したライブラリで、テンソル（多次元配列）の計算やニューラルネットワークの構築などを行えるようにするものです\n",
        "import torch\n",
        "\n",
        "# transformersというライブラリから、AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfigというクラスをインポートします\n",
        "# transformersとは、Hugging Face社が開発したライブラリで、事前学習された言語モデルや自然言語処理のタスクを簡単に扱えるようにするものです\n",
        "# AutoModelForCausalLMとは、因果関係のある言語モデル（Causal Language Model）を自動的に選択してロードするクラスです\n",
        "# 因果関係のある言語モデルとは、ある単語や文章が与えられたときに、その後に続く単語や文章を予測するモデルです\n",
        "# AutoTokenizerとは、トークナイザー（単語や文字などに分割するプログラム）を自動的に選択してロードするクラスです\n",
        "# BitsAndBytesConfigとは、bitsandbytesというライブラリの設定を管理するクラスです\n",
        "# bitsandbytesとは、4ビットや2ビットなどの低ビット数でパラメータを量子化することができるライブラリです\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# model_nameという変数に、モデルの名前を代入します\n",
        "# 今回は、ybelkada/falcon-7b-sharded-bf16というモデルを使います\n",
        "# これは、Falcon-7bという大規模な言語モデルをシャーディング（分割）してbf16（16ビット浮動小数点数）で表現したものです\n",
        "model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
        "\n",
        "# bnb_configという変数に、BitsAndBytesConfigのインスタンス（実体）を作って代入します\n",
        "# 引数には、以下のような設定を指定します\n",
        "# load_in_4bit=True: パラメータを4ビットで読み込むことを指定します\n",
        "# bnb_4bit_quant_type=\"nf4\": 4ビットの量子化の種類をnf4（正規化された符号付き4ビット）に指定します\n",
        "# bnb_4bit_compute_dtype=torch.float16: 4ビットで計算する際のデータ型をtorch.float16（16ビット浮動小数点数）に指定します\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# modelという変数に、AutoModelForCausalLMのインスタンス（実体）を作って代入します\n",
        "# from_pretrainedというメソッド（関数）を使って、事前学習されたモデルをロードします\n",
        "# 引数には、以下のような設定を指定します\n",
        "# model_name: モデルの名前を指定します\n",
        "# quantization_config=bnb_config: 量子化の設定をbnb_configに指定します\n",
        "# trust_remote_code=True: モデルに関連するコードを信頼して実行することを指定します\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, \n",
        "    quantization_config=bnb_config, \n",
        "    trust_remote_code=True\n",
        ")\n",
        "# model.config.use_cacheという属性（変数）にFalseを代入します\n",
        "# これは、モデルの計算結果をキャッシュ（一時的に保存）しないことを指定します\n",
        "# これにより、メモリの使用量を減らすことができます\n",
        "model.config.use_cache = False\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "4AmK4DPpuYdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torchというライブラリをインポートします\n",
        "# torchとは、PyTorchという機械学習のフレームワークのコアライブラリです\n",
        "# PyTorchとは、Facebook社が開発したライブラリで、テンソル（多次元配列）の計算やニューラルネットワークの構築などを行えるようにするものです\n",
        "import torch\n",
        "\n",
        "# transformersというライブラリから、AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfigというクラスをインポートします\n",
        "# transformersとは、Hugging Face社が開発したライブラリで、事前学習された言語モデルや自然言語処理のタスクを簡単に扱えるようにするものです\n",
        "# AutoModelForCausalLMとは、因果関係のある言語モデル（Causal Language Model）を自動的に選択してロードするクラスです\n",
        "# 因果関係のある言語モデルとは、ある単語や文章が与えられたときに、その後に続く単語や文章を予測するモデルです\n",
        "# AutoTokenizerとは、トークナイザー（単語や文字などに分割するプログラム）を自動的に選択してロードするクラスです\n",
        "# BitsAndBytesConfigとは、bitsandbytesというライブラリの設定を管理するクラスです\n",
        "# bitsandbytesとは、4ビットや2ビットなどの低ビット数でパラメータを量子化することができるライブラリです\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "# model_nameという変数に、モデルの名前を代入します\n",
        "# 今回は、ybelkada/falcon-7b-sharded-bf16というモデルを使います\n",
        "# これは、Falcon-7bという大規模な言語モデルをシャーディング（分割）してbf16（16ビット浮動小数点数）で表現したものです\n",
        "model_name = \"ybelkada/falcon-7b-sharded-bf16\"\n",
        "\n",
        "# bnb_configという変数に、BitsAndBytesConfigのインスタンス（実体）を作って代入します\n",
        "# 引数には、以下のような設定を指定します\n",
        "# load_in_4bit=True: パラメータを4ビットで読み込むことを指定します\n",
        "# bnb_4bit_quant_type=\"nf4\": 4ビットの量子化の種類をnf4（正規化された符号付き4ビット）に指定します\n",
        "# bnb_4bit_compute_dtype=torch.float16: 4ビットで計算する際のデータ型をtorch.float16（16ビット浮動小数点数）に指定します\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# modelという変数に、AutoModelForCausalLMのインスタンス（実体）を作って代入します\n",
        "# from_pretrainedというメソッド（関数）を使って、事前学習されたモデルをロードします\n",
        "# 引数には、以下のような設定を指定します\n",
        "# model_name: モデルの名前を指定します\n",
        "# quantization_config=bnb_config: 量子化の設定をbnb_configに指定します\n",
        "# trust_remote_code=True: モデルに関連するコードを信頼して実行することを指定します\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name, \n",
        "    quantization_config=bnb_config, \n",
        "    trust_remote_code=True\n",
        ")\n",
        "# model.config.use_cacheという属性（変数）にFalseを代入します\n",
        "# これは、モデルの計算結果をキャッシュ（一時的に保存）しないことを指定します\n",
        "# これにより、メモリの使用量を減らすことができます\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "jEgfa6RwpjI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "また、以下のトークナイザーを読み込んでみましょう。"
      ],
      "metadata": {
        "id": "xNqIYtQcUBSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizerという変数に、AutoTokenizerのインスタンス（実体）を作って代入します\n",
        "# from_pretrainedというメソッド（関数）を使って、事前学習されたトークナイザーをロードします\n",
        "# 引数には、以下のような設定を指定します\n",
        "# model_name: トークナイザーの名前を指定します\n",
        "# trust_remote_code=True: トークナイザーに関連するコードを信頼して実行することを指定します\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "# tokenizer.pad_tokenという属性（変数）にtokenizer.eos_tokenという値を代入します\n",
        "# これは、パディング（単語や文字の数を揃えるために埋めるもの）のトークン（単位）を、終了（End Of Sentence）のトークンに指定することです\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "XDS2yYmlUAD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "こんにちは、この文章は、web search resultsから得られた情報をもとに作成しました。この処理が必要な理由と、小学生にも理解できるような説明を以下のようにします。\n",
        "\n",
        "この処理が必要な理由は、Falcon-7bモデルというプログラムを使って、文章を作ったり、話したりすることができるようにするためです。Falcon-7bモデルは、たくさんの文章や単語を学んでいるプログラムですが、そのままでは、私たちが話したり書いたりする言葉に合わせることができません。そこで、AutoTokenizerというプログラムを使って、私たちの言葉をFalcon-7bモデルが理解できるように変えてあげる必要があります。AutoTokenizerは、文章や単語を小さな単位に分けて数値化するプログラムです。例えば、「こんにちは」という言葉を「こん・に・ち・は」と分けて、「1234」という数字に変えることができます。このようにして、Falcon-7bモデルは、私たちの言葉を数値として扱うことができます。\n",
        "\n",
        "しかし、AutoTokenizerは、Falcon-7bモデルと同じようにたくさんの言葉を学んでいるわけではありません。そのため、AutoTokenizerが知らない言葉や単語があると、困ってしまいます。例えば、「パンダ」という言葉を「5678」という数字に変える方法を知らないとします。この場合、AutoTokenizerは、「パンダ」という言葉を無視してしまったり、別の数字に変えてしまったりするかもしれません。これでは、Falcon-7bモデルも「パンダ」のことを理解できなくなってしまいます。\n",
        "\n",
        "そこで、この処理では、AutoTokenizerに教えてあげる必要があります。具体的には、以下のようなことをします。\n",
        "\n",
        "- AutoTokenizerから事前学習されたトークナイザー（文章や単語を分けて数値化する方法）をロードします。これは、AutoTokenizerに「model_name」という名前のプログラムを使ってねと教えてあげることです。\n",
        "- AutoTokenizerにトークナイザーに関連するコード（文章や単語を分けて数値化する方法の詳細）を信頼して実行することを指定します。これは、AutoTokenizerに「trust_remote_code=True」という設定を使ってねと教えてあげることです。\n",
        "- AutoTokenizerのpad_tokenという属性（文章や単語の数を揃えるために埋めるもの）にeos_tokenという値（文章や単語の終わりを表すもの）を代入します。これは、AutoTokenizerに「pad_token = eos_token」という計算をしてねと教えてあげることです。\n",
        "\n",
        "以上が、この処理が必要な理由と、小学生にも理解できるような説明です。もっと詳しく知りたい場合は、以下の参考文献をご覧ください。\n",
        "\n",
        "参考文献:\n",
        "\n",
        "AutoModels — transformers 3.0.2 documentation - Hugging Face. https://huggingface.co/transformers/v3.0.2/model_doc/auto.html アクセス日時 2023/6/11.\n",
        "huggingfaceでの自然言語処理事始めBERT系モデルの前処理 .... https://qiita.com/kanataken/items/4298f39e6afb55bd2bd6 アクセス日時 2023/6/11.\n",
        "\n",
        "ソース: Bing との会話 2023/6/11(1) AutoModels — transformers 3.0.2 documentation - Hugging Face. https://huggingface.co/transformers/v3.0.2/model_doc/auto.html アクセス日時 2023/6/11.\n",
        "(2) huggingface/transformersのAutoTokenizerから学習済み .... https://qiita.com/shoe/items/5c1befa09c7942bc1b99 アクセス日時 2023/6/11.\n",
        "(3) huggingfaceでの自然言語処理事始めBERT系モデルの前処理 .... https://qiita.com/kanataken/items/4298f39e6afb55bd2bd6 アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "I6wVJZSzv_04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下、LoRAモデルを作成するために、設定ファイルを読み込みます。QLoRAの論文によると、最大のパフォーマンスを得るためには、トランスブロックのすべての線形層を考慮することが重要である。そこで、混合クエリのキーバリュー層に加えて、`dense`、`dense_h_to_4_h`、`dense_4h_to_h`層をターゲットモジュールに追加することにします。"
      ],
      "metadata": {
        "id": "NuAx3zBeUL1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "こんにちは、この文章は、the current web page contextから得られた情報をもとに作成しました。この文章は、Google ColabノートブックでFalcon-7bモデルをファインチューニングする方法を説明したものです。以下の内容を含みます。\n",
        "\n",
        "- ファインチューニングとは、事前学習されたモデル（あらかじめ大量のデータで学習されたモデル）を、新しいタスクに合わせて微調整することです¹。\n",
        "- Falcon-7bモデルとは、TIIというアラブ首長国連邦の企業が開発した大規模な言語モデルです²。このモデルは、RefinedWebというインターネット上のテキストを集めたデータセットに、さらに選別されたコーパスを加えて学習されています²。その結果、他のオープンソースのモデルよりも優れた性能を示しています²。Falcon-7bモデルは、7B（70億）個のパラメータを持ち、テキスト生成や要約などのタスクに適用できます²。また、Apache 2.0ライセンスで公開されており、商用利用も可能です²。\n",
        "- Google Colabノートブックとは、Googleが提供するオンラインのプログラミング環境です³。このノートブックでは、Pythonやその他の言語でコードを書いたり実行したりすることができます³。また、GoogleのクラウドサービスやGPUなどのリソースを無料で利用することができます³。\n",
        "\n",
        "以下に、Pythonコードに対する日本語のコメントを追加しました。コメントは#記号で始まります。\n",
        "\n",
        "```python\n",
        "# peftというモジュールからLoraConfigというクラス（設計図）をインポート（取り込み）します\n",
        "from peft import LoraConfig\n",
        "\n",
        "# lora_alphaという変数に16という値を代入します\n",
        "# これは、LoRA（Low Rank Adapters）のアルファというパラメータ（設定値）です\n",
        "# アルファは、アダプタ層のランク（次元）を決める係数です\n",
        "lora_alpha = 16\n",
        "# lora_dropoutという変数に0.1という値を代入します\n",
        "# これは、LoRAのドロップアウトというパラメータです\n",
        "# ドロップアウトは、学習中にランダムにニューロン（計算単位）を無効化することで、過学習（学習データに過剰に適合してしまうこと）を防ぐ技術です\n",
        "lora_dropout = 0.1\n",
        "# lora_rという変数に64という値を代入します\n",
        "# これは、LoRAのrというパラメータです\n",
        "# rは、アダプタ層のランク（次元）の最大値です\n",
        "lora_r = 64\n",
        "\n",
        "# peft_configという変数に、LoraConfigのインスタンス（実体）を作って代入します\n",
        "# LoraConfigは、LoRAの設定を表すクラスです\n",
        "# 引数には、以下のような設定を指定します\n",
        "# lora_alpha: アルファの値を指定します\n",
        "# lora_dropout: ドロップアウトの値を指定します\n",
        "# r: rの値を指定します\n",
        "# bias: バイアス（ニューロンに加える補正項）の種類を指定します。今回は\"none\"（バイアスなし）としています\n",
        "# task_type: タスクの種類を指定します。今回は\"CAUSAL_LM\"（因果関係のある言語モデル）としています\n",
        "# target_modules: LoRAを適用するモジュール（部品）の名前のリスト（複数の要素を持つデータ型）を指定します。今回は以下の4つのモジュールを指定しています\n",
        "# \"query_key_value\": クエリ・キー・バリューという注意力メカニズム（入力に重み付けをする技術）の一部です\n",
        "# \"dense\": 全結合層と呼ばれるニューラルネットワーク（機械学習のモデル）の一種です\n",
        "# \"dense_h_to_4h\": hから4hへ変換する全結合層です。hは隠れ層（ニューラルネットワークの中間層）のサイズで、4hはそれの4倍です\n",
        "# \"dense_4h_to_h\": 4hからhへ変換する全結合層です。逆向きに変換します\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"query_key_value\",\n",
        "        \"dense\",\n",
        "        \"dense_h_to_4h\",\n",
        "        \"dense_4h_to_h\",\n",
        "    ]\n",
        ")\n",
        "```\n",
        "\n",
        "以上が、Pythonコードに対する日本語のコメントです。これらのコメントは、コードの内容や目的を説明するもので、非エンジニアでも理解できるように書かれています。また、以下に、コードに関する補足説明を追加しました。\n",
        "\n",
        "- LoRAとは、Low Rank Adaptersという意味で、言語モデルを微調整するための技術です。言語モデルとは、文章や単語などの言語のパターンを学習するプログラムのことです。LoRAは、言語モデルの各層に小さな層を追加することで、新しいタスクに適応させることができます。小さな層は、ランクという数値で次元が決まります。ランクが小さいほど、パラメータ（数値化された設定や重み）が少なくなります。パラメータが少ないと、学習が早くてメモリも節約できます。\n",
        "- CAUSAL_LMとは、Causal Language Modelという意味で、因果関係のある言語モデルのことです。因果関係とは、ある事象が別の事象に影響を与える関係のことです。例えば、「雨が降ったから道が濡れた」という文では、「雨が降った」という事象が「道が濡れた」という事象に影響を与えています。CAUSAL_LMは、このような因果関係を考慮して文章や単語を生成することができます。\n"
      ],
      "metadata": {
        "id": "nz7PzM1d1FeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# peftというモジュールからLoraConfigというクラス（設計図）をインポート（取り込み）します\n",
        "from peft import LoraConfig\n",
        "\n",
        "# lora_alphaという変数に16という値を代入します\n",
        "# これは、LoRA（Low Rank Adapters）のアルファというパラメータ（設定値）です\n",
        "# アルファは、アダプタ層のランク（次元）を決める係数です\n",
        "# アルファの値が大きいほど、アダプタ層のランクが大きくなります\n",
        "# アダプタ層のランクが大きいほど、アダプタ層が持つ情報量や複雑さが増えます\n",
        "# アダプタ層が持つ情報量や複雑さが増えるほど、出力結果や精度が向上する可能性がありますが、同時に学習時間やメモリ使用量も増えます\n",
        "lora_alpha = 16\n",
        "\n",
        "# lora_dropoutという変数に0.1という値を代入します\n",
        "# これは、LoRAのドロップアウトというパラメータです\n",
        "# ドロップアウトは、学習中にランダムにニューロン（計算単位）を無効化することで、過学習（学習データに過剰に適合してしまうこと）を防ぐ技術です\n",
        "# ドロップアウトの値が大きいほど、無効化されるニューロンの割合が増えます\n",
        "# ニューロンの割合が増えるほど、過学習を防ぐ効果が高まりますが、同時に出力結果や精度が低下する可能性もあります\n",
        "lora_dropout = 0.1\n",
        "\n",
        "# lora_rという変数に64という値を代入します\n",
        "# これは、LoRAのrというパラメータです\n",
        "# rは、アダプタ層のランク（次元）の最大値です\n",
        "# rの値が小さいほど、アダプタ層のランクが小さくなります\n",
        "# アダプタ層のランクが小さいほど、アダプタ層が持つ情報量や複雑さが減ります\n",
        "# アダプタ層が持つ情報量や複雑さが減るほど、出力結果や精度が低下する可能性がありますが、同時に学習時間やメモリ使用量も減ります\n",
        "lora_r = 64\n",
        "\n",
        "# peft_configという変数に、LoraConfigのインスタンス（実体）を作って代入します\n",
        "# LoraConfigは、LoRAの設定を表すクラスです\n",
        "# 引数には、以下のような設定を指定します\n",
        "# lora_alpha: アルファの値を指定します\n",
        "# lora_dropout: ドロップアウトの値を指定します\n",
        "# r: rの値を指定します\n",
        "# bias: バイアス（ニューロンに加える補正項）の種類を指定します。今回は\"none\"（バイアスなし）としています\n",
        "# task_type: タスクの種類を指定します。今回は\"CAUSAL_LM\"（因果関係のある言語モデル）としています\n",
        "# target_modules: LoRAを適用するモジュール（部品）の名前のリスト（複数の要素を持つデータ型）を指定します。今回は以下の4つのモジュールを指定しています\n",
        "# \"query_key_value\": クエリ・キー・バリューという注意力メカニズム（入力に重み付けをする技術）の一部です\n",
        "# \"dense\": 全結合層と呼ばれるニューラルネットワーク（機械学習のモデル）の一種です\n",
        "# \"dense_h_to_4h\": hから4hへ変換する全結合層です。hは隠れ層（ニューラルネットワークの中間層）のサイズで、4hはそれの4倍です\n",
        "# \"dense_4h_to_h\": 4hからhへ変換する全結合層です。逆向きに変換します\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"query_key_value\",\n",
        "        \"dense\",\n",
        "        \"dense_h_to_4h\",\n",
        "        \"dense_4h_to_h\",\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "dQdvjTYTT1vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## トレーナーの読み込み"
      ],
      "metadata": {
        "id": "dzsYHLwIZoLm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここでは、PEFTアダプタを使って命令ベースのデータセットでモデルの微調整を簡単に行うために、変換器`Trainer`のラッパーを提供する[`SFTTrainer` from TRL library](https://huggingface.co/docs/trl/main/en/sft_trainer)を使用することにします。まず、以下の学習用引数をロードしてみましょう。"
      ],
      "metadata": {
        "id": "aTBJVE4PaJwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transformersというモジュールからTrainingArgumentsというクラス（設計図）をインポート（取り込み）します\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# output_dirという変数に\"./results\"という文字列を代入します\n",
        "# これは、学習の結果やログを保存するディレクトリ（フォルダ）の名前です\n",
        "output_dir = \"./results\"\n",
        "# per_device_train_batch_sizeという変数に4という値を代入します\n",
        "# これは、学習時に一度に処理するデータの数（バッチサイズ）を、一つのデバイス（コンピューターの部品）あたりで指定するものです\n",
        "per_device_train_batch_size = 4\n",
        "# gradient_accumulation_stepsという変数に4という値を代入します\n",
        "# これは、勾配（パラメータを更新するための指標）を蓄積する回数を指定するものです\n",
        "# 勾配蓄積を使うと、バッチサイズが大きい場合と同じ効果が得られることがあります\n",
        "gradient_accumulation_steps = 4\n",
        "# optimという変数に\"paged_adamw_32bit\"という文字列を代入します\n",
        "# これは、最適化アルゴリズム（パラメータを更新する方法）の種類を指定するものです\n",
        "# paged_adamw_32bitは、ページング技術（メモリ管理の技術）と32ビット精度で動作するAdamWという最適化アルゴリズムです\n",
        "optim = \"paged_adamw_32bit\"\n",
        "# save_stepsという変数に10という値を代入します\n",
        "# これは、学習中にモデルやトークナイザー（文章や単語を数値化する仕組み）を保存する間隔（ステップ数）を指定するものです\n",
        "save_steps = 10\n",
        "# logging_stepsという変数に10という値を代入します\n",
        "# これは、学習中にログ（学習の進捗や性能などの記録）を出力する間隔（ステップ数）を指定するものです\n",
        "logging_steps = 10\n",
        "# learning_rateという変数に2e-4という値を代入します\n",
        "# これは、学習率（パラメータを更新する速さ）の初期値を指定するものです\n",
        "learning_rate = 2e-4\n",
        "# max_grad_normという変数に0.3という値を代入します\n",
        "# これは、勾配の大きさ（パラメータを更新する強さ）の最大値を指定するものです\n",
        "max_grad_norm = 0.3\n",
        "# max_stepsという変数に500という値を代入します\n",
        "# これは、学習を終了するまでに行うステップ数（データの処理回数）の最大値を指定するものです\n",
        "max_steps = 500\n",
        "# warmup_ratioという変数に0.03という値を代入します\n",
        "# これは、学習率が初期値から最大値まで上昇する割合（ステップ数/全ステップ数）を指定するものです\n",
        "warmup_ratio = 0.03\n",
        "# lr_scheduler_typeという変数に\"constant\"という文字列を代入します\n",
        "# これは、学習率の変化の仕方（スケジューラー）の種類を指定するものです\n",
        "# constantは、学習率を一定に保つスケジューラーです\n",
        "lr_scheduler_type = \"constant\"\n",
        "\n",
        "# training_argumentsという変数に、TrainingArgumentsのインスタンス（実体）を作って代入します\n",
        "# TrainingArgumentsは、学習の設定を表すクラスです\n",
        "# 引数には、以下のような設定を指定します\n",
        "# output_dir: 結果やログを保存するディレクトリの名前を指定します\n",
        "# per_device_train_batch_size: 一つのデバイスあたりのバッチサイズを指定します\n",
        "# gradient_accumulation_steps: 勾配蓄積の回数を指定します\n",
        "# optim: 最適化アルゴリズムの種類を指定します\n",
        "# save_steps: モデルやトークナイザーを保存する間隔を指定します\n",
        "# logging_steps: ログを出力する間隔を指定します\n",
        "# learning_rate: 学習率の初期値を指定します\n",
        "# fp16: 16ビット精度で学習するかどうかを指定します。Trueならする、Falseならしないです\n",
        "# max_grad_norm: 勾配の大きさの最大値を指定します\n",
        "# max_steps: 学習を終了するまでに行うステップ数の最大値を指定します\n",
        "# warmup_ratio: 学習率が初期値から最大値まで上昇する割合を指定します\n",
        "# group_by_length: データを長さごとにグループ化してバッチ化するかどうかを指定します。Trueならする、Falseならしないです\n",
        "# lr_scheduler_type: 学習率の変化の仕方の種類を指定します\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    fp16=True,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        ")"
      ],
      "metadata": {
        "id": "PGp_rg2SrH6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "そして、最後にトレーナーにすべてを託す"
      ],
      "metadata": {
        "id": "I3t6b2TkcJwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "こんにちは、この文章は、the current web page contextから得られた情報をもとに作成しました。この文章は、Falcon-7bモデルをファインチューニングするためのPythonコードの一部です。以下のように説明します。\n",
        "\n",
        "まず、trlというライブラリをインポートします。trlとは、Hugging Face社が開発したライブラリで、事前学習された言語モデルをパラメータ効率的にファインチューニングするための技術を提供しています¹。\n",
        "\n",
        "次に、max_seq_lengthという変数に512という値を代入します。max_seq_lengthとは、言語モデルに入力する文章の最大の長さを表す数値です。512という値は、Falcon-7bモデルが扱える最大の長さです²。\n",
        "\n",
        "次に、SFTTrainerというクラスのインスタンスを作ります。SFTTrainerとは、trlライブラリの中にあるクラスで、言語モデルをファインチューニングするためのトレーナー（学習器）です¹。SFTTrainerには、以下のようなオプション（引数）を渡します。\n",
        "\n",
        "- model: ファインチューニングする言語モデルです。ここでは、Falcon-7bモデルを使っています。\n",
        "- train_dataset: ファインチューニングに使うデータセットです。ここでは、Guanacoデータセットという人間とAIアシスタントの対話を集めたデータセットを使っています³。\n",
        "- peft_config: ファインチューニングに使うPEFT（パラメータ効率的なファインチューニング）の設定です。PEFTとは、アダプタ層やプレフィックス層などの追加のパラメータを言語モデルに適用することで、全てのパラメータをファインチューニングする必要なく、下流のタスクに適応させることができる技術です¹²。\n",
        "- dataset_text_field: データセットの中で、言語モデルに入力する文章が格納されているフィールド（列）の名前です。ここでは、「text」という名前のフィールドを使っています。\n",
        "- max_seq_length: 先ほど定義した変数です。言語モデルに入力する文章の最大の長さです。\n",
        "- tokenizer: 言語モデルに入力する文章を数値化するためのトークナイザー（分割器）です。ここでは、Falcon-7bモデルに対応したトークナイザーを使っています²。\n",
        "- args: ファインチューニングに関するその他の設定です。例えば、学習率やバッチサイズやエポック数などです。ここでは、training_argumentsという変数に格納されている設定を使っています。\n",
        "\n",
        "以上が、Pythonコードの説明です。もっと詳しく知りたい場合は、以下の参考文献をご覧ください。\n",
        "\n",
        "参考文献:\n",
        "\n",
        "¹: PEFT - Hugging Face. https://huggingface.co/docs/peft アクセス日時 2023/6/11. ²: tiiuae/falcon-7b · Hugging Face. https://huggingface.co/tiiuae/falcon-7b アクセス日時 2023/6/11. ³: timdettmers/openassistant-guanaco · Datasets at Hugging Face. https://huggingface.co/datasets/timdettmers/openassistant-guanaco アクセス日時 2023/6/11."
      ],
      "metadata": {
        "id": "hjgHH_fFsSCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "こんにちは、この文章は、the current web page contextから得られた情報をもとに作成しました。この文章は、Falcon-7bモデルをファインチューニングするためのPythonコードの一部です。以下のように日本語でコメントを付けて説明します。\n",
        "\n",
        "```python\n",
        "# trlというライブラリをインポートする\n",
        "# trlは、Parameter-Efficient Fine-Tuning（PEFT）という技術を使って、大規模な言語モデルを効率的にファインチューニングするためのライブラリです\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# 言語モデルに入力する文章の最大の長さを512に設定する\n",
        "# これは、言語モデルが扱える文章の長さに制限があるためです\n",
        "max_seq_length = 512\n",
        "\n",
        "# SFTTrainerというクラスのインスタンスを作る\n",
        "# SFTTrainerは、trlライブラリが提供するクラスで、PEFTの設定や学習の実行などを行うことができます\n",
        "trainer = SFTTrainer(\n",
        "    # ファインチューニングする言語モデルをmodelという変数に代入する\n",
        "    # ここでは、Falcon-7bという大規模な言語モデルを使っています\n",
        "    model=model,\n",
        "    # ファインチューニングに使うデータセットをdatasetという変数に代入する\n",
        "    # ここでは、GuanacoというOpenAssistantデータセットのクリーンなサブセットを使っています\n",
        "    train_dataset=dataset,\n",
        "    # ファインチューニングに使うPEFTの設定をpeft_configという変数に代入する\n",
        "    # ここでは、QLoRAという技術を使って、言語モデルのパラメータを4ビットに量子化しています\n",
        "    peft_config=peft_config,\n",
        "    # データセットの中で、言語モデルに入力する文章が格納されているフィールド名を指定する\n",
        "    # ここでは、textというフィールド名を指定しています\n",
        "    dataset_text_field=\"text\",\n",
        "    # 言語モデルに入力する文章の最大の長さを指定する\n",
        "    # ここでは、上で設定したmax_seq_lengthという変数を指定しています\n",
        "    max_seq_length=max_seq_length,\n",
        "    # 言語モデルに入力する文章をトークン化（単語や文字などの単位に分割）するためのトークナイザー（分割器）を指定する\n",
        "    # ここでは、tokenizerという変数に代入されているトークナイザーを指定しています\n",
        "    tokenizer=tokenizer,\n",
        "    # 学習に関する設定やオプションを指定するための引数を指定する\n",
        "    # ここでは、training_argumentsという変数に代入されている引数を指定しています\n",
        "    args=training_arguments,\n",
        ")\n",
        "```\n",
        "\n",
        "以上が、Falcon-7bモデルをファインチューニングするためのPythonコードの一部です。このコードは、以下の機能や目的があります。\n",
        "\n",
        "- Falcon-7bモデルは、インターネット上のテキストや選別されたコーパスから学習された大規模な言語モデルで、テキスト生成や要約などのタスクに適用できます\n",
        "- Guanacoデータセットは、人間とAIアシスタントの間の対話を集めたデータセットで、自然で多様な対話を含んでいます\n",
        "- QLoRAは、言語モデルのパラメータを4ビットに量子化してメモリや計算量を削減しながら、下流のタスクに適応させる技術です\n",
        "- このコードは、Falcon-7bモデルをQLoRAを使ってGuanacoデータセットにファインチューニングすることで、チャットボットとしての性能を向上させることを目的としています"
      ],
      "metadata": {
        "id": "XzOU3K6xshup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trlというライブラリをインポートする\n",
        "# trlは、Parameter-Efficient Fine-Tuning（PEFT）という技術を使って、大規模な言語モデルを効率的にファインチューニングするためのライブラリです\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# 言語モデルに入力する文章の最大の長さを512に設定する\n",
        "# これは、言語モデルが扱える文章の長さに制限があるためです\n",
        "max_seq_length = 512\n",
        "\n",
        "# SFTTrainerというクラスのインスタンスを作る\n",
        "# SFTTrainerは、trlライブラリが提供するクラスで、PEFTの設定や学習の実行などを行うことができます\n",
        "trainer = SFTTrainer(\n",
        "    # ファインチューニングする言語モデルをmodelという変数に代入する\n",
        "    # ここでは、Falcon-7bという大規模な言語モデルを使っています\n",
        "    model=model,\n",
        "    # ファインチューニングに使うデータセットをdatasetという変数に代入する\n",
        "    # ここでは、GuanacoというOpenAssistantデータセットのクリーンなサブセットを使っています\n",
        "    train_dataset=dataset,\n",
        "    # ファインチューニングに使うPEFTの設定をpeft_configという変数に代入する\n",
        "    # ここでは、QLoRAという技術を使って、言語モデルのパラメータを4ビットに量子化しています\n",
        "    peft_config=peft_config,\n",
        "    # データセットの中で、言語モデルに入力する文章が格納されているフィールド名を指定する\n",
        "    # ここでは、textというフィールド名を指定しています\n",
        "    dataset_text_field=\"text\",\n",
        "    # 言語モデルに入力する文章の最大の長さを指定する\n",
        "    # ここでは、上で設定したmax_seq_lengthという変数を指定しています\n",
        "    max_seq_length=max_seq_length,\n",
        "    # 言語モデルに入力する文章をトークン化（単語や文字などの単位に分割）するためのトークナイザー（分割器）を指定する\n",
        "    # ここでは、tokenizerという変数に代入されているトークナイザーを指定しています\n",
        "    tokenizer=tokenizer,\n",
        "    # 学習に関する設定やオプションを指定するための引数を指定する\n",
        "    # ここでは、training_argumentsという変数に代入されている引数を指定しています\n",
        "    args=training_arguments,\n",
        ")"
      ],
      "metadata": {
        "id": "R5V_3pP4snG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "また、より安定した学習のために、float32のレイヤーノルムをアップキャストすることでモデルの前処理を行う予定です"
      ],
      "metadata": {
        "id": "GWplqqDjb3sS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "この文章は、the current web page contextから得られた情報をもとに作成しました。この文章は、Falcon-7bモデルをファインチューニングするためのPythonコードの続きです。以下のように日本語でコメントを付けて説明します。\n",
        "\n",
        "```python\n",
        "# trainer.modelという変数に代入されている言語モデルの各モジュール（部品）について、名前とモジュール自体を取り出す\n",
        "# ここでは、named_modulesというメソッドを使って、名前とモジュールの組み合わせを順番に取り出しています\n",
        "for name, module in trainer.model.named_modules():\n",
        "    # モジュールの名前に\"norm\"という文字列が含まれているかどうかを判定する\n",
        "    # ここでは、inという演算子を使って、文字列の部分一致を調べています\n",
        "    if \"norm\" in name:\n",
        "        # モジュールのデータ型をtorch.float32に変換する\n",
        "        # ここでは、toというメソッドを使って、データ型の変換を行っています\n",
        "        # torch.float32とは、PyTorchというライブラリが提供する32ビットの浮動小数点数のデータ型です\n",
        "        module = module.to(torch.float32)\n",
        "```\n",
        "\n",
        "以上が、Falcon-7bモデルをファインチューニングするためのPythonコードの続きです。このコードは、以下の機能や目的があります。\n",
        "\n",
        "- 言語モデルの各モジュールには、正規化（norm）という処理を行うモジュールが含まれています\n",
        "- 正規化とは、入力されたデータやパラメータを一定の範囲や分布に合わせることで、学習の安定性や速度を向上させる処理です\n",
        "- 正規化モジュールは、通常32ビットの浮動小数点数で表現されますが、QLoRAを使って言語モデルのパラメータを4ビットに量子化するときには、正規化モジュールも4ビットに量子化されます\n",
        "- しかし、4ビットに量子化された正規化モジュールは、性能が低下したり、エラーが発生したりすることがあります\n",
        "- そこで、このコードでは、正規化モジュールだけを32ビットに戻すことで、問題を回避することを目的としています"
      ],
      "metadata": {
        "id": "VLmaAWurtB7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.modelという変数に代入されている言語モデルの各モジュール（部品）について、名前とモジュール自体を取り出す\n",
        "# ここでは、named_modulesというメソッドを使って、名前とモジュールの組み合わせを順番に取り出しています\n",
        "for name, module in trainer.model.named_modules():\n",
        "    # モジュールの名前に\"norm\"という文字列が含まれているかどうかを判定する\n",
        "    # ここでは、inという演算子を使って、文字列の部分一致を調べています\n",
        "    if \"norm\" in name:\n",
        "        # モジュールのデータ型をtorch.float32に変換する\n",
        "        # ここでは、toというメソッドを使って、データ型の変換を行っています\n",
        "        # torch.float32とは、PyTorchというライブラリが提供する32ビットの浮動小数点数のデータ型です\n",
        "        module = module.to(torch.float32)"
      ],
      "metadata": {
        "id": "7OyIvEx7b1GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルの訓練"
      ],
      "metadata": {
        "id": "1JApkSrCcL3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "それでは、モデルを訓練してみましょう！単純に `trainer.train()` を呼び出すだけです。"
      ],
      "metadata": {
        "id": "JjvisllacNZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "この文章は、the current web page contextから得られた情報をもとに作成しました。この文章は、Falcon-7bモデルをファインチューニングするためのPythonコードの続きです。以下のように日本語でコメントを付けて説明します。\n",
        "\n",
        "```python\n",
        "# trainerという変数に代入されているSFTTrainerというクラスのインスタンス（オブジェクト）を使って、言語モデルのファインチューニングを開始する\n",
        "# ここでは、trainというメソッドを呼び出して、ファインチューニングの処理を実行しています\n",
        "trainer.train()\n",
        "```\n",
        "\n",
        "以上が、Falcon-7bモデルをファインチューニングするためのPythonコードの続きです。このコードは、以下の機能や目的があります。\n",
        "\n",
        "- SFTTrainerとは、Hugging Face社が開発したクラスで、Parameter-Efficient Fine-Tuning（PEFT）という技術を使って、大規模な言語モデルを効率的にファインチューニングすることができます\n",
        "- PEFTとは、アダプタ層やプレフィックス層などの追加のパラメータを言語モデルに適用することで、全てのパラメータをファインチューニングする必要なく、下流のタスクに適応させることができる技術です\n",
        "- trainer.trainというメソッドは、事前に設定されたデータセットやハイパーパラメータ（学習率やバッチサイズなど）に基づいて、言語モデルの追加のパラメータを学習させます\n",
        "- trainer.trainというメソッドは、学習の進捗や結果を表示したり、保存したりすることもできます"
      ],
      "metadata": {
        "id": "CqUaVmxXtXYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# trainerという変数に代入されているSFTTrainerというクラスのインスタンス（オブジェクト）を使って、言語モデルのファインチューニングを開始する\n",
        "# ここでは、trainというメソッドを呼び出して、ファインチューニングの処理を実行しています\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "_kbS7nRxcMt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "トレーニング中、モデルは次のようにきれいに収束していくはずです：\n",
        "\n",
        "![image](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/loss-falcon-7b.png)\n",
        "\n",
        "また、`SFTTrainer`は、トレーニング中にモデル全体を保存するのではなく、アダプターだけを適切に保存するように配慮しています。"
      ],
      "metadata": {
        "id": "H5c0ppfasK29"
      }
    }
  ]
}